"""
Enhanced Acquisition Worker - F√°ze 2
Pokroƒçil√° akvizice dat s Tor/I2P anonymizac√≠ a Playwright scraping
"""

import asyncio
import logging
import os
import random
import json
from typing import Dict, Any, Optional, List
from pathlib import Path
import redis
import dramatiq
from dramatiq.brokers.redis import RedisBroker
import aiohttp
import aiohttp_socks
import polars as pl
from datetime import datetime, timezone
import hashlib
import re
import urllib.parse
from bs4 import BeautifulSoup

# Tor/Anonymization imports
import stem
from stem import Signal
from stem.control import Controller

# Playwright imports
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from fake_useragent import UserAgent

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Redis broker setup
redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
redis_client = redis.from_url(redis_url)
broker = RedisBroker(url=redis_url)
dramatiq.set_broker(broker)


class PersonaManager:
    """Spr√°vce browser person pro anti-detection"""

    def __init__(self):
        self.personas = self._generate_personas()
        self.domain_assignments = {}  # domain -> persona mapping

    def _generate_personas(self) -> List[Dict[str, Any]]:
        """Generuj pool realistick√Ωch browser person"""
        ua = UserAgent()
        personas = []

        # Generuj r≈Øzn√© kombinace browser fingerprints
        screen_resolutions = [
            {"width": 1920, "height": 1080},
            {"width": 1366, "height": 768},
            {"width": 1440, "height": 900},
            {"width": 1536, "height": 864},
            {"width": 1280, "height": 720},
        ]

        languages = ["en-US", "en-GB", "en-CA", "fr-FR", "de-DE", "es-ES"]
        timezones = ["America/New_York", "Europe/London", "Europe/Paris", "Europe/Berlin"]

        for i in range(int(os.getenv("PERSONA_POOL_SIZE", "10"))):
            resolution = random.choice(screen_resolutions)

            persona = {
                "id": f"persona_{i:03d}",
                "user_agent": ua.random,
                "viewport": resolution,
                "screen": resolution,
                "language": random.choice(languages),
                "timezone": random.choice(timezones),
                "platform": random.choice(["Win32", "MacIntel", "Linux x86_64"]),
                "webgl_vendor": random.choice(
                    ["Google Inc. (NVIDIA)", "Google Inc. (Intel)", "Google Inc. (AMD)"]
                ),
                "webgl_renderer": random.choice(
                    [
                        "ANGLE (NVIDIA GeForce GTX 1060 Direct3D11 vs_5_0 ps_5_0)",
                        "ANGLE (Intel(R) HD Graphics 620 Direct3D11 vs_5_0 ps_5_0)",
                        "ANGLE (AMD Radeon RX 580 Direct3D11 vs_5_0 ps_5_0)",
                    ]
                ),
            }
            personas.append(persona)

        logger.info(f"üìã Vygenerov√°no {len(personas)} browser person")
        return personas

    def get_persona_for_domain(self, domain: str) -> Dict[str, Any]:
        """Z√≠skej nebo p≈ôi≈ôaƒè personu pro dom√©nu"""
        if domain not in self.domain_assignments:
            # P≈ôi≈ôaƒè n√°hodnou personu pro novou dom√©nu
            persona = random.choice(self.personas)
            self.domain_assignments[domain] = persona
            logger.info(f"üé≠ P≈ôi≈ôazena persona {persona['id']} pro dom√©nu {domain}")

        return self.domain_assignments[domain]


class TorManager:
    """Spr√°vce Tor p≈ôipojen√≠ a circuit management"""

    def __init__(self):
        self.tor_proxy_url = os.getenv("TOR_PROXY_URL", "socks5://localhost:9050")
        self.control_password = os.getenv("TOR_CONTROL_PASSWORD", "deepresearch2025")
        self.controller = None

    async def init_controller(self):
        """Inicializuj Tor controller"""
        try:
            self.controller = Controller.from_port(port=9051)
            self.controller.authenticate(password=self.control_password)
            logger.info("‚úÖ Tor controller p≈ôipojen")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Tor controller nedostupn√Ω: {e}")
            self.controller = None

    async def new_identity(self):
        """Po≈æ√°dej o novou Tor identitu"""
        if self.controller:
            try:
                self.controller.signal(Signal.NEWNYM)
                logger.info("üîÑ Nov√° Tor identita po≈æ√°d√°na")
                await asyncio.sleep(5)  # Poƒçkej na zmƒõnu okruhu
                return True
            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi zmƒõnƒõ Tor identity: {e}")
        return False

    def get_proxy_config(self) -> Dict[str, str]:
        """Z√≠skej proxy konfiguraci"""
        return {"http": self.tor_proxy_url, "https": self.tor_proxy_url}


class EnhancedAcquisitionWorker:
    """Pokroƒçil√Ω worker pro sbƒõr dat s anonymizac√≠ a stealth scraping"""

    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.data_dir = Path("/app/data")
        self.cache_dir = Path("/app/cache")
        self.persona_manager = PersonaManager()
        self.tor_manager = TorManager()

        # Zajisti ≈æe adres√°≈ôe existuj√≠
        self.data_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)

        # Statistiky
        self.scraping_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "tor_circuits_changed": 0,
            "playwright_sessions": 0,
        }

    async def init_playwright(self):
        """Inicializuj Playwright browser"""
        if not self.playwright:
            self.playwright = await async_playwright().start()

            # Spus≈• browser s anti-detection nastaven√≠m
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-blink-features=AutomationControlled",
                    "--disable-dev-shm-usage",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                ],
            )
            logger.info("üé≠ Playwright browser inicializov√°n")

    async def create_stealth_context(
        self, persona: Dict[str, Any], use_proxy: bool = True
    ) -> BrowserContext:
        """Vytvo≈ô stealth browser context s danou personou"""
        proxy_config = None
        if use_proxy:
            proxy_config = {
                "server": self.tor_manager.tor_proxy_url.replace("socks5://", "socks5://")
            }

        context = await self.browser.new_context(
            user_agent=persona["user_agent"],
            viewport=persona["viewport"],
            screen=persona["screen"],
            locale=persona["language"],
            timezone_id=persona["timezone"],
            proxy=proxy_config,
            # Anti-detection headers
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": f"{persona['language']},en;q=0.5",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            },
        )

        # Injektuj anti-detection skripty
        await context.add_init_script(
            """
            // Odstranit webdriver vlastnosti
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            // Upravit permissions API
            Object.defineProperty(navigator, 'permissions', {
                get: () => ({
                    query: () => Promise.resolve({ state: 'granted' }),
                }),
            });
            
            // Upravit plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            // Upravit languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['"""
            + persona["language"]
            + """'],
            });
        """
        )

        return context

    async def scrape_with_playwright(
        self, url: str, persona: Dict[str, Any], task_id: str
    ) -> Dict[str, Any]:
        """Scraping pomoc√≠ Playwright s anti-detection"""
        try:
            await self.init_playwright()

            context = await self.create_stealth_context(persona)
            page = await context.new_page()

            # Simulace lidsk√©ho chov√°n√≠
            await page.set_extra_http_headers({"User-Agent": persona["user_agent"]})

            # Naviguj na str√°nku s timeoutem
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)

            # N√°hodn√© ƒçek√°n√≠ (simulace ƒçten√≠)
            await asyncio.sleep(random.uniform(2, 5))

            # Simulace scrollov√°n√≠
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight/3)")
            await asyncio.sleep(random.uniform(1, 2))

            # Z√≠skej obsah
            content = await page.content()
            title = await page.title()

            # Metadata
            metadata = {
                "url": url,
                "title": title,
                "content_length": len(content),
                "persona_id": persona["id"],
                "method": "playwright",
                "scraped_at": datetime.now(timezone.utc).isoformat(),
                "task_id": task_id,
            }

            await context.close()
            self.scraping_stats["playwright_sessions"] += 1

            return {"success": True, "content": content, "metadata": metadata}

        except Exception as e:
            logger.error(f"‚ùå Playwright scraping error for {url}: {e}")
            return {"success": False, "error": str(e), "url": url}

    async def scrape_with_aiohttp(
        self, url: str, persona: Dict[str, Any], task_id: str
    ) -> Dict[str, Any]:
        """Fallback scraping pomoc√≠ aiohttp s Tor proxy"""
        try:
            # Konfiguruj proxy pro aiohttp
            connector = aiohttp_socks.ProxyConnector.from_url(self.tor_manager.tor_proxy_url)

            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            async with aiohttp.ClientSession(
                connector=connector, timeout=timeout, headers={"User-Agent": persona["user_agent"]}
            ) as session:

                async with session.get(url) as response:
                    content = await response.text()

                    metadata = {
                        "url": url,
                        "status_code": response.status,
                        "content_type": response.headers.get("content-type", ""),
                        "content_length": len(content),
                        "persona_id": persona["id"],
                        "method": "aiohttp",
                        "scraped_at": datetime.now(timezone.utc).isoformat(),
                        "task_id": task_id,
                    }

                    return {"success": True, "content": content, "metadata": metadata}

        except Exception as e:
            logger.error(f"‚ùå aiohttp scraping error for {url}: {e}")
            return {"success": False, "error": str(e), "url": url}

    async def scrape_url_enhanced(
        self, url: str, task_id: str, force_tor: bool = False
    ) -> Dict[str, Any]:
        """Enhanced scraping s automatickou metodou selection"""
        try:
            # Extrahuj dom√©nu pro persona assignment
            from urllib.parse import urlparse

            domain = urlparse(url).netloc

            # Z√≠skej personu pro tuto dom√©nu
            persona = self.persona_manager.get_persona_for_domain(domain)

            # Detekce typu str√°nky
            needs_js = any(
                indicator in url.lower()
                for indicator in [
                    "single-page",
                    "spa",
                    "react",
                    "angular",
                    "vue",
                    "javascript",
                    "dynamic",
                    "ajax",
                ]
            )

            # Rozhodnut√≠ o metodƒõ scraping
            if needs_js or ".onion" in url or force_tor:
                logger.info(f"üé≠ Pou≈æ√≠v√°m Playwright pro {url}")
                result = await self.scrape_with_playwright(url, persona, task_id)
            else:
                logger.info(f"üåê Pou≈æ√≠v√°m aiohttp pro {url}")
                result = await self.scrape_with_aiohttp(url, persona, task_id)

            self.scraping_stats["total_requests"] += 1
            if result["success"]:
                self.scraping_stats["successful_requests"] += 1

            # Ulo≈æ data v Parquet form√°tu
            if result["success"]:
                raw_data = pl.DataFrame(
                    {
                        "url": [url],
                        "content": [result["content"]],
                        "metadata": [json.dumps(result["metadata"])],
                        "scraped_at": [datetime.now(timezone.utc)],
                    }
                )

                output_path = self.data_dir / f"raw_{task_id}.parquet"
                raw_data.write_parquet(output_path)

                # Po≈°li do processing queue
                process_scraped_data.send(str(output_path), task_id)

                result["output_path"] = str(output_path)
                result["stats"] = self.scraping_stats.copy()

            return result

        except Exception as e:
            logger.error(f"‚ùå Enhanced scraping error for {url}: {e}")
            return {"success": False, "url": url, "error": str(e), "task_id": task_id}

    async def extract_and_download_images(
        self, content: str, base_url: str, task_id: str
    ) -> Dict[str, Any]:
        """
        Extrahuj a st√°hni obr√°zky ze str√°nky (F√ÅZE 3: Multi-Modality)

        Args:
            content: HTML obsah str√°nky
            base_url: Z√°kladn√≠ URL pro relativn√≠ odkazy
            task_id: ID √∫lohy

        Returns:
            Informace o nalezen√Ωch a sta≈æen√Ωch obr√°zc√≠ch
        """
        try:
            logger.info(f"üñºÔ∏è Detekuji obr√°zky na str√°nce: {base_url}")

            soup = BeautifulSoup(content, "html.parser")

            # Najdi v≈°echny img tagy
            img_tags = soup.find_all("img")

            # Extrakce obr√°zk≈Ø z r≈Øzn√Ωch zdroj≈Ø
            image_urls = set()

            # Standard img src
            for img in img_tags:
                src = img.get("src")
                if src:
                    absolute_url = urllib.parse.urljoin(base_url, src)
                    image_urls.add(absolute_url)

                # Lazy loading obr√°zky
                data_src = img.get("data-src") or img.get("data-lazy-src")
                if data_src:
                    absolute_url = urllib.parse.urljoin(base_url, data_src)
                    image_urls.add(absolute_url)

            # CSS background images
            style_tags = soup.find_all(["div", "section", "header"], style=True)
            for tag in style_tags:
                style = tag.get("style", "")
                bg_matches = re.findall(r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', style)
                for match in bg_matches:
                    absolute_url = urllib.parse.urljoin(base_url, match)
                    image_urls.add(absolute_url)

            # Inline SVG (mal√© obr√°zky)
            svg_tags = soup.find_all("svg")
            svg_count = len(svg_tags)

            # Filtruj platn√© image URLs
            valid_image_urls = []
            image_extensions = {".jpg", ".jpeg", ".png", ".gif", ".webp", ".bmp", ".svg", ".ico"}

            for url in image_urls:
                # Kontrola roz≈°√≠≈ôen√≠
                parsed = urllib.parse.urlparse(url)
                path_lower = parsed.path.lower()

                if (
                    any(path_lower.endswith(ext) for ext in image_extensions)
                    or "image" in parsed.query.lower()
                    or url.startswith("data:image/")
                ):

                    # Filtrace podle velikosti URL (vylouƒçen√≠ mal√Ωch ikon)
                    if not any(
                        skip in url.lower() for skip in ["favicon", "icon", "logo", "button"]
                    ):
                        valid_image_urls.append(url)

            logger.info(f"üìä Nalezeno {len(valid_image_urls)} potenci√°ln√≠ch obr√°zk≈Ø na {base_url}")

            # Stahov√°n√≠ obr√°zk≈Ø (omezeno na rozumn√Ω poƒçet)
            max_images = int(os.getenv("MAX_IMAGES_PER_PAGE", "10"))
            download_urls = valid_image_urls[:max_images]

            downloaded_images = []
            images_dir = self.data_dir / "images" / task_id
            images_dir.mkdir(parents=True, exist_ok=True)

            for i, img_url in enumerate(download_urls):
                try:
                    image_result = await self._download_single_image(
                        img_url, images_dir, f"{task_id}_img_{i:03d}", base_url
                    )
                    if image_result["success"]:
                        downloaded_images.append(image_result)

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠ {img_url}: {e}")
                    continue

            return {
                "success": True,
                "total_images_found": len(image_urls),
                "valid_images_found": len(valid_image_urls),
                "images_downloaded": len(downloaded_images),
                "svg_elements": svg_count,
                "downloaded_images": downloaded_images,
                "images_directory": str(images_dir),
            }

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi extrakci obr√°zk≈Ø: {e}")
            return {
                "success": False,
                "error": str(e),
                "total_images_found": 0,
                "images_downloaded": 0,
            }

    async def _download_single_image(
        self, img_url: str, images_dir: Path, filename_prefix: str, source_url: str
    ) -> Dict[str, Any]:
        """St√°hni jeden obr√°zek"""
        try:
            # Detekce roz≈°√≠≈ôen√≠
            parsed = urllib.parse.urlparse(img_url)
            extension = Path(parsed.path).suffix.lower()
            if not extension or extension not in {
                ".jpg",
                ".jpeg",
                ".png",
                ".gif",
                ".webp",
                ".bmp",
                ".svg",
            }:
                extension = ".jpg"  # Default

            filename = f"{filename_prefix}{extension}"
            filepath = images_dir / filename

            # Pokud je to data URL, dek√≥duj p≈ô√≠mo
            if img_url.startswith("data:image/"):
                try:
                    header, data = img_url.split(",", 1)
                    image_data = base64.b64decode(data)

                    with open(filepath, "wb") as f:
                        f.write(image_data)

                    return {
                        "success": True,
                        "url": img_url[:100] + "..." if len(img_url) > 100 else img_url,
                        "filepath": str(filepath),
                        "filename": filename,
                        "file_size": len(image_data),
                        "source_url": source_url,
                        "download_method": "data_url",
                    }
                except Exception as e:
                    logger.error(f"‚ùå Data URL decode error: {e}")
                    return {"success": False, "error": str(e)}

            # Stahov√°n√≠ p≈ôes HTTP
            timeout = aiohttp.ClientTimeout(total=20, connect=5)

            # Pou≈æij proxy pokud je .onion
            if ".onion" in img_url or ".onion" in source_url:
                connector = aiohttp_socks.ProxyConnector.from_url(self.tor_manager.tor_proxy_url)
            else:
                connector = aiohttp.TCPConnector(limit=10)

            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    "Referer": source_url,
                    "Accept": "image/webp,image/apng,image/*,*/*;q=0.8",
                }

                async with session.get(img_url, headers=headers) as response:
                    if response.status == 200:
                        content_type = response.headers.get("content-type", "")

                        # Ovƒõ≈ô ≈æe je to opravdu obr√°zek
                        if not content_type.startswith("image/"):
                            logger.warning(f"‚ö†Ô∏è {img_url} nen√≠ obr√°zek: {content_type}")
                            return {"success": False, "error": f"Not an image: {content_type}"}

                        # Kontrola velikosti (max 10MB)
                        content_length = response.headers.get("content-length")
                        if content_length and int(content_length) > 10 * 1024 * 1024:
                            return {"success": False, "error": "Image too large"}

                        image_data = await response.read()

                        # Ulo≈æen√≠ souboru
                        with open(filepath, "wb") as f:
                            f.write(image_data)

                        return {
                            "success": True,
                            "url": img_url,
                            "filepath": str(filepath),
                            "filename": filename,
                            "file_size": len(image_data),
                            "content_type": content_type,
                            "source_url": source_url,
                            "download_method": "http",
                        }
                    else:
                        return {"success": False, "error": f"HTTP {response.status}"}

        except Exception as e:
            logger.error(f"‚ùå Image download error for {img_url}: {e}")
            return {"success": False, "error": str(e)}

    async def cleanup(self):
        """Cleanup resources"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        if self.session and not self.session.closed:
            await self.session.close()


# Enhanced Dramatiq tasks
@dramatiq.actor(queue_name="acquisition")
def scrape_url_enhanced_task(
    url: str, task_id: str = None, force_tor: bool = False
) -> Dict[str, Any]:
    """Enhanced Dramatiq actor pro pokroƒçil√Ω scraping"""
    if task_id is None:
        task_id = f"enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    worker = EnhancedAcquisitionWorker()

    # Spus≈• async operaci
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        # Inicializuj Tor manager
        loop.run_until_complete(worker.tor_manager.init_controller())

        # Proveƒè scraping
        result = loop.run_until_complete(worker.scrape_url_enhanced(url, task_id, force_tor))

        # Cleanup
        loop.run_until_complete(worker.cleanup())

        return result
    finally:
        loop.close()


# Import z processing worker pro cross-service communication
@dramatiq.actor(queue_name="processing")
def process_scraped_data(file_path: str, task_id: str):
    """Forward declaration - implementov√°no v processing_worker"""
    pass


if __name__ == "__main__":
    logger.info("Starting Enhanced Acquisition Worker (Phase 2)...")

    # Spus≈• worker
    from dramatiq.cli import main
    import sys

    sys.argv = ["dramatiq", "workers.acquisition_worker", "--processes", "2", "--threads", "4"]
    main()
