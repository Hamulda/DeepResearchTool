"""
Discovery Worker - Autonomn√≠ discovery engine (F√°ze 4)
Automaticky objevuje nov√© zdroje dat a p≈ôid√°v√° je do scraping fronty
"""

import asyncio
import logging
import os
import json
import re
import random
from typing import Dict, Any, List, Set, Optional
from pathlib import Path
import redis
import dramatiq
from dramatiq.brokers.redis import RedisBroker
import aiohttp
import aiohttp_socks
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin, urlparse
import schedule
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Redis broker setup
redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
redis_client = redis.from_url(redis_url)
broker = RedisBroker(url=redis_url)
dramatiq.set_broker(broker)


class DiscoveryEngine:
    """Autonomn√≠ engine pro objevov√°n√≠ nov√Ωch zdroj≈Ø"""

    def __init__(self):
        self.tor_proxy_url = os.getenv("TOR_PROXY_URL", "socks5://localhost:9050")
        self.discovery_interval = int(os.getenv("DISCOVERY_INTERVAL", "3600"))  # 1 hour
        self.data_dir = Path("/app/data")
        self.configs_dir = Path("/app/configs")

        # Zajisti ≈æe adres√°≈ôe existuj√≠
        self.data_dir.mkdir(exist_ok=True)
        self.configs_dir.mkdir(exist_ok=True)

        # Naƒçti whitelist a konfiguraci
        self.whitelist = self._load_whitelist()
        self.discovered_urls = set()
        self.discovery_stats = {
            "total_discovered": 0,
            "successful_discoveries": 0,
            "last_discovery": None,
            "discovery_sources": {},
        }

        # Discovery sources
        self.discovery_sources = {
            "ahmia": "https://ahmia.fi/search/?q={query}",
            "torch": "http://xmh57jrknzkhv6y3ls3ubitzfqnkrwxhopf5aygthi7d6rplyvk3noyd.onion/cgi-bin/omega/omega?P={query}",
            "hidden_wiki": "http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/index.php/Main_Page",
            "duckduckgo": "https://duckduckgo.com/html/?q={query}+site:*.onion",
        }

        # Discovery patterns
        self.url_patterns = [
            r"https?://[a-z2-7]{16,56}\.onion[/\w\-._~:/?#[\]@!$&\'()*+,;=]*",
            r"http://[a-z2-7]{16,56}\.onion[/\w\-._~:/?#[\]@!$&\'()*+,;=]*",
            r"[a-z2-7]{16,56}\.onion",
            r"https?://[a-z0-9\-]+\.i2p[/\w\-._~:/?#[\]@!$&\'()*+,;=]*",
        ]

        self.session = None

    def _load_whitelist(self) -> Dict[str, Any]:
        """Naƒçti whitelist a compliance pravidla"""
        try:
            whitelist_path = self.configs_dir / "tor_legal_whitelist.json"
            if whitelist_path.exists():
                with open(whitelist_path, "r") as f:
                    return json.load(f)
            else:
                logger.warning("‚ö†Ô∏è Whitelist soubor nenalezen, pou≈æ√≠v√°m v√Ωchoz√≠")
                return {
                    "legal_domains": ["httpbin.org", "example.com"],
                    "blocked_domains": [],
                    "compliance_notes": {"purpose": "Research and educational purposes only"},
                }
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ whitelist: {e}")
            return {"legal_domains": [], "blocked_domains": []}

    async def _get_session(self) -> aiohttp.ClientSession:
        """Z√≠skej HTTP session s Tor proxy"""
        if self.session is None or self.session.closed:
            connector = aiohttp_socks.ProxyConnector.from_url(self.tor_proxy_url)
            timeout = aiohttp.ClientTimeout(total=30, connect=10)

            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
            )
        return self.session

    def _is_url_allowed(self, url: str) -> bool:
        """Zkontroluj zda je URL povolena podle whitelist"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # Zkontroluj blocked domains
            blocked_domains = self.whitelist.get("blocked_domains", [])
            if any(blocked in domain for blocked in blocked_domains):
                return False

            # Pro .onion adresy buƒè v√≠ce permisivn√≠ (research √∫ƒçely)
            if domain.endswith(".onion"):
                # Z√°kladn√≠ bezpeƒçnostn√≠ kontroly
                if len(domain.split(".")[0]) < 16:  # Minim√°ln√≠ d√©lka onion adresy
                    return False
                return True

            # Pro clearnet zkontroluj whitelist
            legal_domains = self.whitelist.get("legal_domains", [])
            if legal_domains and not any(legal in domain for legal in legal_domains):
                return False

            return True

        except Exception as e:
            logger.error(f"‚ùå URL validation error: {e}")
            return False

    def _extract_urls_from_text(self, text: str) -> Set[str]:
        """Extrahuj URL z textu pomoc√≠ regex patterns"""
        urls = set()

        for pattern in self.url_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # Normalizuj URL
                if not match.startswith(("http://", "https://")):
                    if match.endswith(".onion"):
                        match = f"http://{match}"
                    elif match.endswith(".i2p"):
                        match = f"http://{match}"

                if self._is_url_allowed(match):
                    urls.add(match)

        return urls

    async def discover_from_source(self, source_name: str, query: str = "research") -> Set[str]:
        """Objevuj URL z konkr√©tn√≠ho zdroje"""
        try:
            if source_name not in self.discovery_sources:
                logger.warning(f"‚ö†Ô∏è Nezn√°m√Ω discovery source: {source_name}")
                return set()

            source_url = self.discovery_sources[source_name].format(query=query)
            logger.info(f"üîç Discovering from {source_name}: {query}")

            session = await self._get_session()
            async with session.get(source_url) as response:
                if response.status == 200:
                    content = await response.text()
                    discovered_urls = self._extract_urls_from_text(content)

                    logger.info(f"‚úÖ Discovered {len(discovered_urls)} URLs from {source_name}")

                    # Aktualizuj statistiky
                    self.discovery_stats["discovery_sources"][source_name] = self.discovery_stats[
                        "discovery_sources"
                    ].get(source_name, 0) + len(discovered_urls)

                    return discovered_urls
                else:
                    logger.warning(f"‚ö†Ô∏è Discovery source {source_name} returned {response.status}")
                    return set()

        except Exception as e:
            logger.error(f"‚ùå Discovery error from {source_name}: {e}")
            return set()

    async def discover_from_known_sites(self) -> Set[str]:
        """Objevuj nov√© URL z ji≈æ zn√°m√Ωch str√°nek"""
        try:
            discovered_urls = set()

            # Naƒçti u≈æ zn√°m√© URL z Redis cache
            known_urls_key = "discovery:known_urls"
            known_urls_data = redis_client.get(known_urls_key)

            if known_urls_data:
                known_urls = json.loads(known_urls_data)
            else:
                # Fallback na z√°kladn√≠ seed URL
                known_urls = [
                    "http://3g2upl4pq6kufc4m.onion",  # DuckDuckGo onion
                    "https://ahmia.fi",  # Ahmia search
                ]

            # Prohledej zn√°m√© str√°nky pro nov√© odkazy
            session = await self._get_session()

            for url in known_urls[:10]:  # Omez na 10 URL pro performance
                try:
                    logger.info(f"üï∑Ô∏è Crawling known site: {url}")

                    async with session.get(url) as response:
                        if response.status == 200:
                            content = await response.text()
                            new_urls = self._extract_urls_from_text(content)
                            discovered_urls.update(new_urls)

                            await asyncio.sleep(random.uniform(2, 5))  # Zdvo≈ôil√© ƒçek√°n√≠

                except Exception as crawl_error:
                    logger.warning(f"‚ö†Ô∏è Crawling error for {url}: {crawl_error}")
                    continue

            logger.info(f"üï∏Ô∏è Discovered {len(discovered_urls)} URLs from known sites")
            return discovered_urls

        except Exception as e:
            logger.error(f"‚ùå Known sites discovery error: {e}")
            return set()

    async def run_discovery_cycle(self) -> Dict[str, Any]:
        """Spus≈• kompletn√≠ discovery cyklus"""
        try:
            cycle_start = datetime.now(timezone.utc)
            logger.info("üöÄ Spou≈°t√≠m discovery cyklus")

            all_discovered = set()

            # 1. Discovery z search engines
            search_queries = ["research", "academic", "information", "data", "science"]
            for query in search_queries[:2]:  # Omez poƒçet dotaz≈Ø
                for source in ["ahmia", "duckduckgo"]:  # Bezpeƒçn√© zdroje
                    urls = await self.discover_from_source(source, query)
                    all_discovered.update(urls)
                    await asyncio.sleep(random.uniform(5, 10))  # Rate limiting

            # 2. Discovery z zn√°m√Ωch str√°nek
            known_site_urls = await self.discover_from_known_sites()
            all_discovered.update(known_site_urls)

            # 3. Filtruj duplicity a u≈æ zpracovan√© URL
            new_urls = all_discovered - self.discovered_urls
            self.discovered_urls.update(new_urls)

            # 4. P≈ôidej nov√© URL do scraping fronty
            queued_count = 0
            for url in new_urls:
                try:
                    # Import acquisition worker task
                    from workers.acquisition_worker import scrape_url_enhanced_task

                    task_id = f"discovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{queued_count}"
                    scrape_url_enhanced_task.send(url, task_id, force_tor=True)
                    queued_count += 1

                    if queued_count >= 50:  # Omez poƒçet nov√Ωch √∫loh
                        break

                except Exception as queue_error:
                    logger.error(f"‚ùå Queue error for {url}: {queue_error}")
                    continue

            # 5. Aktualizuj statistiky
            cycle_duration = (datetime.now(timezone.utc) - cycle_start).total_seconds()

            self.discovery_stats.update(
                {
                    "total_discovered": len(all_discovered),
                    "successful_discoveries": len(new_urls),
                    "last_discovery": cycle_start.isoformat(),
                    "cycle_duration": cycle_duration,
                    "queued_for_scraping": queued_count,
                }
            )

            # 6. Ulo≈æ discovered URLs do Redis
            redis_client.setex(
                "discovery:known_urls", 86400, json.dumps(list(self.discovered_urls))  # 24 hours
            )

            # 7. Ulo≈æ statistiky
            redis_client.setex("discovery:stats", 3600, json.dumps(self.discovery_stats))  # 1 hour

            logger.info(
                f"‚úÖ Discovery cyklus dokonƒçen: {len(new_urls)} nov√Ωch URL, {queued_count} p≈ôid√°no do fronty"
            )

            return {
                "success": True,
                "discovered_total": len(all_discovered),
                "new_urls": len(new_urls),
                "queued": queued_count,
                "duration": cycle_duration,
                "cycle_time": cycle_start.isoformat(),
            }

        except Exception as e:
            logger.error(f"‚ùå Discovery cycle error: {e}")
            return {
                "success": False,
                "error": str(e),
                "cycle_time": datetime.now(timezone.utc).isoformat(),
            }

        finally:
            # Cleanup session
            if self.session and not self.session.closed:
                await self.session.close()
                self.session = None

    async def cleanup(self):
        """Cleanup resources"""
        if self.session and not self.session.closed:
            await self.session.close()


# Dramatiq tasks
@dramatiq.actor(queue_name="discovery")
def run_discovery_cycle() -> Dict[str, Any]:
    """Dramatiq actor pro discovery cyklus"""
    engine = DiscoveryEngine()

    # Spus≈• async operaci
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        result = loop.run_until_complete(engine.run_discovery_cycle())
        loop.run_until_complete(engine.cleanup())
        return result
    finally:
        loop.close()


@dramatiq.actor(queue_name="discovery")
def schedule_discovery():
    """Pl√°novan√Ω discovery task"""
    logger.info("‚è∞ Scheduled discovery task triggered")
    return run_discovery_cycle.send()


# Scheduler pro automatick√© discovery
def setup_discovery_scheduler():
    """Nastav automatick√© discovery"""
    interval_hours = int(os.getenv("DISCOVERY_INTERVAL", "3600")) // 3600

    schedule.every(interval_hours).hours.do(lambda: run_discovery_cycle.send())
    logger.info(f"üìÖ Discovery napl√°nov√°no ka≈æd√Ωch {interval_hours} hodin")

    # Spus≈• prvn√≠ discovery po 5 minut√°ch
    schedule.every(5).minutes.do(lambda: run_discovery_cycle.send()).tag("initial")


if __name__ == "__main__":
    logger.info("Starting Discovery Worker (Phase 4)...")

    # Nastav scheduler
    setup_discovery_scheduler()

    # Spus≈• prvn√≠ discovery cycle okam≈æitƒõ (pro testov√°n√≠)
    run_discovery_cycle.send()

    # Spus≈• scheduler
    while True:
        schedule.run_pending()
        time.sleep(60)  # Zkontroluj ka≈ædou minutu
