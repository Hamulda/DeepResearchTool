"""
Autonomous Worker - Phase 4
Kontinu√°ln√≠, proaktivn√≠ syst√©m pro autonomn√≠ monitorov√°n√≠ a anal√Ωzu
"""

import asyncio
import logging
import os
import json
import hashlib
import smtplib
from typing import Dict, Any, List, Optional, Set
from pathlib import Path
from datetime import datetime, timezone, timedelta
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
import requests
import schedule
import time
import threading
from dataclasses import dataclass, asdict
from urllib.parse import urlparse

# Redis a Dramatiq setup s fallback
try:
    import redis
    import dramatiq
    from dramatiq.brokers.redis import RedisBroker

    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    redis_client = redis.from_url(redis_url)
    broker = RedisBroker(url=redis_url)
    dramatiq.set_broker(broker)
    DRAMATIQ_AVAILABLE = True
    logger.info("‚úÖ Redis/Dramatiq dostupn√©")
except ImportError as e:
    DRAMATIQ_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Redis/Dramatiq nen√≠ dostupn√Ω: {e}")

    # Mock objekty pro lok√°ln√≠ testov√°n√≠
    class MockRedisClient:
        def from_url(self, url):
            return self

    class MockBroker:
        def __init__(self, url):
            pass

    class MockDramatiq:
        def set_broker(self, broker):
            pass

        def actor(self, queue_name=None):
            def decorator(func):
                return func

            return decorator

    redis_client = MockRedisClient()
    broker = MockBroker("")
    dramatiq = MockDramatiq()

# Import workers
import sys

# P≈ôidej lok√°ln√≠ cestu m√≠sto Docker cesty
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

try:
    from acquisition_worker import scrape_url

    ACQUISITION_WORKER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Acquisition worker nen√≠ dostupn√Ω: {e}")
    ACQUISITION_WORKER_AVAILABLE = False

try:
    from processing_worker import process_scraped_data_enhanced, process_with_knowledge_graph

    PROCESSING_WORKER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Processing worker nen√≠ dostupn√Ω: {e}")
    PROCESSING_WORKER_AVAILABLE = False

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class MonitoredSource:
    """Reprezentace monitorovan√©ho zdroje"""

    url: str
    check_interval: int  # v minut√°ch
    last_check: Optional[datetime] = None
    last_content_hash: Optional[str] = None
    last_etag: Optional[str] = None
    last_modified: Optional[str] = None
    priority: int = 1  # 1=n√≠zk√°, 5=vysok√°
    enabled: bool = True
    keywords: List[str] = None  # kl√≠ƒçov√° slova pro upozornƒõn√≠
    task_id: Optional[str] = None


@dataclass
class AlertRule:
    """Pravidlo pro upozornƒõn√≠"""

    rule_id: str
    name: str
    condition_type: str  # "keyword_mention", "entity_connection", "new_source"
    condition_params: Dict[str, Any]
    notification_channels: List[str]  # "email", "file", "telegram"
    enabled: bool = True
    created_at: datetime = None


@dataclass
class Alert:
    """Vygenerovan√© upozornƒõn√≠"""

    alert_id: str
    rule_id: str
    title: str
    message: str
    severity: str  # "low", "medium", "high", "critical"
    source_url: str
    triggered_at: datetime
    sent: bool = False


class ChangeDetector:
    """Detektor zmƒõn na webov√Ωch str√°nk√°ch"""

    def __init__(self):
        # Pou≈æij lok√°ln√≠ cestu m√≠sto Docker cesty
        base_dir = Path.cwd() / "data"
        self.data_dir = base_dir
        self.cache_dir = base_dir / "cache"
        self.data_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)

    def check_page_changes(self, source: MonitoredSource) -> Dict[str, Any]:
        """
        Zkontroluj zmƒõny na str√°nce s optimalizovan√Ωm p≈ô√≠stupem
        """
        try:
            logger.info(f"üîç Kontroluji zmƒõny: {source.url}")

            headers = {"User-Agent": "Mozilla/5.0 (compatible; ResearchBot/1.0)"}

            # P≈ôidej podm√≠nƒõn√© hlaviƒçky pokud jsou dostupn√©
            if source.last_etag:
                headers["If-None-Match"] = source.last_etag
            if source.last_modified:
                headers["If-Modified-Since"] = source.last_modified

            # Nejprve zkus HEAD request
            try:
                head_response = requests.head(source.url, headers=headers, timeout=30)

                # Pokud 304 Not Modified, ≈æ√°dn√© zmƒõny
                if head_response.status_code == 304:
                    logger.info(f"‚úÖ ≈Ω√°dn√© zmƒõny (304): {source.url}")
                    return {"changed": False, "reason": "not_modified_304", "status_code": 304}

                # Zkontroluj ETag a Last-Modified
                current_etag = head_response.headers.get("ETag")
                current_modified = head_response.headers.get("Last-Modified")

                if current_etag and source.last_etag and current_etag == source.last_etag:
                    logger.info(f"‚úÖ ≈Ω√°dn√© zmƒõny (ETag): {source.url}")
                    return {"changed": False, "reason": "etag_match", "etag": current_etag}

            except requests.RequestException as e:
                logger.warning(f"‚ö†Ô∏è HEAD request selhal pro {source.url}: {e}")
                # Pokraƒçuj s GET requestem

            # Pokud HEAD nedorazil nebo naznaƒçuje zmƒõny, st√°hni obsah
            response = requests.get(source.url, headers=headers, timeout=30)
            response.raise_for_status()

            # Vypoƒç√≠tej hash obsahu
            content_hash = hashlib.sha256(response.content).hexdigest()

            # Porovnej s p≈ôedchoz√≠m hashem
            if source.last_content_hash and content_hash == source.last_content_hash:
                logger.info(f"‚úÖ ≈Ω√°dn√© zmƒõny (content hash): {source.url}")
                return {
                    "changed": False,
                    "reason": "content_hash_match",
                    "content_hash": content_hash,
                }

            # Zmƒõny detekovan√©!
            logger.info(f"üî• Zmƒõny detekovan√©: {source.url}")

            return {
                "changed": True,
                "content": response.text,
                "content_hash": content_hash,
                "etag": response.headers.get("ETag"),
                "last_modified": response.headers.get("Last-Modified"),
                "status_code": response.status_code,
                "size": len(response.content),
            }

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi kontrole {source.url}: {e}")
            return {"changed": False, "error": str(e)}

    def update_source_metadata(self, source: MonitoredSource, check_result: Dict[str, Any]):
        """Aktualizuj metadata zdroje po kontrole"""
        source.last_check = datetime.now(timezone.utc)

        if not check_result.get("error"):
            if check_result.get("content_hash"):
                source.last_content_hash = check_result["content_hash"]
            if check_result.get("etag"):
                source.last_etag = check_result["etag"]
            if check_result.get("last_modified"):
                source.last_modified = check_result["last_modified"]


class EventDrivenAnalyzer:
    """Analyz√©r pro event-driven anal√Ωzy"""

    def __init__(self):
        # Pou≈æij lok√°ln√≠ cestu m√≠sto Docker cesty
        self.data_dir = Path.cwd() / "data"
        self.data_dir.mkdir(exist_ok=True)

    async def analyze_new_content(self, source: MonitoredSource, content: str) -> Dict[str, Any]:
        """
        Analyzuj nov√Ω obsah a spus≈• navazuj√≠c√≠ √∫lohy
        """
        try:
            logger.info(f"üß† Event-driven anal√Ωza: {source.url}")

            # Vytvo≈ô doƒçasn√Ω task ID
            task_id = f"autonomous_{int(datetime.now().timestamp())}"

            # Spus≈• scraping √∫lohu pro nov√Ω obsah
            scrape_result = await self._trigger_scraping(source.url, task_id, content)

            if not scrape_result.get("success"):
                return {"success": False, "error": f"Scraping failed: {scrape_result.get('error')}"}

            # Spus≈• zpracov√°n√≠ s Knowledge Graph
            processing_result = await self._trigger_processing(
                scrape_result["output_file"], task_id, use_knowledge_graph=True
            )

            # Analyzuj v√Ωsledky pro potenci√°ln√≠ upozornƒõn√≠
            alerts = await self._analyze_for_alerts(processing_result, source)

            return {
                "success": True,
                "task_id": task_id,
                "scrape_result": scrape_result,
                "processing_result": processing_result,
                "alerts_generated": len(alerts),
                "alerts": alerts,
            }

        except Exception as e:
            logger.error(f"‚ùå Event-driven anal√Ωza selhala: {e}")
            return {"success": False, "error": str(e)}

    async def _trigger_scraping(self, url: str, task_id: str, content: str) -> Dict[str, Any]:
        """Spus≈• scraping √∫lohu"""
        try:
            # P≈ô√≠mo zpracuj obsah m√≠sto opƒõtovn√©ho stahov√°n√≠
            temp_file = self.data_dir / f"temp_content_{task_id}.html"
            with open(temp_file, "w", encoding="utf-8") as f:
                f.write(content)

            # Vytvo≈ô parquet se z√°kladn√≠mi daty
            data = [
                {
                    "url": url,
                    "content": content,
                    "metadata": json.dumps(
                        {
                            "scraped_at": datetime.now(timezone.utc).isoformat(),
                            "autonomous": True,
                            "task_id": task_id,
                        }
                    ),
                    "task_id": task_id,
                }
            ]

            df = pl.DataFrame(data)
            output_file = self.data_dir / f"scraped_autonomous_{task_id}.parquet"
            df.write_parquet(output_file)

            return {"success": True, "output_file": str(output_file), "records": 1}

        except Exception as e:
            logger.error(f"‚ùå Autonomous scraping error: {e}")
            return {"success": False, "error": str(e)}

    async def _trigger_processing(
        self, file_path: str, task_id: str, use_knowledge_graph: bool = True
    ) -> Dict[str, Any]:
        """Spus≈• zpracov√°n√≠ dat"""
        try:
            if use_knowledge_graph:
                # Spus≈• processing s Knowledge Graph
                result = process_with_knowledge_graph.send(file_path, task_id)
                return {"success": True, "knowledge_graph": True, "dramatiq_result": str(result)}
            else:
                # Spus≈• standardn√≠ enhanced processing
                result = process_scraped_data_enhanced.send(file_path, task_id)
                return {"success": True, "knowledge_graph": False, "dramatiq_result": str(result)}

        except Exception as e:
            logger.error(f"‚ùå Processing trigger error: {e}")
            return {"success": False, "error": str(e)}

    async def _analyze_for_alerts(
        self, processing_result: Dict[str, Any], source: MonitoredSource
    ) -> List[Alert]:
        """Analyzuj v√Ωsledky zpracov√°n√≠ pro potenci√°ln√≠ upozornƒõn√≠"""
        alerts = []

        try:
            # Zkontroluj kl√≠ƒçov√° slova pokud jsou definov√°na
            if source.keywords:
                # Toto by mƒõlo ƒç√≠st z v√Ωsledk≈Ø zpracov√°n√≠
                # Pro demo √∫ƒçely vytvo≈ô√≠me z√°kladn√≠ alert
                alert = Alert(
                    alert_id=f"alert_{int(datetime.now().timestamp())}",
                    rule_id="keyword_watch",
                    title=f"Zmƒõny na monitorovan√© str√°nce: {source.url}",
                    message=f"Detekov√°na zmƒõna na {source.url}. Spu≈°tƒõna automatick√° anal√Ωza.",
                    severity="medium",
                    source_url=source.url,
                    triggered_at=datetime.now(timezone.utc),
                )
                alerts.append(alert)

            return alerts

        except Exception as e:
            logger.error(f"‚ùå Alert analysis error: {e}")
            return []


class AlertManager:
    """Spr√°vce upozornƒõn√≠"""

    def __init__(self):
        # Pou≈æij lok√°ln√≠ cestu m√≠sto Docker cesty
        base_dir = Path.cwd() / "data"
        self.data_dir = base_dir
        self.alerts_file = self.data_dir / "alerts.json"
        self.rules_file = self.data_dir / "alert_rules.json"
        self.data_dir.mkdir(exist_ok=True)

        # Email konfigurace
        self.smtp_server = os.getenv("SMTP_SERVER", "localhost")
        self.smtp_port = int(os.getenv("SMTP_PORT", "587"))
        self.email_user = os.getenv("EMAIL_USER", "")
        self.email_password = os.getenv("EMAIL_PASSWORD", "")
        self.email_from = os.getenv("EMAIL_FROM", "research-bot@localhost")
        self.email_to = os.getenv("EMAIL_TO", "admin@localhost")

        # Telegram konfigurace
        self.telegram_bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
        self.telegram_chat_id = os.getenv("TELEGRAM_CHAT_ID", "")

        self._load_rules()

    def _load_rules(self):
        """Naƒçti pravidla pro upozornƒõn√≠"""
        try:
            if self.rules_file.exists():
                with open(self.rules_file, "r") as f:
                    rules_data = json.load(f)
                    self.rules = [AlertRule(**rule) for rule in rules_data]
            else:
                # Vytvo≈ô z√°kladn√≠ pravidla
                self.rules = [
                    AlertRule(
                        rule_id="change_detected",
                        name="Zmƒõna na monitorovan√© str√°nce",
                        condition_type="page_change",
                        condition_params={},
                        notification_channels=["file", "email"],
                        created_at=datetime.now(timezone.utc),
                    )
                ]
                self._save_rules()

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ pravidel: {e}")
            self.rules = []

    def _save_rules(self):
        """Ulo≈æ pravidla"""
        try:
            with open(self.rules_file, "w") as f:
                json.dump([asdict(rule) for rule in self.rules], f, indent=2, default=str)
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi ukl√°d√°n√≠ pravidel: {e}")

    async def send_alert(self, alert: Alert) -> bool:
        """Po≈°li upozornƒõn√≠ podle konfigurace"""
        success = False

        try:
            # Ulo≈æ alert do souboru
            if await self._send_file_alert(alert):
                success = True

            # Po≈°li email pokud je nakonfigurov√°n
            if self.email_user and await self._send_email_alert(alert):
                success = True

            # Po≈°li Telegram zpr√°vu pokud je nakonfigurov√°no
            if self.telegram_bot_token and await self._send_telegram_alert(alert):
                success = True

            alert.sent = success

            return success

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi odes√≠l√°n√≠ alertu: {e}")
            return False

    async def _send_file_alert(self, alert: Alert) -> bool:
        """Ulo≈æ alert do souboru"""
        try:
            alerts = []

            # Naƒçti existuj√≠c√≠ alerty
            if self.alerts_file.exists():
                with open(self.alerts_file, "r") as f:
                    alerts = json.load(f)

            # P≈ôidej nov√Ω alert
            alerts.append(asdict(alert))

            # Ulo≈æ zpƒõt
            with open(self.alerts_file, "w") as f:
                json.dump(alerts, f, indent=2, default=str)

            logger.info(f"üìù Alert ulo≈æen do souboru: {alert.title}")
            return True

        except Exception as e:
            logger.error(f"‚ùå File alert error: {e}")
            return False

    async def _send_email_alert(self, alert: Alert) -> bool:
        """Po≈°li email alert"""
        try:
            if not self.email_user:
                return False

            msg = MimeMultipart()
            msg["From"] = self.email_from
            msg["To"] = self.email_to
            msg["Subject"] = f"[Research Alert] {alert.title}"

            body = f"""
Upozornƒõn√≠ z autonomn√≠ho v√Ωzkumn√©ho syst√©mu:

N√°zev: {alert.title}
Z√°va≈ænost: {alert.severity}
Zdroj: {alert.source_url}
ƒåas: {alert.triggered_at}

Zpr√°va:
{alert.message}

---
Automaticky generov√°no v√Ωzkumn√Ωm botem
            """

            msg.attach(MimeText(body, "plain", "utf-8"))

            server = smtplib.SMTP(self.smtp_server, self.smtp_port)
            server.starttls()
            server.login(self.email_user, self.email_password)
            server.send_message(msg)
            server.quit()

            logger.info(f"üìß Email alert odesl√°n: {alert.title}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Email alert error: {e}")
            return False

    async def _send_telegram_alert(self, alert: Alert) -> bool:
        """Po≈°li Telegram alert"""
        try:
            if not self.telegram_bot_token:
                return False

            message = f"""
üîî *Research Alert*

üìå {alert.title}
‚ö†Ô∏è Z√°va≈ænost: {alert.severity}
üåê Zdroj: {alert.source_url}
üïê ƒåas: {alert.triggered_at.strftime('%Y-%m-%d %H:%M:%S')}

üìù {alert.message}
            """

            url = f"https://api.telegram.org/bot{self.telegram_bot_token}/sendMessage"
            data = {"chat_id": self.telegram_chat_id, "text": message, "parse_mode": "Markdown"}

            response = requests.post(url, data=data, timeout=10)
            response.raise_for_status()

            logger.info(f"üì± Telegram alert odesl√°n: {alert.title}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Telegram alert error: {e}")
            return False


class AutonomousWorker:
    """Hlavn√≠ autonomn√≠ worker"""

    def __init__(self):
        # Pou≈æij lok√°ln√≠ cestu m√≠sto Docker cesty
        base_dir = Path.cwd() / "data"
        self.data_dir = base_dir
        self.sources_file = self.data_dir / "monitored_sources.json"
        self.data_dir.mkdir(exist_ok=True)

        self.change_detector = ChangeDetector()
        self.event_analyzer = EventDrivenAnalyzer()
        self.alert_manager = AlertManager()

        self.monitored_sources: List[MonitoredSource] = []
        self.running = False

        self._load_sources()
        self._setup_scheduler()

        logger.info("ü§ñ Autonomous Worker inicializov√°n")

    def _load_sources(self):
        """Naƒçti monitorovan√© zdroje"""
        try:
            if self.sources_file.exists():
                with open(self.sources_file, "r") as f:
                    sources_data = json.load(f)
                    self.monitored_sources = [MonitoredSource(**source) for source in sources_data]
                logger.info(f"‚úÖ Naƒçteno {len(self.monitored_sources)} monitorovan√Ωch zdroj≈Ø")
            else:
                # Vytvo≈ô uk√°zkov√© zdroje
                self.monitored_sources = [
                    MonitoredSource(
                        url="https://news.ycombinator.com",
                        check_interval=60,  # ka≈ædou hodinu
                        priority=3,
                        keywords=["security", "privacy", "crypto"],
                    )
                ]
                self._save_sources()

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ zdroj≈Ø: {e}")
            self.monitored_sources = []

    def _save_sources(self):
        """Ulo≈æ monitorovan√© zdroje"""
        try:
            with open(self.sources_file, "w") as f:
                json.dump(
                    [asdict(source) for source in self.monitored_sources], f, indent=2, default=str
                )
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi ukl√°d√°n√≠ zdroj≈Ø: {e}")

    def _setup_scheduler(self):
        """Nastav pl√°novaƒç √∫loh"""
        # Napl√°nuj kontroly podle priorit
        schedule.every(5).minutes.do(self._check_high_priority_sources)
        schedule.every(30).minutes.do(self._check_medium_priority_sources)
        schedule.every(2).hours.do(self._check_low_priority_sources)

        # Napl√°nuj √∫dr≈æbov√© √∫lohy
        schedule.every().day.at("02:00").do(self._cleanup_old_data)
        schedule.every().week.do(self._generate_summary_report)

        logger.info("‚è∞ Pl√°novaƒç √∫loh nastaven")

    def _check_high_priority_sources(self):
        """Zkontroluj vysoce prioritn√≠ zdroje (priority >= 4)"""
        asyncio.create_task(self._check_sources_by_priority(4, 5))

    def _check_medium_priority_sources(self):
        """Zkontroluj st≈ôednƒõ prioritn√≠ zdroje (priority 2-3)"""
        asyncio.create_task(self._check_sources_by_priority(2, 3))

    def _check_low_priority_sources(self):
        """Zkontroluj n√≠zko prioritn√≠ zdroje (priority 1)"""
        asyncio.create_task(self._check_sources_by_priority(1, 1))

    async def _check_sources_by_priority(self, min_priority: int, max_priority: int):
        """Zkontroluj zdroje podle priority"""
        try:
            sources_to_check = [
                source
                for source in self.monitored_sources
                if (
                    min_priority <= source.priority <= max_priority
                    and source.enabled
                    and self._should_check_source(source)
                )
            ]

            if not sources_to_check:
                return

            logger.info(
                f"üîç Kontroluji {len(sources_to_check)} zdroj≈Ø (priorita {min_priority}-{max_priority})"
            )

            for source in sources_to_check:
                await self._check_single_source(source)

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi kontrole zdroj≈Ø: {e}")

    def _should_check_source(self, source: MonitoredSource) -> bool:
        """Zjisti zda je ƒças zkontrolovat zdroj"""
        if not source.last_check:
            return True

        time_since_check = datetime.now(timezone.utc) - source.last_check
        return time_since_check.total_seconds() >= (source.check_interval * 60)

    async def _check_single_source(self, source: MonitoredSource):
        """Zkontroluj jednotliv√Ω zdroj"""
        try:
            logger.info(f"üîç Kontroluji zdroj: {source.url}")

            # Detekuj zmƒõny
            check_result = self.change_detector.check_page_changes(source)

            # Aktualizuj metadata
            self.change_detector.update_source_metadata(source, check_result)

            # Pokud jsou zmƒõny, spus≈• anal√Ωzu
            if check_result.get("changed") and check_result.get("content"):
                logger.info(f"üî• Spou≈°t√≠m anal√Ωzu zmƒõn pro: {source.url}")

                analysis_result = await self.event_analyzer.analyze_new_content(
                    source, check_result["content"]
                )

                # Po≈°li upozornƒõn√≠
                if analysis_result.get("alerts"):
                    for alert in analysis_result["alerts"]:
                        await self.alert_manager.send_alert(alert)

                logger.info(f"‚úÖ Anal√Ωza dokonƒçena pro: {source.url}")

            # Ulo≈æ aktualizovan√© zdroje
            self._save_sources()

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi kontrole zdroje {source.url}: {e}")

    def _cleanup_old_data(self):
        """Vyƒçisti star√° data"""
        try:
            logger.info("üßπ Spou≈°t√≠m cleanup star√Ωch dat")

            # Vyma≈æ soubory star≈°√≠ ne≈æ 30 dn√≠
            cutoff_date = datetime.now() - timedelta(days=30)

            for file_path in self.data_dir.glob("*.parquet"):
                if file_path.stat().st_mtime < cutoff_date.timestamp():
                    file_path.unlink()
                    logger.info(f"üóëÔ∏è Vymaz√°n star√Ω soubor: {file_path}")

        except Exception as e:
            logger.error(f"‚ùå Cleanup error: {e}")

    def _generate_summary_report(self):
        """Generuj t√Ωdenn√≠ souhrn"""
        try:
            logger.info("üìä Generuji t√Ωdenn√≠ souhrn")

            # P≈ôeƒçti alerty z posledn√≠ho t√Ωdne
            week_ago = datetime.now(timezone.utc) - timedelta(days=7)

            recent_alerts = []
            if self.alert_manager.alerts_file.exists():
                with open(self.alert_manager.alerts_file, "r") as f:
                    all_alerts = json.load(f)
                    for alert_data in all_alerts:
                        alert_time = datetime.fromisoformat(
                            alert_data["triggered_at"].replace("Z", "+00:00")
                        )
                        if alert_time >= week_ago:
                            recent_alerts.append(alert_data)

            summary = {
                "period": f"{week_ago.date()} - {datetime.now().date()}",
                "monitored_sources": len(self.monitored_sources),
                "active_sources": len([s for s in self.monitored_sources if s.enabled]),
                "alerts_generated": len(recent_alerts),
                "alerts_by_severity": {},
                "generated_at": datetime.now(timezone.utc).isoformat(),
            }

            # Spoƒç√≠tej alerty podle z√°va≈ænosti
            for alert in recent_alerts:
                severity = alert.get("severity", "unknown")
                summary["alerts_by_severity"][severity] = (
                    summary["alerts_by_severity"].get(severity, 0) + 1
                )

            # Ulo≈æ souhrn
            summary_file = (
                self.data_dir / f"weekly_summary_{datetime.now().strftime('%Y_%m_%d')}.json"
            )
            with open(summary_file, "w") as f:
                json.dump(summary, f, indent=2)

            logger.info(f"üìä T√Ωdenn√≠ souhrn ulo≈æen: {summary_file}")

        except Exception as e:
            logger.error(f"‚ùå Summary generation error: {e}")

    def add_monitored_source(
        self, url: str, check_interval: int = 60, priority: int = 1, keywords: List[str] = None
    ) -> bool:
        """P≈ôidej nov√Ω monitorovan√Ω zdroj"""
        try:
            source = MonitoredSource(
                url=url, check_interval=check_interval, priority=priority, keywords=keywords or []
            )

            self.monitored_sources.append(source)
            self._save_sources()

            logger.info(f"‚úÖ P≈ôid√°n nov√Ω monitorovan√Ω zdroj: {url}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi p≈ôid√°v√°n√≠ zdroje: {e}")
            return False

    def start(self):
        """Spus≈• autonomn√≠ worker"""
        self.running = True
        logger.info("üöÄ Autonomous Worker spu≈°tƒõn")

        def run_scheduler():
            while self.running:
                schedule.run_pending()
                time.sleep(60)  # Kontroluj ka≈ædou minutu

        # Spus≈• pl√°novaƒç v separ√°tn√≠m threadu
        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()

        logger.info("‚è∞ Pl√°novaƒç spu≈°tƒõn v background threadu")

    def stop(self):
        """Zastav autonomn√≠ worker"""
        self.running = False
        logger.info("üõë Autonomous Worker zastaven")


# Dramatiq actors pro autonomn√≠ operace
@dramatiq.actor(queue_name="autonomous")
def check_monitored_sources() -> Dict[str, Any]:
    """Dramatiq actor pro kontrolu monitorovan√Ωch zdroj≈Ø"""
    worker = AutonomousWorker()
    # Spus≈• jednor√°zovou kontrolu v≈°ech zdroj≈Ø
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(worker._check_sources_by_priority(1, 5))
    finally:
        loop.close()


@dramatiq.actor(queue_name="autonomous")
def add_monitored_source(
    url: str, check_interval: int = 60, priority: int = 1, keywords: List[str] = None
) -> bool:
    """Dramatiq actor pro p≈ôid√°n√≠ monitorovan√©ho zdroje"""
    worker = AutonomousWorker()
    return worker.add_monitored_source(url, check_interval, priority, keywords)


# Global autonomous worker instance
autonomous_worker = AutonomousWorker()

logger.info("ü§ñ Autonomous Worker p≈ôipraven pro Phase 4!")
