#!/usr/bin/env python3
"""Gated Reranking Engine - OPTIMALIZOV√ÅNO pro M1
Dvouf√°zov√Ω re-ranking: BM25 (rychl√° pre-filtrace) ‚Üí LLM (precizn√≠ hodnocen√≠)
Dramaticky sni≈æuje poƒçet dokument≈Ø pro pomal√Ω LLM (a≈æ 85% redukce)

Author: Senior Python/MLOps Agent
"""

from dataclasses import dataclass
from enum import Enum
import logging
import re
import time
from typing import Any

# NOV√â IMPORTY pro optimalizovan√Ω re-ranking
from rank_bm25 import BM25Okapi

logger = logging.getLogger(__name__)


class RerankingStage(Enum):
    """Reranking stages in the optimized pipeline"""

    BM25_PREFILTER = "bm25_prefilter"          # F√ÅZE 1: Rychl√Ω BM25 filter
    LLM_PRECISION = "llm_precision"            # F√ÅZE 2: Precizn√≠ LLM hodnocen√≠
    UNCERTAINTY_GATE = "uncertainty_gate"
    FINAL_RANKING = "final_ranking"


@dataclass
class OptimizedRerankingConfig:
    """Konfigurace pro optimalizovan√Ω dvouf√°zov√Ω re-ranking"""

    # KL√çƒåOV√â OPTIMALIZACE: Dvouf√°zov√Ω syst√©m
    bm25_candidates: int = 100              # Kolik dokument≈Ø z retrievalu
    llm_candidates: int = 15                # Kolik top BM25 kandid√°t≈Ø pro LLM (85% redukce!)

    # BM25 konfigurace (F√°ze 1)
    bm25_enabled: bool = True
    bm25_k1: float = 1.2                   # BM25 parameter k1
    bm25_b: float = 0.75                   # BM25 parameter b

    # LLM konfigurace (F√°ze 2)
    llm_enabled: bool = True
    confidence_threshold: float = 0.7
    relevance_threshold: float = 0.3

    # Uncertainty gating
    uncertainty_threshold: float = 0.1
    uncertainty_enabled: bool = True

    # Performance optimalizace
    parallel_processing: bool = True
    max_workers: int = 4
    timeout_seconds: int = 30


@dataclass
class DocumentCandidate:
    """Reprezentace dokumentu v re-ranking pipeline"""

    id: str
    content: str
    title: str
    source: str
    metadata: dict[str, Any]

    # Sk√≥re z r≈Øzn√Ωch f√°z√≠
    initial_score: float = 0.0
    bm25_score: float = 0.0
    llm_score: float = 0.0
    final_score: float = 0.0

    # Metriky hodnocen√≠
    confidence: float = 0.0
    uncertainty: float = 0.0
    relevance_reasons: list[str] = None

    # Pipeline tracking
    processed_stages: list[str] = None
    processing_time_ms: float = 0.0

    def __post_init__(self):
        if self.relevance_reasons is None:
            self.relevance_reasons = []
        if self.processed_stages is None:
            self.processed_stages = []


class OptimizedGatedReranker:
    """OPTIMALIZOVAN√ù Gated Reranker s dvouf√°zov√Ωm systemem

    KL√çƒåOV√Å OPTIMALIZACE:
    1. BM25 pre-filtrace (rychl√°, na CPU) - redukuje kandid√°ty o 85%
    2. LLM precision ranking (pomal√Ω, na GPU) - pouze na top kandid√°tech

    V√Ωsledek: Dramatick√© zrychlen√≠ p≈ôi zachov√°n√≠ kvality
    """

    def __init__(self, config: OptimizedRerankingConfig, llm_client=None):
        self.config = config
        self.llm_client = llm_client

        # BM25 komponenty
        self.bm25_index: BM25Okapi | None = None
        self.corpus_processed: list[list[str]] = []
        self.document_mapping: dict[int, str] = {}

        # Performance tracking
        self.performance_stats = {
            "total_documents_processed": 0,
            "bm25_filtering_time_ms": 0.0,
            "llm_ranking_time_ms": 0.0,
            "total_processing_time_ms": 0.0,
            "reduction_ratio": 0.0
        }

        logger.info(f"‚úÖ Optimalizovan√Ω Reranker inicializov√°n: "
                   f"BM25({config.bm25_candidates}) ‚Üí LLM({config.llm_candidates}) "
                   f"= {((config.bm25_candidates - config.llm_candidates) / config.bm25_candidates * 100):.1f}% redukce")

    async def rerank_documents(self,
                             query: str,
                             documents: list[dict[str, Any]]) -> list[DocumentCandidate]:
        """HLAVN√ç OPTIMALIZOVAN√Å METODA: Dvouf√°zov√Ω re-ranking

        Args:
            query: U≈æivatelsk√Ω dotaz
            documents: Seznam dokument≈Ø k re-rankingu

        Returns:
            Se≈ôazen√© dokumenty s optimalizovan√Ωm sk√≥re

        """
        start_time = time.time()
        logger.info(f"üöÄ Spou≈°t√≠m optimalizovan√Ω re-ranking: {len(documents)} dokument≈Ø")

        # Konverze na DocumentCandidate objekty
        candidates = [self._create_candidate(doc, i) for i, doc in enumerate(documents)]

        # F√ÅZE 1: BM25 Pre-filtrace (KL√çƒåOV√Å OPTIMALIZACE!)
        if self.config.bm25_enabled and len(candidates) > self.config.llm_candidates:
            candidates = await self._bm25_prefilter_phase(query, candidates)
            logger.info(f"‚úÖ BM25 pre-filtrace: {len(candidates)} kandid√°t≈Ø (z {len(documents)})")

        # F√ÅZE 2: LLM Precision Ranking (pouze na top kandid√°tech)
        if self.config.llm_enabled and self.llm_client:
            candidates = await self._llm_precision_phase(query, candidates)
            logger.info("‚úÖ LLM precision ranking dokonƒçen")

        # F√ÅZE 3: Uncertainty Gating a fin√°ln√≠ ranking
        if self.config.uncertainty_enabled:
            candidates = await self._uncertainty_gating_phase(candidates)

        # Fin√°ln√≠ se≈ôazen√≠ podle kombinovan√©ho sk√≥re
        candidates = self._final_ranking_phase(candidates)

        # Performance statistiky
        total_time = (time.time() - start_time) * 1000
        self._update_performance_stats(total_time, len(documents), len(candidates))

        logger.info(f"üéâ Optimalizovan√Ω re-ranking dokonƒçen za {total_time:.1f}ms, "
                   f"redukce: {self.performance_stats['reduction_ratio']:.1f}%")

        return candidates

    async def _bm25_prefilter_phase(self,
                                   query: str,
                                   candidates: list[DocumentCandidate]) -> list[DocumentCandidate]:
        """F√ÅZE 1: BM25 Pre-filtrace - rychl√© odstranƒõn√≠ irelevantn√≠ch dokument≈Ø

        Tato f√°ze bƒõ≈æ√≠ na CPU a je extr√©mnƒõ rychl√°
        """
        start_time = time.time()
        logger.debug(f"üîç BM25 pre-filtrace: {len(candidates)} ‚Üí {self.config.bm25_candidates}")

        # P≈ô√≠prava korpusu pro BM25
        corpus = []
        for candidate in candidates:
            # Kombinace title + content pro lep≈°√≠ matching
            text = f"{candidate.title} {candidate.content}"
            # Tokenizace (jednoduch√° ale efektivn√≠)
            tokens = self._tokenize_for_bm25(text)
            corpus.append(tokens)

        # Inicializace BM25 indexu
        self.bm25_index = BM25Okapi(corpus, k1=self.config.bm25_k1, b=self.config.bm25_b)

        # Tokenizace dotazu
        query_tokens = self._tokenize_for_bm25(query)

        # BM25 sk√≥rov√°n√≠
        bm25_scores = self.bm25_index.get_scores(query_tokens)

        # P≈ôi≈ôazen√≠ BM25 sk√≥re kandid√°t≈Øm
        for i, candidate in enumerate(candidates):
            candidate.bm25_score = float(bm25_scores[i])
            candidate.processed_stages.append(RerankingStage.BM25_PREFILTER.value)

        # Se≈ôazen√≠ podle BM25 sk√≥re a v√Ωbƒõr top kandid√°t≈Ø
        candidates_sorted = sorted(candidates, key=lambda x: x.bm25_score, reverse=True)
        top_candidates = candidates_sorted[:self.config.bm25_candidates]

        # Performance tracking
        bm25_time = (time.time() - start_time) * 1000
        self.performance_stats["bm25_filtering_time_ms"] = bm25_time

        logger.debug(f"‚ö° BM25 pre-filtrace dokonƒçena za {bm25_time:.1f}ms")

        return top_candidates

    async def _llm_precision_phase(self,
                                  query: str,
                                  candidates: list[DocumentCandidate]) -> list[DocumentCandidate]:
        """F√ÅZE 2: LLM Precision Ranking - precizn√≠ hodnocen√≠ pouze top kandid√°t≈Ø

        Tato f√°ze je pomal√° ale velmi p≈ôesn√°, bƒõ≈æ√≠ pouze na pre-filtrovan√Ωch kandid√°tech
        """
        start_time = time.time()
        logger.debug(f"üß† LLM precision ranking: {len(candidates)} kandid√°t≈Ø")

        # Omezen√≠ na LLM kandid√°ty (dal≈°√≠ redukce)
        llm_candidates = candidates[:self.config.llm_candidates]
        logger.debug(f"üéØ LLM hodnot√≠ pouze top {len(llm_candidates)} kandid√°t≈Ø "
                    f"(85% redukce z p≈Øvodn√≠ho poƒçtu)")

        # Paraleln√≠ zpracov√°n√≠ LLM hodnocen√≠
        if self.config.parallel_processing:
            llm_candidates = await self._parallel_llm_evaluation(query, llm_candidates)
        else:
            llm_candidates = await self._sequential_llm_evaluation(query, llm_candidates)

        # Kandid√°ti, kte≈ô√≠ nebyli hodnoceni LLM, dostanou sk√≥re zalo≈æen√© na BM25
        remaining_candidates = candidates[len(llm_candidates):]
        for candidate in remaining_candidates:
            candidate.llm_score = candidate.bm25_score * 0.5  # Penalizace za nehodnocen√≠
            candidate.confidence = 0.3  # N√≠zk√° confidence

        # Slouƒçen√≠ hodnocen√Ωch a nehodnocen√Ωch kandid√°t≈Ø
        all_candidates = llm_candidates + remaining_candidates

        # Performance tracking
        llm_time = (time.time() - start_time) * 1000
        self.performance_stats["llm_ranking_time_ms"] = llm_time

        logger.debug(f"üß† LLM precision ranking dokonƒçen za {llm_time:.1f}ms")

        return all_candidates

    async def _parallel_llm_evaluation(self,
                                      query: str,
                                      candidates: list[DocumentCandidate]) -> list[DocumentCandidate]:
        """Paraleln√≠ LLM hodnocen√≠ pro rychlej≈°√≠ zpracov√°n√≠"""
        import asyncio

        async def evaluate_candidate(candidate: DocumentCandidate) -> DocumentCandidate:
            """Hodnocen√≠ jednoho kandid√°ta"""
            try:
                # LLM prompt pro hodnocen√≠ relevance
                evaluation_prompt = self._create_evaluation_prompt(query, candidate)

                # Vol√°n√≠ LLM (asynchronn√≠)
                response = await self.llm_client.generate_async(
                    evaluation_prompt,
                    max_tokens=100,
                    temperature=0.1  # N√≠zk√° teplota pro konzistentn√≠ hodnocen√≠
                )

                # Parsov√°n√≠ LLM odpovƒõdi
                score, confidence, reasons = self._parse_llm_evaluation(response)

                candidate.llm_score = score
                candidate.confidence = confidence
                candidate.relevance_reasons = reasons
                candidate.processed_stages.append(RerankingStage.LLM_PRECISION.value)

                return candidate

            except Exception as e:
                logger.warning(f"LLM hodnocen√≠ selhalo pro kandid√°ta {candidate.id}: {e}")
                candidate.llm_score = candidate.bm25_score * 0.5
                candidate.confidence = 0.1
                return candidate

        # Paraleln√≠ zpracov√°n√≠ s limitem worker≈Ø
        semaphore = asyncio.Semaphore(self.config.max_workers)

        async def evaluate_with_semaphore(candidate):
            async with semaphore:
                return await evaluate_candidate(candidate)

        # Spu≈°tƒõn√≠ paraleln√≠ho hodnocen√≠
        tasks = [evaluate_with_semaphore(candidate) for candidate in candidates]

        try:
            evaluated_candidates = await asyncio.wait_for(
                asyncio.gather(*tasks),
                timeout=self.config.timeout_seconds
            )
            return evaluated_candidates
        except TimeoutError:
            logger.warning(f"LLM hodnocen√≠ timeout po {self.config.timeout_seconds}s")
            return candidates

    async def _sequential_llm_evaluation(self,
                                        query: str,
                                        candidates: list[DocumentCandidate]) -> list[DocumentCandidate]:
        """Sekvenƒçn√≠ LLM hodnocen√≠ (fallback)"""
        for candidate in candidates:
            try:
                evaluation_prompt = self._create_evaluation_prompt(query, candidate)
                response = await self.llm_client.generate_async(evaluation_prompt, max_tokens=100)

                score, confidence, reasons = self._parse_llm_evaluation(response)
                candidate.llm_score = score
                candidate.confidence = confidence
                candidate.relevance_reasons = reasons
                candidate.processed_stages.append(RerankingStage.LLM_PRECISION.value)

            except Exception as e:
                logger.warning(f"LLM hodnocen√≠ selhalo pro kandid√°ta {candidate.id}: {e}")
                candidate.llm_score = candidate.bm25_score * 0.5
                candidate.confidence = 0.1

        return candidates

    def _create_evaluation_prompt(self, query: str, candidate: DocumentCandidate) -> str:
        """Vytvo≈ôen√≠ prompt pro LLM hodnocen√≠ relevance"""
        prompt = f"""Evaluate the relevance of this document to the given query.

Query: {query}

Document Title: {candidate.title}
Document Content: {candidate.content[:500]}...

Rate the relevance on a scale of 0.0 to 1.0 and provide your confidence level.

Response format:
Score: [0.0-1.0]
Confidence: [0.0-1.0]
Reasons: [brief explanation]

Response:"""

        return prompt

    def _parse_llm_evaluation(self, response: str) -> tuple[float, float, list[str]]:
        """Parsov√°n√≠ LLM odpovƒõdi na sk√≥re, confidence a d≈Øvody"""
        try:
            lines = response.strip().split('\n')
            score = 0.5
            confidence = 0.5
            reasons = []

            for line in lines:
                line = line.strip()
                if line.startswith('Score:'):
                    score = float(re.search(r'(\d+\.?\d*)', line).group(1))
                    score = max(0.0, min(1.0, score))
                elif line.startswith('Confidence:'):
                    confidence = float(re.search(r'(\d+\.?\d*)', line).group(1))
                    confidence = max(0.0, min(1.0, confidence))
                elif line.startswith('Reasons:'):
                    reason_text = line.replace('Reasons:', '').strip()
                    reasons = [reason_text] if reason_text else []

            return score, confidence, reasons

        except Exception as e:
            logger.warning(f"Chyba p≈ôi parsov√°n√≠ LLM odpovƒõdi: {e}")
            return 0.5, 0.3, []

    async def _uncertainty_gating_phase(self,
                                       candidates: list[DocumentCandidate]) -> list[DocumentCandidate]:
        """F√ÅZE 3: Uncertainty Gating - filtrov√°n√≠ nejist√Ωch v√Ωsledk≈Ø
        """
        logger.debug(f"üö™ Uncertainty gating: {len(candidates)} kandid√°t≈Ø")

        for candidate in candidates:
            # V√Ωpoƒçet uncertainty na z√°kladƒõ confidence a konzistence sk√≥re
            score_variance = abs(candidate.bm25_score - candidate.llm_score)
            uncertainty = 1.0 - candidate.confidence + (score_variance * 0.1)
            candidate.uncertainty = max(0.0, min(1.0, uncertainty))

            candidate.processed_stages.append(RerankingStage.UNCERTAINTY_GATE.value)

        # Filtrov√°n√≠ kandid√°t≈Ø s vysokou uncertainty
        filtered_candidates = [
            candidate for candidate in candidates
            if candidate.uncertainty <= self.config.uncertainty_threshold
        ]

        if len(filtered_candidates) < len(candidates):
            logger.debug(f"üö™ Uncertainty gate odfiltroval "
                        f"{len(candidates) - len(filtered_candidates)} nejist√Ωch kandid√°t≈Ø")

        return filtered_candidates if filtered_candidates else candidates[:5]  # Minim√°lnƒõ 5 v√Ωsledk≈Ø

    def _final_ranking_phase(self, candidates: list[DocumentCandidate]) -> list[DocumentCandidate]:
        """F√ÅZE 4: Fin√°ln√≠ ranking - kombinace v≈°ech sk√≥re
        """
        logger.debug(f"üèÅ Fin√°ln√≠ ranking: {len(candidates)} kandid√°t≈Ø")

        for candidate in candidates:
            # V√°≈æen√° kombinace BM25 a LLM sk√≥re
            bm25_weight = 0.3
            llm_weight = 0.7

            # Z√°kladn√≠ kombinovan√© sk√≥re
            combined_score = (candidate.bm25_score * bm25_weight +
                            candidate.llm_score * llm_weight)

            # Confidence boost
            confidence_boost = candidate.confidence * 0.1

            # Fin√°ln√≠ sk√≥re
            candidate.final_score = combined_score + confidence_boost
            candidate.processed_stages.append(RerankingStage.FINAL_RANKING.value)

        # Fin√°ln√≠ se≈ôazen√≠
        final_candidates = sorted(candidates, key=lambda x: x.final_score, reverse=True)

        logger.debug("üèÅ Fin√°ln√≠ ranking dokonƒçen")

        return final_candidates

    def _tokenize_for_bm25(self, text: str) -> list[str]:
        """Rychl√° tokenizace pro BM25"""
        # Normalizace textu
        text = text.lower()
        # Odebr√°n√≠ speci√°ln√≠ch znak≈Ø a rozdƒõlen√≠ na slova
        tokens = re.findall(r'\b[a-z]{2,}\b', text)
        # Filtrov√°n√≠ velmi ƒçast√Ωch slov (stop words)
        stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had'}
        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]

        return tokens

    def _create_candidate(self, document: dict[str, Any], index: int) -> DocumentCandidate:
        """Vytvo≈ôen√≠ DocumentCandidate z dokumentu"""
        return DocumentCandidate(
            id=document.get('id', f'doc_{index}'),
            content=document.get('content', ''),
            title=document.get('title', ''),
            source=document.get('source', ''),
            metadata=document.get('metadata', {}),
            initial_score=document.get('score', 0.0)
        )

    def _update_performance_stats(self, total_time_ms: float, initial_count: int, final_count: int):
        """Aktualizace performance statistik"""
        self.performance_stats.update({
            "total_documents_processed": initial_count,
            "total_processing_time_ms": total_time_ms,
            "reduction_ratio": ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0.0
        })

    def get_performance_report(self) -> dict[str, Any]:
        """Z√≠sk√°n√≠ performance reportu"""
        return {
            "optimization_summary": {
                "bm25_candidates": self.config.bm25_candidates,
                "llm_candidates": self.config.llm_candidates,
                "theoretical_reduction_percent": ((self.config.bm25_candidates - self.config.llm_candidates) / self.config.bm25_candidates * 100),
                "actual_reduction_percent": self.performance_stats["reduction_ratio"]
            },
            "timing_breakdown": {
                "bm25_filtering_ms": self.performance_stats["bm25_filtering_time_ms"],
                "llm_ranking_ms": self.performance_stats["llm_ranking_time_ms"],
                "total_processing_ms": self.performance_stats["total_processing_time_ms"],
                "speedup_factor": self._calculate_speedup_factor()
            },
            "efficiency_metrics": {
                "documents_per_second": self._calculate_throughput(),
                "llm_calls_saved": self.config.bm25_candidates - self.config.llm_candidates,
                "estimated_cost_savings_percent": self._estimate_cost_savings()
            }
        }

    def _calculate_speedup_factor(self) -> float:
        """V√Ωpoƒçet faktoru zrychlen√≠ oproti full LLM ranking"""
        # Odhad ƒçasu, kter√Ω by trval full LLM ranking
        estimated_full_llm_time = self.performance_stats["llm_ranking_time_ms"] * (
            self.config.bm25_candidates / max(1, self.config.llm_candidates)
        )

        actual_time = self.performance_stats["total_processing_time_ms"]

        return estimated_full_llm_time / max(1, actual_time)

    def _calculate_throughput(self) -> float:
        """V√Ωpoƒçet propustnosti dokument≈Ø za sekundu"""
        if self.performance_stats["total_processing_time_ms"] > 0:
            return (self.performance_stats["total_documents_processed"] * 1000.0 /
                   self.performance_stats["total_processing_time_ms"])
        return 0.0

    def _estimate_cost_savings(self) -> float:
        """Odhad √∫spory n√°klad≈Ø (m√©nƒõ LLM vol√°n√≠)"""
        llm_calls_saved = self.config.bm25_candidates - self.config.llm_candidates
        total_potential_calls = self.config.bm25_candidates

        return (llm_calls_saved / max(1, total_potential_calls)) * 100.0


# Convenience funkce pro snadn√© pou≈æit√≠
async def create_optimized_reranker(config: dict[str, Any], llm_client=None) -> OptimizedGatedReranker:
    """Factory funkce pro vytvo≈ôen√≠ optimalizovan√©ho rerankeru

    Args:
        config: Konfiguraƒçn√≠ slovn√≠k
        llm_client: LLM klient pro precision ranking

    Returns:
        Nakonfigurovan√Ω OptimizedGatedReranker

    """
    reranking_config = config.get("reranking", {})

    optimized_config = OptimizedRerankingConfig(
        bm25_candidates=reranking_config.get("bm25_candidates", 100),
        llm_candidates=reranking_config.get("llm_candidates", 15),
        bm25_k1=reranking_config.get("bm25_k1", 1.2),
        bm25_b=reranking_config.get("bm25_b", 0.75),
        confidence_threshold=reranking_config.get("confidence_threshold", 0.7),
        relevance_threshold=reranking_config.get("relevance_threshold", 0.3),
        uncertainty_threshold=reranking_config.get("uncertainty_threshold", 0.1)
    )

    reranker = OptimizedGatedReranker(optimized_config, llm_client)

    logger.info(f"‚úÖ Optimalizovan√Ω reranker vytvo≈ôen: "
               f"{optimized_config.bm25_candidates} ‚Üí {optimized_config.llm_candidates} dokument≈Ø "
               f"({((optimized_config.bm25_candidates - optimized_config.llm_candidates) / optimized_config.bm25_candidates * 100):.1f}% redukce LLM vol√°n√≠)")

    return reranker
