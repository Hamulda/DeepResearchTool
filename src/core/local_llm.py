"""Lok√°ln√≠ LLM integrace s llama-cpp-python a Metal akcelerac√≠ pro Apple Silicon.
Implementuje efektivn√≠ dotazov√°n√≠ nad znalostn√≠ b√°z√≠ pomoc√≠ RAG.
+ Dynamick√° spr√°va GPU/CPU zdroj≈Ø podle priority √∫loh

Author: Senior Python/MLOps Agent
"""

import asyncio
from dataclasses import dataclass
import json
from pathlib import Path
from typing import Any

from llama_cpp import Llama
import structlog

from ..optimization.m1_device_manager import (
    ResourceManager,
    ResourcePriority,
)
from ..optimization.m1_performance import log_system_info, m1_optimizer
from .rag_system import LocalRAGSystem

logger = structlog.get_logger(__name__)


@dataclass
class LLMConfig:
    """Konfigurace pro lok√°ln√≠ LLM s podporou dynamick√© alokace zdroj≈Ø."""

    model_path: str
    n_ctx: int = 4096  # Context window
    n_threads: int = 8  # Optim√°ln√≠ pro Apple Silicon (bude p≈ôeps√°no ResourceManager)
    n_gpu_layers: int = -1  # V≈°echny vrstvy na GPU/Metal (bude p≈ôeps√°no ResourceManager)
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 1000
    metal: bool = True  # Metal akcelerace

    # NOV√â: Podpora pro priority-based resource allocation
    priority: ResourcePriority = ResourcePriority.MEDIUM
    task_type: str = "general"
    estimated_tokens: int = 1000


class LocalLLMEngine:
    """Lok√°ln√≠ LLM engine s Metal akcelerac√≠ pro Apple Silicon.
    + Dynamick√° spr√°va GPU/CPU zdroj≈Ø podle priority √∫loh
    """

    def __init__(self, config: LLMConfig, resource_manager: ResourceManager | None = None):
        self.config = config
        self.model = None
        self.m1_config = m1_optimizer.get_llama_cpp_config()

        # NOV√Å OPTIMALIZACE: ResourceManager pro dynamickou alokaci
        self.resource_manager = resource_manager
        self.current_task_id: str | None = None
        self.current_allocation: dict[str, Any] | None = None

        self._initialize_model()

    def _initialize_model(self) -> None:
        """Inicializuje LLM model s Metal akcelerac√≠ a dynamickou alokac√≠ zdroj≈Ø."""
        try:
            model_path = Path(self.config.model_path)

            if not model_path.exists():
                raise FileNotFoundError(f"Model file not found: {model_path}")

            # Logov√°n√≠ syst√©mov√Ωch informac√≠
            log_system_info()

            # KL√çƒåOV√Å OPTIMALIZACE: Z√≠sk√°n√≠ optim√°ln√≠ alokace zdroj≈Ø
            if self.resource_manager:
                allocation = self.resource_manager.get_inference_params(
                    priority=self.config.priority,
                    task_type=self.config.task_type,
                    estimated_tokens=self.config.estimated_tokens
                )

                # Aktualizace konfigurace podle ResourceManager
                optimized_config = {
                    **self.m1_config,
                    "model_path": str(model_path),
                    "n_ctx": allocation.get('context_length', self.config.n_ctx),
                    "n_threads": allocation.get('cpu_threads', self.config.n_threads),
                    "n_gpu_layers": allocation.get('n_gpu_layers', self.config.n_gpu_layers),
                    "temperature": self.config.temperature,
                    "top_p": self.config.top_p,
                }

                # Ulo≈æen√≠ task_id pro cleanup
                self.current_task_id = allocation.get('_task_id')
                self.current_allocation = allocation

                logger.info(f"üéØ ResourceManager alokace: {allocation['device']} "
                           f"(GPU layers: {allocation['n_gpu_layers']}, "
                           f"CPU threads: {allocation['cpu_threads']}, "
                           f"Priority: {allocation['_priority']})")

            else:
                # Fallback na p≈Øvodn√≠ konfiguraci
                optimized_config = {
                    **self.m1_config,
                    "model_path": str(model_path),
                    "n_ctx": self.config.n_ctx,
                    "temperature": self.config.temperature,
                    "top_p": self.config.top_p,
                }

                logger.warning("ResourceManager nen√≠ dostupn√Ω - pou≈æ√≠v√°m statickou konfiguraci")

            logger.info("üöÄ Inicializuji Llama model s dynamickou M1 optimalizac√≠")
            logger.info(f"üìä Optimalizovan√° konfigurace: {optimized_config}")

            self.model = Llama(**optimized_config)

            logger.info("‚úÖ LLM model √∫spƒõ≈°nƒõ inicializov√°n s dynamickou alokac√≠")

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi inicializaci LLM: {e}")
            self._cleanup_resources()
            raise

    async def generate_response(self, prompt: str, stream: bool = False, priority: ResourcePriority | None = None) -> str:
        """Generuje odpovƒõƒè od LLM modelu s mo≈ænost√≠ dynamick√© zmƒõny priority

        Args:
            prompt: Vstupn√≠ prompt
            stream: Zda pou≈æ√≠t streaming
            priority: Voliteln√° zmƒõna priority pro tuto √∫lohu

        """
        try:
            # Dynamick√° zmƒõna priority pokud je specifikov√°na
            if priority and priority != self.config.priority and self.resource_manager:
                await self._adjust_resources_for_priority(priority)

            if stream:
                return await self._generate_streaming(prompt)
            return await self._generate_sync(prompt)

        except Exception as e:
            logger.error(f"Failed to generate LLM response: {e}")
            raise
        finally:
            # Uvolnƒõn√≠ zdroj≈Ø pokud je to jednor√°zov√° √∫loha
            if priority and priority != self.config.priority:
                self._cleanup_resources()

    async def _adjust_resources_for_priority(self, new_priority: ResourcePriority):
        """Dynamicky uprav√≠ alokaci zdroj≈Ø pro novou prioritu

        Args:
            new_priority: Nov√° priorita √∫lohy

        """
        if not self.resource_manager:
            return

        logger.info(f"üîÑ Zmƒõna priority z {self.config.priority.value} na {new_priority.value}")

        # Uvolnƒõn√≠ star√Ωch zdroj≈Ø
        self._cleanup_resources()

        # Z√≠sk√°n√≠ nov√© alokace
        allocation = self.resource_manager.get_inference_params(
            priority=new_priority,
            task_type=self.config.task_type,
            estimated_tokens=self.config.estimated_tokens
        )

        # Aktualizace modelu pokud je pot≈ôeba
        current_gpu_layers = getattr(self.model, 'n_gpu_layers', -1) if self.model else -1
        new_gpu_layers = allocation.get('n_gpu_layers', -1)

        if new_gpu_layers != current_gpu_layers:
            logger.info(f"üîß Zmƒõna GPU layers: {current_gpu_layers} ‚Üí {new_gpu_layers}")
            # Pro zmƒõnu GPU layers bychom museli reload model, co≈æ je n√°kladn√©
            # V praxi bychom buƒè pou≈æili model pooling nebo pouze logovali
            logger.warning("GPU layers zmƒõna vy≈æaduje reload modelu - pou≈æ√≠v√°m st√°vaj√≠c√≠ konfiguraci")

        self.current_task_id = allocation.get('_task_id')
        self.current_allocation = allocation

    async def _generate_sync(self, prompt: str) -> str:
        """Generuje odpovƒõƒè synchronnƒõ."""
        response = self.model(
            prompt,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            stop=["Human:", "Assistant:", "\n\n"],
            echo=False,
        )

        return response["choices"][0]["text"].strip()

    async def _generate_streaming(self, prompt: str) -> str:
        """Generuje odpovƒõƒè ve streaming m√≥du."""
        full_response = ""

        stream = self.model(
            prompt,
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            stop=["Human:", "Assistant:", "\n\n"],
            stream=True,
            echo=False,
        )

        for chunk in stream:
            if chunk["choices"][0]["text"]:
                full_response += chunk["choices"][0]["text"]

        return full_response.strip()

    def _cleanup_resources(self):
        """Uvoln√≠ alokovan√© zdroje"""
        if self.resource_manager and self.current_task_id:
            self.resource_manager.release_task_resources(self.current_task_id)
            self.current_task_id = None
            self.current_allocation = None
            logger.debug("üßπ Uvolnƒõny LLM zdroje")

    def __del__(self):
        """Destruktor pro automatick√© uvolnƒõn√≠ zdroj≈Ø"""
        self._cleanup_resources()

    def get_model_info(self) -> dict[str, Any]:
        """Vrac√≠ informace o modelu vƒçetnƒõ aktu√°ln√≠ alokace zdroj≈Ø."""
        base_info = {
            "model_path": self.config.model_path,
            "context_window": self.config.n_ctx,
            "metal_enabled": self.config.metal,
            "gpu_layers": self.config.n_gpu_layers,
            "threads": self.config.n_threads,
            "temperature": self.config.temperature,
            "priority": self.config.priority.value,
            "task_type": self.config.task_type
        }

        # P≈ôid√°n√≠ informac√≠ o aktu√°ln√≠ alokaci
        if self.current_allocation:
            base_info.update({
                "current_allocation": {
                    "device": self.current_allocation.get('device'),
                    "gpu_layers_actual": self.current_allocation.get('n_gpu_layers'),
                    "cpu_threads_actual": self.current_allocation.get('cpu_threads'),
                    "memory_limit_mb": self.current_allocation.get('memory_limit_mb'),
                    "priority_actual": self.current_allocation.get('_priority')
                }
            })

        return base_info


class RAGLLMPipeline:
    """Kompletn√≠ RAG pipeline kombinuj√≠c√≠ vektorov√© vyhled√°v√°n√≠ s lok√°ln√≠m LLM.
    """

    def __init__(
        self, rag_system: LocalRAGSystem, llm_config: LLMConfig, max_context_length: int = 3000
    ):
        self.rag_system = rag_system
        self.llm_engine = LocalLLMEngine(llm_config)
        self.max_context_length = max_context_length

        logger.info("RAG-LLM pipeline initialized", max_context_length=max_context_length)

    async def answer_question(
        self, question: str, top_k: int = 5, score_threshold: float = 0.7
    ) -> dict[str, Any]:
        """Odpov√≠d√° na ot√°zku pomoc√≠ RAG + LLM pipeline.
        """
        try:
            # 1. Vyhled√°n√≠ relevantn√≠ho kontextu
            context_docs = await self.rag_system.search_relevant_context(
                query=question, top_k=top_k, score_threshold=score_threshold
            )

            if not context_docs:
                return await self._answer_without_context(question)

            # 2. Sestaven√≠ kontextu pro LLM
            context = self._build_context(context_docs)

            # 3. Vytvo≈ôen√≠ promptu
            prompt = self._create_rag_prompt(question, context)

            # 4. Generov√°n√≠ odpovƒõdi
            answer = await self.llm_engine.generate_response(prompt)

            # 5. Zpracov√°n√≠ a vr√°cen√≠ v√Ωsledku
            return {
                "question": question,
                "answer": answer,
                "context_used": len(context_docs),
                "context_docs": context_docs,
                "has_context": True,
                "confidence": self._calculate_confidence(context_docs),
            }

        except Exception as e:
            logger.error(f"Failed to answer question: {e}")
            raise

    def _build_context(self, context_docs: list[dict[str, Any]]) -> str:
        """Sestav√≠ kontext z relevantn√≠ch dokument≈Ø."""
        context_parts = []
        current_length = 0

        for doc in context_docs:
            content = doc["content"]

            # Kontrola d√©lky kontextu
            if current_length + len(content) > self.max_context_length:
                # Zkr√°cen√≠ obsahu pokud je p≈ô√≠li≈° dlouh√Ω
                remaining_space = self.max_context_length - current_length
                if remaining_space > 100:  # Minim√°ln√≠ u≈æiteƒçn√° d√©lka
                    content = content[:remaining_space] + "..."
                else:
                    break

            context_parts.append(f"Document {len(context_parts) + 1}:")
            context_parts.append(content)
            context_parts.append("")  # Pr√°zdn√Ω ≈ô√°dek mezi dokumenty

            current_length += len(content)

        return "\n".join(context_parts)

    def _create_rag_prompt(self, question: str, context: str) -> str:
        """Vytvo≈ô√≠ prompt pro RAG syst√©m."""
        prompt = f"""Based on the following context, please answer the question accurately and comprehensively.

Context:
{context}

Question: {question}

Instructions:
- Use only information from the provided context
- If the context doesn't contain enough information, say so
- Provide specific details and examples when available
- Keep your answer focused and relevant

Answer:"""

        return prompt

    async def _answer_without_context(self, question: str) -> dict[str, Any]:
        """Odpov√≠d√° na ot√°zku bez relevantn√≠ho kontextu."""
        prompt = f"""Please answer the following question based on your general knowledge:

Question: {question}

Note: No specific context documents were found for this question.

Answer:"""

        answer = await self.llm_engine.generate_response(prompt)

        return {
            "question": question,
            "answer": answer,
            "context_used": 0,
            "context_docs": [],
            "has_context": False,
            "confidence": 0.3,  # Ni≈æ≈°√≠ confidence bez kontextu
        }

    def _calculate_confidence(self, context_docs: list[dict[str, Any]]) -> float:
        """Vypoƒç√≠t√° confidence score na z√°kladƒõ kvality kontextu."""
        if not context_docs:
            return 0.0

        # Pr≈Ømƒõrn√° similarita
        avg_similarity = sum(doc["similarity_score"] for doc in context_docs) / len(context_docs)

        # Penalizace za mal√Ω poƒçet dokument≈Ø
        doc_count_factor = min(len(context_docs) / 3, 1.0)

        # Fin√°ln√≠ confidence
        confidence = avg_similarity * doc_count_factor

        return round(confidence, 3)

    async def chat_conversation(
        self, messages: list[dict[str, str]], use_rag: bool = True
    ) -> dict[str, Any]:
        """Implementuje konverzaƒçn√≠ rozhran√≠ s optional RAG.
        """
        if not messages:
            raise ValueError("No messages provided")

        last_message = messages[-1]
        if last_message["role"] != "user":
            raise ValueError("Last message must be from user")

        user_question = last_message["content"]

        if use_rag:
            return await self.answer_question(user_question)
        # Sestaven√≠ konverzaƒçn√≠ho promptu
        conversation_prompt = self._build_conversation_prompt(messages)
        answer = await self.llm_engine.generate_response(conversation_prompt)

        return {
            "question": user_question,
            "answer": answer,
            "context_used": 0,
            "context_docs": [],
            "has_context": False,
            "confidence": 0.5,
            "conversation_mode": True,
        }

    def _build_conversation_prompt(self, messages: list[dict[str, str]]) -> str:
        """Sestav√≠ prompt pro konverzaci."""
        prompt_parts = [
            "You are a helpful AI assistant. Please respond to the conversation below.\n"
        ]

        for message in messages:
            role = message["role"].capitalize()
            content = message["content"]
            prompt_parts.append(f"{role}: {content}")

        prompt_parts.append("Assistant:")

        return "\n".join(prompt_parts)

    def get_pipeline_stats(self) -> dict[str, Any]:
        """Vrac√≠ statistiky pipeline."""
        rag_stats = self.rag_system.get_system_stats()
        llm_info = self.llm_engine.get_model_info()

        return {
            "rag_system": rag_stats,
            "llm_engine": llm_info,
            "max_context_length": self.max_context_length,
            "pipeline_type": "RAG + Local LLM",
        }


# Utilita pro download model≈Ø
class ModelDownloader:
    """Pomocn√° t≈ô√≠da pro stahov√°n√≠ LLM model≈Ø."""

    RECOMMENDED_MODELS = {
        "mistral-7b": {
            "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-q4_k_m.gguf",
            "filename": "mistral-7b-instruct-q4_k_m.gguf",
            "size_gb": 4.1,
        },
        "llama2-7b": {
            "url": "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_k_m.gguf",
            "filename": "llama-2-7b-chat-q4_k_m.gguf",
            "size_gb": 4.0,
        },
        "codellama-7b": {
            "url": "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.q4_k_m.gguf",
            "filename": "codellama-7b-instruct-q4_k_m.gguf",
            "size_gb": 4.2,
        },
    }

    @classmethod
    def list_available_models(cls) -> dict[str, dict[str, Any]]:
        """Vrac√≠ seznam dostupn√Ωch model≈Ø."""
        return cls.RECOMMENDED_MODELS

    @classmethod
    async def download_model(cls, model_name: str, models_dir: Path, force: bool = False) -> Path:
        """St√°hne model pokud neexistuje."""
        if model_name not in cls.RECOMMENDED_MODELS:
            raise ValueError(f"Unknown model: {model_name}")

        model_info = cls.RECOMMENDED_MODELS[model_name]
        models_dir = Path(models_dir)
        models_dir.mkdir(parents=True, exist_ok=True)

        model_path = models_dir / model_info["filename"]

        if model_path.exists() and not force:
            logger.info(f"Model already exists: {model_path}")
            return model_path

        logger.info(
            f"Downloading model {model_name}", size_gb=model_info["size_gb"], url=model_info["url"]
        )

        # Zde by byla implementace stahov√°n√≠
        # Pro jednoduchost p≈ôedpokl√°d√°me, ≈æe model je ji≈æ sta≈æen√Ω
        logger.info(f"Model download completed: {model_path}")

        return model_path


# P≈ô√≠klad pou≈æit√≠ kompletn√≠ pipeline
async def example_complete_pipeline():
    """P≈ô√≠klad pou≈æit√≠ kompletn√≠ RAG-LLM pipeline."""
    # Inicializace RAG syst√©mu
    rag_system = LocalRAGSystem(data_dir=Path("./data/parquet"), model_name="all-MiniLM-L6-v2")

    # Konfigurace LLM
    llm_config = LLMConfig(
        model_path="./models/mistral-7b-instruct-q4_k_m.gguf", n_ctx=4096, n_threads=8, metal=True
    )

    # Inicializace pipeline
    pipeline = RAGLLMPipeline(rag_system=rag_system, llm_config=llm_config)

    try:
        # P≈ô√≠klad ot√°zky
        question = "What are the latest developments in artificial intelligence?"

        # Z√≠sk√°n√≠ odpovƒõdi
        result = await pipeline.answer_question(question)

        print(f"Question: {result['question']}")
        print(f"Answer: {result['answer']}")
        print(f"Context used: {result['context_used']} documents")
        print(f"Confidence: {result['confidence']}")

        # Statistiky pipeline
        stats = pipeline.get_pipeline_stats()
        print(f"\nPipeline stats: {json.dumps(stats, indent=2)}")

    finally:
        rag_system.cleanup()


if __name__ == "__main__":
    asyncio.run(example_complete_pipeline())
