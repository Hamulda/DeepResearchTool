"""ğŸ¤– AutonomnÃ­ vÃ½zkumnÃ½ agent s rekurzivnÃ­ smyÄkou
Implementuje inteligentnÃ­ rozhodovÃ¡nÃ­ o dalÅ¡Ã­ch krocÃ­ch na zÃ¡kladÄ› zjiÅ¡tÄ›nÃ­
"""

import asyncio
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from typing import Any
import uuid

from ..scrapers.web_scraper import WebScraper
from ..storage.document_store import DocumentStore
from ..synthesis.correlation_engine import CorrelationEngine
from ..synthesis.credibility_assessor import CredibilityAssessor
from ..synthesis.deep_pattern_detector import DeepPatternDetector
from ..utils.logging_utils import get_logger

logger = get_logger(__name__)


class TaskType(Enum):
    """Typy ÃºkolÅ¯ pro autonomnÃ­ agenta"""

    SCRAPE = "scrape"
    ANALYZE = "analyze"
    CORRELATE = "correlate"
    VALIDATE = "validate"
    EXPAND_SEARCH = "expand_search"
    DEEP_DIVE = "deep_dive"
    CROSS_REFERENCE = "cross_reference"


class TaskPriority(Enum):
    """Priorita ÃºkolÅ¯"""

    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4


@dataclass
class AgentTask:
    """Struktura Ãºkolu pro autonomnÃ­ agenta"""

    id: str
    task_type: TaskType
    priority: TaskPriority
    parameters: dict[str, Any]
    created_at: datetime
    parent_task_id: str | None = None
    attempts: int = 0
    max_attempts: int = 3
    completed: bool = False
    failed: bool = False
    result: dict[str, Any] | None = None
    credibility_score: float = 0.0

    def to_dict(self) -> dict[str, Any]:
        data = asdict(self)
        data["task_type"] = self.task_type.value
        data["priority"] = self.priority.value
        data["created_at"] = self.created_at.isoformat()
        return data


class TaskQueue:
    """PrioritnÃ­ fronta ÃºkolÅ¯ s inteligentnÃ­m plÃ¡novÃ¡nÃ­m"""

    def __init__(self, max_concurrent_tasks: int = 5):
        self.tasks: list[AgentTask] = []
        self.completed_tasks: list[AgentTask] = []
        self.failed_tasks: list[AgentTask] = []
        self.max_concurrent_tasks = max_concurrent_tasks
        self.running_tasks: set[str] = set()

    def add_task(self, task: AgentTask) -> None:
        """PÅ™idÃ¡ Ãºkol do fronty podle priority"""
        # Kontrola duplicit
        if self._is_duplicate_task(task):
            logger.info(f"DuplicitnÃ­ Ãºkol ignorovÃ¡n: {task.task_type.value}")
            return

        self.tasks.append(task)
        self.tasks.sort(key=lambda t: (t.priority.value, t.created_at))
        logger.info(f"PÅ™idÃ¡n Ãºkol: {task.task_type.value} (priorita: {task.priority.value})")

    def get_next_task(self) -> AgentTask | None:
        """VrÃ¡tÃ­ dalÅ¡Ã­ Ãºkol k vykonÃ¡nÃ­"""
        if len(self.running_tasks) >= self.max_concurrent_tasks:
            return None

        for task in self.tasks:
            if not task.completed and not task.failed and task.id not in self.running_tasks:
                if task.attempts < task.max_attempts:
                    return task
        return None

    def mark_task_running(self, task_id: str) -> None:
        """OznaÄÃ­ Ãºkol jako bÄ›Å¾Ã­cÃ­"""
        self.running_tasks.add(task_id)

    def complete_task(self, task_id: str, result: dict[str, Any]) -> None:
        """OznaÄÃ­ Ãºkol jako dokonÄenÃ½"""
        for task in self.tasks:
            if task.id == task_id:
                task.completed = True
                task.result = result
                self.completed_tasks.append(task)
                self.running_tasks.discard(task_id)
                break

    def fail_task(self, task_id: str, error: str) -> None:
        """OznaÄÃ­ Ãºkol jako neÃºspÄ›Å¡nÃ½"""
        for task in self.tasks:
            if task.id == task_id:
                task.attempts += 1
                if task.attempts >= task.max_attempts:
                    task.failed = True
                    self.failed_tasks.append(task)
                self.running_tasks.discard(task_id)
                logger.warning(f"Ãškol {task_id} selhal: {error}")
                break

    def _is_duplicate_task(self, new_task: AgentTask) -> bool:
        """Kontroluje duplicitnÃ­ Ãºkoly"""
        for task in self.tasks:
            if (
                task.task_type == new_task.task_type
                and task.parameters == new_task.parameters
                and not task.completed
                and not task.failed
            ):
                return True
        return False

    def get_stats(self) -> dict[str, int]:
        """VrÃ¡tÃ­ statistiky fronty"""
        return {
            "pending": len([t for t in self.tasks if not t.completed and not t.failed]),
            "running": len(self.running_tasks),
            "completed": len(self.completed_tasks),
            "failed": len(self.failed_tasks),
        }


class AgenticLoop:
    """ğŸ¤– AutonomnÃ­ vÃ½zkumnÃ½ agent s rekurzivnÃ­ smyÄkou

    Analyzuje zjiÅ¡tÄ›nÃ­ z LLM a generuje novÃ© strukturovanÃ© Ãºkoly
    na zÃ¡kladÄ› dÅ¯vÄ›ryhodnosti a relevance nalezenÃ½ch informacÃ­.
    """

    def __init__(
        self,
        document_store: DocumentStore,
        credibility_assessor: CredibilityAssessor,
        correlation_engine: CorrelationEngine,
        pattern_detector: DeepPatternDetector,
        web_scraper: WebScraper,
        max_iterations: int = 10,
        min_credibility_threshold: float = 0.3,
    ):

        self.document_store = document_store
        self.credibility_assessor = credibility_assessor
        self.correlation_engine = correlation_engine
        self.pattern_detector = pattern_detector
        self.web_scraper = web_scraper

        self.task_queue = TaskQueue()
        self.max_iterations = max_iterations
        self.min_credibility_threshold = min_credibility_threshold
        self.current_iteration = 0
        self.is_running = False

        # Metriky a monitoring
        self.total_tasks_generated = 0
        self.total_discoveries = 0
        self.credibility_scores: list[float] = []

    async def start_research_cycle(
        self, initial_query: str, target_urls: list[str] = None
    ) -> dict[str, Any]:
        """ğŸš€ SpustÃ­ autonomnÃ­ vÃ½zkumnÃ½ cyklus

        Args:
            initial_query: PoÄÃ¡teÄnÃ­ vÃ½zkumnÃ½ dotaz
            target_urls: VolitelnÃ© poÄÃ¡teÄnÃ­ URL pro scraping

        Returns:
            Dict s vÃ½sledky vÃ½zkumu a metrikami

        """
        logger.info(f"ğŸš€ SpouÅ¡tÃ­m autonomnÃ­ vÃ½zkumnÃ½ cyklus pro: {initial_query}")

        self.is_running = True
        self.current_iteration = 0

        # GenerovÃ¡nÃ­ poÄÃ¡teÄnÃ­ch ÃºkolÅ¯
        await self._generate_initial_tasks(initial_query, target_urls)

        try:
            # HlavnÃ­ smyÄka agenta
            while (
                self.is_running
                and self.current_iteration < self.max_iterations
                and self._should_continue_research()
            ):

                await self._execute_iteration()
                self.current_iteration += 1

                # KrÃ¡tkÃ¡ pauza mezi iteracemi
                await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"Chyba v autonomnÃ­ smyÄce: {e}")
        finally:
            self.is_running = False

        return await self._generate_final_report()

    async def _generate_initial_tasks(self, query: str, target_urls: list[str] = None) -> None:
        """Generuje poÄÃ¡teÄnÃ­ Ãºkoly na zÃ¡kladÄ› dotazu"""
        # AnalÃ½za ÃºvodnÃ­ho dotazu pro extrakci entit
        analysis_task = AgentTask(
            id=str(uuid.uuid4()),
            task_type=TaskType.ANALYZE,
            priority=TaskPriority.HIGH,
            parameters={"text": query, "source": "initial_query"},
            created_at=datetime.now(),
        )
        self.task_queue.add_task(analysis_task)

        # ScrapovÃ¡nÃ­ zadanÃ½ch URL
        if target_urls:
            for url in target_urls:
                scrape_task = AgentTask(
                    id=str(uuid.uuid4()),
                    task_type=TaskType.SCRAPE,
                    priority=TaskPriority.HIGH,
                    parameters={"url": url, "depth": 1},
                    created_at=datetime.now(),
                )
                self.task_queue.add_task(scrape_task)

        # RozÅ¡Ã­Å™enÃ© vyhledÃ¡vÃ¡nÃ­ na zÃ¡kladÄ› dotazu
        search_task = AgentTask(
            id=str(uuid.uuid4()),
            task_type=TaskType.EXPAND_SEARCH,
            priority=TaskPriority.MEDIUM,
            parameters={"query": query, "max_results": 10},
            created_at=datetime.now(),
        )
        self.task_queue.add_task(search_task)

    async def _execute_iteration(self) -> None:
        """VykonÃ¡ jednu iteraci agentnÃ­ smyÄky"""
        logger.info(f"ğŸ”„ Iterace {self.current_iteration + 1}/{self.max_iterations}")

        # ParalelnÃ­ vykonÃ¡vÃ¡nÃ­ ÃºkolÅ¯
        running_tasks = []

        # ZÃ­skÃ¡nÃ­ ÃºkolÅ¯ k vykonÃ¡nÃ­
        while len(running_tasks) < self.task_queue.max_concurrent_tasks:
            task = self.task_queue.get_next_task()
            if not task:
                break

            self.task_queue.mark_task_running(task.id)
            running_tasks.append(self._execute_task(task))

        # ÄŒekÃ¡nÃ­ na dokonÄenÃ­ ÃºkolÅ¯
        if running_tasks:
            await asyncio.gather(*running_tasks, return_exceptions=True)

        # GenerovÃ¡nÃ­ novÃ½ch ÃºkolÅ¯ na zÃ¡kladÄ› vÃ½sledkÅ¯
        await self._generate_follow_up_tasks()

    async def _execute_task(self, task: AgentTask) -> None:
        """VykonÃ¡ konkrÃ©tnÃ­ Ãºkol"""
        try:
            logger.info(f"ğŸ¯ VykonÃ¡vÃ¡m Ãºkol: {task.task_type.value}")

            if task.task_type == TaskType.SCRAPE:
                result = await self._execute_scrape_task(task)
            elif task.task_type == TaskType.ANALYZE:
                result = await self._execute_analyze_task(task)
            elif task.task_type == TaskType.CORRELATE:
                result = await self._execute_correlate_task(task)
            elif task.task_type == TaskType.VALIDATE:
                result = await self._execute_validate_task(task)
            elif task.task_type == TaskType.EXPAND_SEARCH:
                result = await self._execute_expand_search_task(task)
            elif task.task_type == TaskType.DEEP_DIVE:
                result = await self._execute_deep_dive_task(task)
            elif task.task_type == TaskType.CROSS_REFERENCE:
                result = await self._execute_cross_reference_task(task)
            else:
                raise ValueError(f"NeznÃ¡mÃ½ typ Ãºkolu: {task.task_type}")

            # HodnocenÃ­ dÅ¯vÄ›ryhodnosti vÃ½sledku
            if result and "content" in result:
                credibility = await self.credibility_assessor.assess_content(
                    result["content"], result.get("url", "")
                )
                result["credibility_score"] = credibility
                task.credibility_score = credibility
                self.credibility_scores.append(credibility)

            self.task_queue.complete_task(task.id, result)
            logger.info(f"âœ… Ãškol dokonÄen: {task.task_type.value}")

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i vykonÃ¡vÃ¡nÃ­ Ãºkolu {task.id}: {e}")
            self.task_queue.fail_task(task.id, str(e))

    async def _execute_scrape_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ scraping Ãºkol"""
        url = task.parameters["url"]
        depth = task.parameters.get("depth", 1)

        content = await self.web_scraper.scrape_page(url)

        # UloÅ¾enÃ­ do document store
        doc_id = await self.document_store.store_document(
            {
                "url": url,
                "content": content,
                "timestamp": datetime.now(),
                "source": "autonomous_scraping",
            }
        )

        return {"url": url, "content": content, "document_id": doc_id, "depth": depth}

    async def _execute_analyze_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ analÃ½zu textu"""
        text = task.parameters["text"]

        # Pattern detection
        patterns = await self.pattern_detector.analyze_text(text)

        # KorelaÄnÃ­ analÃ½za
        correlations = await self.correlation_engine.analyze_text(text)

        return {
            "text": text,
            "patterns": patterns,
            "correlations": correlations,
            "entities_found": len(correlations.get("entities", [])),
            "patterns_found": len(patterns.get("artifacts", [])),
        }

    async def _execute_correlate_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ korelaÄnÃ­ analÃ½zu"""
        entity_id = task.parameters["entity_id"]

        # NajÃ­t souvisejÃ­cÃ­ entity
        related = await self.correlation_engine.find_related_entities(entity_id)

        return {
            "entity_id": entity_id,
            "related_entities": related,
            "correlation_strength": len(related),
        }

    async def _execute_validate_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ validaci informacÃ­"""
        claim = task.parameters["claim"]
        sources = task.parameters.get("sources", [])

        # KÅ™Ã­Å¾ovÃ¡ validace mezi zdroji
        validation_score = await self._cross_validate_claim(claim, sources)

        return {"claim": claim, "validation_score": validation_score, "sources_count": len(sources)}

    async def _execute_expand_search_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ rozÅ¡Ã­Å™enÃ© vyhledÃ¡vÃ¡nÃ­"""
        query = task.parameters["query"]
        max_results = task.parameters.get("max_results", 5)

        # Zde by byla integrace s vyhledÃ¡vaÄi
        # Pro demonstraci vracÃ­me mock data
        results = [f"expanded_result_{i}_for_{query}" for i in range(min(max_results, 3))]

        return {"query": query, "results": results, "results_count": len(results)}

    async def _execute_deep_dive_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ hloubkovou analÃ½zu"""
        target = task.parameters["target"]

        # DetailnÃ­ analÃ½za konkrÃ©tnÃ­ entity nebo tÃ©matu
        deep_analysis = {
            "target": target,
            "depth_level": task.parameters.get("depth_level", 2),
            "findings": f"deep_analysis_of_{target}",
        }

        return deep_analysis

    async def _execute_cross_reference_task(self, task: AgentTask) -> dict[str, Any]:
        """VykonÃ¡ kÅ™Ã­Å¾ovÃ© porovnÃ¡nÃ­"""
        entities = task.parameters["entities"]

        # PorovnÃ¡nÃ­ entit napÅ™Ã­Ä zdroji
        cross_ref = {"entities": entities, "common_attributes": [], "discrepancies": []}

        return cross_ref

    async def _generate_follow_up_tasks(self) -> None:
        """Generuje navazujÃ­cÃ­ Ãºkoly na zÃ¡kladÄ› vÃ½sledkÅ¯"""
        # Analyzuje dokonÄenÃ© Ãºkoly z poslednÃ­ iterace
        recent_completed = [
            task
            for task in self.task_queue.completed_tasks
            if task.result and task.credibility_score >= self.min_credibility_threshold
        ]

        for task in recent_completed[-5:]:  # PoslednÃ­ch 5 ÃºkolÅ¯
            await self._analyze_task_for_follow_ups(task)

    async def _analyze_task_for_follow_ups(self, completed_task: AgentTask) -> None:
        """Analyzuje dokonÄenÃ½ Ãºkol a generuje navazujÃ­cÃ­ Ãºkoly"""
        if completed_task.task_type == TaskType.SCRAPE:
            # NalezenÃ© vzory â†’ dalÅ¡Ã­ analÃ½za
            if completed_task.result.get("patterns_found", 0) > 0:
                analyze_task = AgentTask(
                    id=str(uuid.uuid4()),
                    task_type=TaskType.ANALYZE,
                    priority=TaskPriority.MEDIUM,
                    parameters={"text": completed_task.result["content"]},
                    created_at=datetime.now(),
                    parent_task_id=completed_task.id,
                )
                self.task_queue.add_task(analyze_task)

        elif completed_task.task_type == TaskType.ANALYZE:
            # NalezenÃ© entity â†’ korelace
            entities = completed_task.result.get("correlations", {}).get("entities", [])
            for entity in entities[:3]:  # Max 3 entity
                correlate_task = AgentTask(
                    id=str(uuid.uuid4()),
                    task_type=TaskType.CORRELATE,
                    priority=TaskPriority.LOW,
                    parameters={"entity_id": entity.get("id", str(uuid.uuid4()))},
                    created_at=datetime.now(),
                    parent_task_id=completed_task.id,
                )
                self.task_queue.add_task(correlate_task)

    def _should_continue_research(self) -> bool:
        """Rozhoduje, zda pokraÄovat ve vÃ½zkumu"""
        stats = self.task_queue.get_stats()

        # PokraÄuj pokud jsou Ãºkoly k vykonÃ¡nÃ­
        if stats["pending"] > 0 or stats["running"] > 0:
            return True

        # PokraÄuj pokud byly nedÃ¡vno generovÃ¡ny kvalitnÃ­ vÃ½sledky
        recent_quality = [
            score
            for score in self.credibility_scores[-10:]
            if score >= self.min_credibility_threshold
        ]

        return len(recent_quality) > 2

    async def _cross_validate_claim(self, claim: str, sources: list[str]) -> float:
        """KÅ™Ã­Å¾ovÃ¡ validace tvrzenÃ­ napÅ™Ã­Ä zdroji"""
        if not sources:
            return 0.0

        # Simulace kÅ™Ã­Å¾ovÃ© validace
        # V reÃ¡lnÃ© implementaci by porovnÃ¡vala tvrzenÃ­ napÅ™Ã­Ä zdroji
        return min(1.0, len(sources) * 0.3)

    async def _generate_final_report(self) -> dict[str, Any]:
        """Generuje finÃ¡lnÃ­ report z vÃ½zkumnÃ©ho cyklu"""
        stats = self.task_queue.get_stats()

        # NejlÃ©pe hodnocenÃ© vÃ½sledky
        top_findings = sorted(
            [task for task in self.task_queue.completed_tasks if task.result],
            key=lambda t: t.credibility_score,
            reverse=True,
        )[:10]

        avg_credibility = (
            sum(self.credibility_scores) / len(self.credibility_scores)
            if self.credibility_scores
            else 0.0
        )

        report = {
            "research_summary": {
                "iterations_completed": self.current_iteration,
                "total_tasks_executed": stats["completed"],
                "failed_tasks": stats["failed"],
                "average_credibility": avg_credibility,
                "top_findings_count": len(top_findings),
            },
            "key_discoveries": [
                {
                    "task_type": task.task_type.value,
                    "credibility_score": task.credibility_score,
                    "summary": self._summarize_task_result(task.result),
                }
                for task in top_findings
            ],
            "network_insights": await self._generate_network_insights(),
            "recommendations": self._generate_recommendations(top_findings),
            "metadata": {
                "completed_at": datetime.now().isoformat(),
                "total_runtime_minutes": self.current_iteration * 0.5,  # Estimate
            },
        }

        logger.info(
            f"ğŸ‰ VÃ½zkumnÃ½ cyklus dokonÄen: {stats['completed']} ÃºkolÅ¯, prÅ¯mÄ›rnÃ¡ dÅ¯vÄ›ryhodnost: {avg_credibility:.2f}"
        )

        return report

    def _summarize_task_result(self, result: dict[str, Any]) -> str:
        """VytvoÅ™Ã­ struÄnÃ© shrnutÃ­ vÃ½sledku Ãºkolu"""
        if not result:
            return "Å½Ã¡dnÃ½ vÃ½sledek"

        summary_parts = []

        if "url" in result:
            summary_parts.append(f"URL: {result['url']}")
        if "patterns_found" in result:
            summary_parts.append(f"Vzory: {result['patterns_found']}")
        if "entities_found" in result:
            summary_parts.append(f"Entity: {result['entities_found']}")
        if "credibility_score" in result:
            summary_parts.append(f"DÅ¯vÄ›ryhodnost: {result['credibility_score']:.2f}")

        return "; ".join(summary_parts) if summary_parts else "VÃ½sledek zpracovÃ¡n"

    async def _generate_network_insights(self) -> dict[str, Any]:
        """Generuje insights ze sÃ­Å¥ovÃ© analÃ½zy"""
        # Zde by byla integrace s CorrelationEngine
        return {
            "total_entities": (
                len(self.correlation_engine.knowledge_graph.nodes)
                if hasattr(self.correlation_engine, "knowledge_graph")
                else 0
            ),
            "total_relationships": 0,
            "key_clusters": [],
            "anomalies_detected": 0,
        }

    def _generate_recommendations(self, top_findings: list[AgentTask]) -> list[str]:
        """Generuje doporuÄenÃ­ na zÃ¡kladÄ› zjiÅ¡tÄ›nÃ­"""
        recommendations = []

        high_credibility_count = len(
            [task for task in top_findings if task.credibility_score >= 0.7]
        )

        if high_credibility_count >= 3:
            recommendations.append(
                "Nalezeny vysoce dÅ¯vÄ›ryhodnÃ© zdroje - doporuÄujeme detailnÃ­ manuÃ¡lnÃ­ analÃ½zu"
            )

        if len(top_findings) >= 5:
            recommendations.append("Dostatek dat pro statistickou analÃ½zu trendÅ¯")

        low_credibility_count = len(
            [
                task
                for task in self.task_queue.completed_tasks
                if task.credibility_score < self.min_credibility_threshold
            ]
        )

        if low_credibility_count > len(top_findings):
            recommendations.append(
                "VysokÃ½ podÃ­l nedÅ¯vÄ›ryhodnÃ½ch zdrojÅ¯ - nutnÃ¡ peÄlivÄ›jÅ¡Ã­ filtrace"
            )

        return recommendations

    def stop_research_cycle(self) -> None:
        """ZastavÃ­ vÃ½zkumnÃ½ cyklus"""
        logger.info("ğŸ›‘ Zastavuji vÃ½zkumnÃ½ cyklus")
        self.is_running = False

    def get_current_status(self) -> dict[str, Any]:
        """VrÃ¡tÃ­ aktuÃ¡lnÃ­ stav agenta"""
        stats = self.task_queue.get_stats()

        return {
            "is_running": self.is_running,
            "current_iteration": self.current_iteration,
            "max_iterations": self.max_iterations,
            "task_queue_stats": stats,
            "average_credibility": (
                sum(self.credibility_scores) / len(self.credibility_scores)
                if self.credibility_scores
                else 0.0
            ),
            "total_discoveries": len(self.credibility_scores),
        }
