"""
HierarchickÃ¡ sumarizace pro M1 RAG optimalizaci
RekurzivnÃ­ komprese velkÃ½ch dokumentÅ¯ do zhuÅ¡tÄ›nÃ½ch souhrnÃ½ch textÅ¯
DramatickÃ¡ redukce velikosti kontextu pro vektorovou databÃ¡zi
"""

import gc
import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple, Generator
from dataclasses import dataclass
from pathlib import Path
import time
import hashlib
import json
from concurrent.futures import ThreadPoolExecutor
import threading

logger = logging.getLogger(__name__)


@dataclass
class HierarchicalConfig:
    """Konfigurace pro hierarchickou sumarizaci"""
    max_chunk_size: int = 2000  # MaximÃ¡lnÃ­ velikost jednoho chunky v tokenech
    overlap_size: int = 200  # PÅ™ekryv mezi chunky
    compression_ratio: float = 0.3  # CÃ­lovÃ½ pomÄ›r komprese (30% originÃ¡lnÃ­ velikosti)
    max_recursion_depth: int = 3  # MaximÃ¡lnÃ­ hloubka rekurze
    min_chunk_size: int = 500  # MinimÃ¡lnÃ­ velikost chunky pro dalÅ¡Ã­ dÄ›lenÃ­
    summary_template: str = "ShrÅˆ nÃ¡sledujÃ­cÃ­ text do {target_length} slov, zachovej klÃ­ÄovÃ© informace a faktickÃ½ obsah:"


class M1HierarchicalSummarizer:
    """
    M1-optimalizovanÃ½ hierarchickÃ½ sumarizÃ¡tor

    Algoritmus:
    1. RozdÄ›lÃ­ dokument na sÃ©manticky souvisejÃ­cÃ­ ÄÃ¡sti
    2. VytvoÅ™Ã­ souhrn kaÅ¾dÃ© ÄÃ¡sti pomocÃ­ lokÃ¡lnÃ­ho LLM
    3. RekurzivnÄ› spojÃ­ a zhustÃ­ dÃ­lÄÃ­ souhrny
    4. VÃ½sledek: dramaticky zmenÅ¡enÃ½ text s zachovanÃ½m obsahem

    M1 optimalizace:
    - MalÃ© batch sizes pro stabilnÃ­ inference
    - Streaming processing pro velkÃ© dokumenty
    - Memory pressure monitoring
    - CachovÃ¡nÃ­ pro opakovanÃ© Ãºseky
    """

    def __init__(self,
                 llm_client=None,
                 config: Optional[HierarchicalConfig] = None,
                 cache_dir: Optional[str] = None):
        self.llm_client = llm_client
        self.config = config or HierarchicalConfig()
        self.cache_dir = Path(cache_dir) if cache_dir else Path("./summary_cache")

        # Thread safety a performance tracking
        self._lock = threading.Lock()
        self._cache = {}
        self._stats = {
            'documents_processed': 0,
            'total_input_tokens': 0,
            'total_output_tokens': 0,
            'cache_hits': 0,
            'compression_achieved': 0.0
        }

        # Inicializace cache adresÃ¡Å™e
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"ğŸ§  HierarchickÃ½ sumarizÃ¡tor inicializovÃ¡n")
        logger.info(f"âš™ï¸ Max chunk size: {self.config.max_chunk_size} tokenÅ¯")
        logger.info(f"ğŸ—œï¸ CÃ­lovÃ¡ komprese: {self.config.compression_ratio * 100}%")

    def summarize_document(self,
                          document: str,
                          target_compression: Optional[float] = None) -> Dict[str, Any]:
        """
        HlavnÃ­ metoda pro hierarchickou sumarizaci dokumentu

        Args:
            document: VstupnÃ­ text dokumentu
            target_compression: CÃ­lovÃ½ pomÄ›r komprese (None = pouÅ¾ij config)

        Returns:
            SlovnÃ­k s vÃ½sledky sumarizace
        """
        target_compression = target_compression or self.config.compression_ratio

        logger.info(f"ğŸ“„ ZaÄÃ­nÃ¡m hierarchickou sumarizaci")
        logger.info(f"ğŸ“ VstupnÃ­ dÃ©lka: {len(document)} znakÅ¯")
        logger.info(f"ğŸ¯ CÃ­lovÃ¡ komprese: {target_compression * 100}%")

        start_time = time.time()

        try:
            # FÃ¡ze 1: SÃ©mantickÃ© chunking
            chunks = self._semantic_chunking(document)
            logger.info(f"ğŸ”ª Dokument rozdÄ›len na {len(chunks)} chunkÅ¯")

            # FÃ¡ze 2: HierarchickÃ¡ komprese
            summary_result = self._hierarchical_compression(chunks, target_compression)

            # Statistiky
            processing_time = time.time() - start_time
            input_length = len(document)
            output_length = len(summary_result['final_summary'])
            actual_compression = output_length / input_length

            result = {
                'original_text': document,
                'final_summary': summary_result['final_summary'],
                'compression_tree': summary_result['compression_tree'],
                'statistics': {
                    'input_length': input_length,
                    'output_length': output_length,
                    'compression_ratio': actual_compression,
                    'target_compression': target_compression,
                    'compression_achieved': actual_compression <= target_compression,
                    'processing_time_seconds': processing_time,
                    'chunks_created': len(chunks),
                    'recursion_levels': summary_result['recursion_levels']
                }
            }

            # Update global stats
            with self._lock:
                self._stats['documents_processed'] += 1
                self._stats['total_input_tokens'] += input_length // 4  # Approx tokens
                self._stats['total_output_tokens'] += output_length // 4
                self._stats['compression_achieved'] += actual_compression

            logger.info(f"âœ… Sumarizace dokonÄena za {processing_time:.2f}s")
            logger.info(f"ğŸ—œï¸ Komprese: {input_length} â†’ {output_length} znakÅ¯ ({actual_compression:.1%})")

            return result

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i sumarizaci dokumentu: {e}")
            raise

    def _semantic_chunking(self, document: str) -> List[Dict[str, Any]]:
        """
        SÃ©mantickÃ© rozdÄ›lenÃ­ dokumentu na chunky

        Args:
            document: VstupnÃ­ text

        Returns:
            Seznam chunkÅ¯ s metadaty
        """
        # JednoduchÃ© rozdÄ›lenÃ­ po odstavcÃ­ch s pÅ™ekryvem
        paragraphs = [p.strip() for p in document.split('\n\n') if p.strip()]

        chunks = []
        current_chunk = ""
        current_size = 0
        chunk_id = 0

        for paragraph in paragraphs:
            paragraph_size = len(paragraph)

            # Pokud by pÅ™idÃ¡nÃ­ odstavce pÅ™ekroÄilo limit
            if current_size + paragraph_size > self.config.max_chunk_size and current_chunk:
                # UloÅ¾it souÄasnÃ½ chunk
                chunks.append({
                    'id': chunk_id,
                    'text': current_chunk.strip(),
                    'size': current_size,
                    'start_paragraph': chunk_id * 10,  # Aproximace
                    'end_paragraph': chunk_id * 10 + current_chunk.count('\n\n')
                })

                # ZaÄÃ­t novÃ½ chunk s pÅ™ekryvem
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + "\n\n" + paragraph
                current_size = len(current_chunk)
                chunk_id += 1
            else:
                # PÅ™idat odstavec k souÄasnÃ©mu chunku
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
                current_size += paragraph_size

        # PÅ™idat poslednÃ­ chunk
        if current_chunk:
            chunks.append({
                'id': chunk_id,
                'text': current_chunk.strip(),
                'size': current_size,
                'start_paragraph': chunk_id * 10,
                'end_paragraph': chunk_id * 10 + current_chunk.count('\n\n')
            })

        return chunks

    def _get_overlap_text(self, text: str) -> str:
        """ZÃ­skÃ¡nÃ­ poslednÃ­ch N znakÅ¯ pro pÅ™ekryv"""
        if len(text) <= self.config.overlap_size:
            return text
        return text[-self.config.overlap_size:]

    def _hierarchical_compression(self,
                                 chunks: List[Dict[str, Any]],
                                 target_compression: float) -> Dict[str, Any]:
        """
        HierarchickÃ¡ komprese chunkÅ¯

        Args:
            chunks: Seznam chunkÅ¯ k sumarizaci
            target_compression: CÃ­lovÃ½ pomÄ›r komprese

        Returns:
            VÃ½sledky hierarchickÃ© komprese
        """
        compression_tree = []
        current_level = 0
        current_chunks = chunks.copy()

        logger.info(f"ğŸŒ³ ZaÄÃ­nÃ¡m hierarchickou kompresi s {len(current_chunks)} chunky")

        while len(current_chunks) > 1 and current_level < self.config.max_recursion_depth:
            logger.info(f"ğŸ“Š ÃšroveÅˆ {current_level}: {len(current_chunks)} chunkÅ¯")

            # Sumarizace vÅ¡ech chunkÅ¯ na tÃ©to Ãºrovni
            level_summaries = []

            for chunk in current_chunks:
                summary = self._summarize_chunk(
                    chunk['text'],
                    target_length=int(len(chunk['text']) * target_compression)
                )

                level_summaries.append({
                    'id': f"level_{current_level}_chunk_{chunk['id']}",
                    'text': summary,
                    'size': len(summary),
                    'original_chunk_id': chunk['id'],
                    'compression_ratio': len(summary) / len(chunk['text'])
                })

            # UloÅ¾it ÃºroveÅˆ do stromu
            compression_tree.append({
                'level': current_level,
                'input_chunks': len(current_chunks),
                'output_summaries': len(level_summaries),
                'total_input_size': sum(c['size'] for c in current_chunks),
                'total_output_size': sum(s['size'] for s in level_summaries)
            })

            # PÅ™Ã­prava pro dalÅ¡Ã­ ÃºroveÅˆ
            current_chunks = level_summaries
            current_level += 1

            # Memory cleanup
            gc.collect()

        # FinÃ¡lnÃ­ sumarizace
        if len(current_chunks) == 1:
            final_summary = current_chunks[0]['text']
        else:
            # SpojenÃ­ zbÃ½vajÃ­cÃ­ch chunkÅ¯ a finÃ¡lnÃ­ sumarizace
            combined_text = "\n\n".join(chunk['text'] for chunk in current_chunks)
            final_summary = self._summarize_chunk(
                combined_text,
                target_length=int(len(combined_text) * target_compression)
            )

        logger.info(f"âœ… HierarchickÃ¡ komprese dokonÄena po {current_level} ÃºrovnÃ­ch")

        return {
            'final_summary': final_summary,
            'compression_tree': compression_tree,
            'recursion_levels': current_level
        }

    def _summarize_chunk(self, text: str, target_length: int) -> str:
        """
        Sumarizace jednotlivÃ©ho chunky

        Args:
            text: Text k sumarizaci
            target_length: CÃ­lovÃ¡ dÃ©lka v znacÃ­ch

        Returns:
            SumarizovanÃ½ text
        """
        # Cache kontrola
        cache_key = self._get_cache_key(text, target_length)
        cached_result = self._get_from_cache(cache_key)

        if cached_result:
            self._stats['cache_hits'] += 1
            return cached_result

        try:
            # PÅ™Ã­prava promptu
            target_words = max(50, target_length // 6)  # Aproximace znakÅ¯ na slova
            prompt = self.config.summary_template.format(target_length=target_words)
            full_prompt = f"{prompt}\n\nText:\n{text}\n\nSouhrn:"

            # LLM sumarizace
            if self.llm_client:
                summary = self._call_llm(full_prompt)
            else:
                # Fallback: jednoduchÃ¡ extrakce prvnÃ­ch vÄ›t
                summary = self._simple_summarize(text, target_length)

            # Cache uloÅ¾enÃ­
            self._save_to_cache(cache_key, summary)

            return summary

        except Exception as e:
            logger.warning(f"âš ï¸ Chyba pÅ™i sumarizaci, pouÅ¾iji fallback: {e}")
            return self._simple_summarize(text, target_length)

    def _call_llm(self, prompt: str) -> str:
        """VolÃ¡nÃ­ LLM pro sumarizaci"""
        try:
            # PÅ™edpoklÃ¡dÃ¡me, Å¾e llm_client mÃ¡ metodu generate
            if hasattr(self.llm_client, 'generate'):
                response = self.llm_client.generate(
                    prompt=prompt,
                    max_tokens=min(512, len(prompt) // 4),  # Conservative pro M1
                    temperature=0.3,  # NÃ­zkÃ¡ teplota pro konzistentnÃ­ souhrny
                )
                return response.strip()
            else:
                logger.warning("LLM client nemÃ¡ metodu 'generate'")
                return self._simple_summarize(prompt.split("Text:\n")[1].split("\n\nSouhrn:")[0], 500)

        except Exception as e:
            logger.warning(f"LLM volÃ¡nÃ­ selhalo: {e}")
            raise

    def _simple_summarize(self, text: str, target_length: int) -> str:
        """JednoduchÃ¡ fallback sumarizace"""
        sentences = text.split('. ')

        if len(text) <= target_length:
            return text

        # Vezmi prvnÃ­ N vÄ›t do cÃ­lovÃ© dÃ©lky
        summary = ""
        for sentence in sentences:
            if len(summary) + len(sentence) <= target_length:
                summary += sentence + ". "
            else:
                break

        return summary.strip()

    def _get_cache_key(self, text: str, target_length: int) -> str:
        """VytvoÅ™enÃ­ cache klÃ­Äe"""
        content = f"{text}:{target_length}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[str]:
        """NaÄtenÃ­ z cache"""
        if cache_key in self._cache:
            return self._cache[cache_key]

        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    data = json.load(f)
                    summary = data.get('summary')
                    if summary:
                        self._cache[cache_key] = summary
                        return summary
            except Exception as e:
                logger.debug(f"Cache read error: {e}")

        return None

    def _save_to_cache(self, cache_key: str, summary: str):
        """UloÅ¾enÃ­ do cache"""
        self._cache[cache_key] = summary

        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w') as f:
                json.dump({
                    'summary': summary,
                    'timestamp': time.time()
                }, f)
        except Exception as e:
            logger.debug(f"Cache write error: {e}")

    def batch_summarize_documents(self,
                                 documents: List[str],
                                 target_compression: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        Batch sumarizace vÃ­ce dokumentÅ¯ s M1 optimalizacemi

        Args:
            documents: Seznam dokumentÅ¯
            target_compression: CÃ­lovÃ½ pomÄ›r komprese

        Returns:
            Seznam vÃ½sledkÅ¯ sumarizace
        """
        target_compression = target_compression or self.config.compression_ratio
        results = []

        logger.info(f"ğŸ“š Batch sumarizace {len(documents)} dokumentÅ¯")

        start_time = time.time()

        for i, document in enumerate(documents, 1):
            logger.info(f"ğŸ“„ ZpracovÃ¡vÃ¡m dokument {i}/{len(documents)}")

            try:
                result = self.summarize_document(document, target_compression)
                results.append(result)

                # M1 Memory management
                if i % 5 == 0:  # GC kaÅ¾dÃ½ch 5 dokumentÅ¯
                    gc.collect()
                    logger.debug(f"ğŸ§¹ Memory cleanup po {i} dokumentech")

            except Exception as e:
                logger.error(f"âŒ Chyba pÅ™i zpracovÃ¡nÃ­ dokumentu {i}: {e}")
                results.append({
                    'error': str(e),
                    'original_text': document[:100] + "...",
                    'final_summary': None
                })

        total_time = time.time() - start_time
        logger.info(f"âœ… Batch sumarizace dokonÄena za {total_time:.2f}s")
        logger.info(f"ğŸ“Š PrÅ¯mÄ›rnÃ½ Äas na dokument: {total_time/len(documents):.2f}s")

        return results

    def get_statistics(self) -> Dict[str, Any]:
        """ZÃ­skÃ¡nÃ­ statistik sumarizÃ¡toru"""
        with self._lock:
            avg_compression = (self._stats['compression_achieved'] /
                             max(1, self._stats['documents_processed']))

            return {
                **self._stats,
                'average_compression_ratio': avg_compression,
                'cache_size': len(self._cache),
                'cache_hit_rate': (self._stats['cache_hits'] /
                                 max(1, self._stats['documents_processed']))
            }


# Utility funkce pro jednoduchÃ© pouÅ¾itÃ­
def create_m1_summarizer(llm_client=None,
                        max_chunk_size: int = 2000,
                        compression_ratio: float = 0.3) -> M1HierarchicalSummarizer:
    """Factory funkce pro vytvoÅ™enÃ­ M1-optimalizovanÃ©ho sumarizÃ¡toru"""
    config = HierarchicalConfig(
        max_chunk_size=max_chunk_size,
        compression_ratio=compression_ratio
    )

    return M1HierarchicalSummarizer(
        llm_client=llm_client,
        config=config
    )


if __name__ == "__main__":
    # Test hierarchickÃ© sumarizace
    print("ğŸ§ª TestovÃ¡nÃ­ M1 hierarchickÃ© sumarizace...")

    # Test dokument
    test_document = """
    UmÄ›lÃ¡ inteligence (AI) je oblast poÄÃ­taÄovÃ© vÄ›dy, kterÃ¡ se zamÄ›Å™uje na vytvÃ¡Å™enÃ­ 
    systÃ©mÅ¯ schopnÃ½ch vykonÃ¡vat Ãºkoly, kterÃ© typicky vyÅ¾adujÃ­ lidskou inteligenci.
    
    Historie AI sahÃ¡ do 50. let 20. stoletÃ­, kdy Alan Turing navrhl test pro urÄenÃ­,
    zda stroj mÅ¯Å¾e vykazovat inteligentnÃ­ chovÃ¡nÃ­ nerozeznatelnÃ© od ÄlovÄ›ka.
    
    ModernÃ­ AI zahrnuje mnoho rÅ¯znÃ½ch pÅ™Ã­stupÅ¯, vÄetnÄ› strojovÃ©ho uÄenÃ­, hlubokÃ©ho uÄenÃ­,
    zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka, poÄÃ­taÄovÃ©ho vidÄ›nÃ­ a robotiky.
    
    StrojovÃ© uÄenÃ­ je podoblast AI, kterÃ¡ umoÅ¾Åˆuje poÄÃ­taÄÅ¯m uÄit se a zlepÅ¡ovat
    ze zkuÅ¡enostÃ­ bez explicitnÃ­ho programovÃ¡nÃ­ pro kaÅ¾dou specifickou Ãºlohu.
    
    HlubokÃ© uÄenÃ­ je specifickÃ½ typ strojovÃ©ho uÄenÃ­ inspirovanÃ½ strukturou
    a funkcÃ­ lidskÃ©ho mozku, vyuÅ¾Ã­vajÃ­cÃ­ neuronovÃ© sÃ­tÄ› s mnoha vrstvami.
    
    Aplikace AI jsou rozÅ¡Ã­Å™enÃ© v mnoha oblastech, vÄetnÄ› zdravotnictvÃ­, financÃ­,
    dopravy, vzdÄ›lÃ¡vÃ¡nÃ­ a zÃ¡bavy. AI systÃ©my pomÃ¡hajÃ­ s diagnÃ³zou nemocÃ­,
    automatizacÃ­ obchodovÃ¡nÃ­, Å™Ã­zenÃ­m autonomnÃ­ch vozidel a personalizacÃ­ obsahu.
    """ * 5  # ZvÄ›tÅ¡enÃ­ pro test

    # VytvoÅ™enÃ­ sumarizÃ¡toru (bez LLM klienta - pouÅ¾ije fallback)
    summarizer = create_m1_summarizer(
        max_chunk_size=1000,
        compression_ratio=0.4
    )

    # Test sumarizace
    result = summarizer.summarize_document(test_document)

    print(f"ğŸ“Š VÃ½sledky:")
    print(f"VstupnÃ­ dÃ©lka: {result['statistics']['input_length']} znakÅ¯")
    print(f"VÃ½stupnÃ­ dÃ©lka: {result['statistics']['output_length']} znakÅ¯")
    print(f"Komprese: {result['statistics']['compression_ratio']:.1%}")
    print(f"ÄŒas zpracovÃ¡nÃ­: {result['statistics']['processing_time_seconds']:.2f}s")
    print(f"PoÄet chunkÅ¯: {result['statistics']['chunks_created']}")
    print(f"ÃšrovnÃ­ rekurze: {result['statistics']['recursion_levels']}")

    print(f"\nğŸ“ Souhrn:")
    print(result['final_summary'][:200] + "...")

    print("âœ… Test dokonÄen!")
