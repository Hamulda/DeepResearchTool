"""Forensic Export System
Generov√°n√≠ forenzn√≠ch report≈Ø s kompletn√≠ provenienc√≠ dat

Author: Senior Python/MLOps Agent
"""

import asyncio
import csv
from dataclasses import dataclass
from datetime import UTC, datetime
import hashlib
import json
import logging
from pathlib import Path
from typing import Any
import zipfile

logger = logging.getLogger(__name__)


@dataclass
class ForensicReport:
    """Struktura forenzn√≠ho reportu"""

    report_id: str
    query: str
    timestamp: datetime
    total_sources: int
    verified_sources: int
    claim_count: int
    contradiction_count: int
    confidence_score: float
    processing_time: float
    methodology: str = "LangGraph Research Agent"


class ForensicExporter:
    """Hlavn√≠ t≈ô√≠da pro export forenzn√≠ch report≈Ø
    """

    def __init__(self, output_dir: str = "./forensic_reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def _generate_report_id(self, query: str) -> str:
        """Generov√°n√≠ unique ID pro report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        return f"report_{timestamp}_{query_hash}"

    def _calculate_confidence_score(self,
                                  validation_scores: dict[str, float],
                                  claim_graph_stats: dict[str, Any]) -> float:
        """V√Ωpoƒçet celkov√©ho confidence score"""
        # V√°hy pro r≈Øzn√© faktory
        weights = {
            "source_quality": 0.3,
            "citation_completeness": 0.25,
            "claim_support": 0.25,
            "verification_completeness": 0.2
        }

        # Z√≠sk√°n√≠ sk√≥re z validace
        source_quality = validation_scores.get("quality", 0.5)
        relevance = validation_scores.get("relevance", 0.5)

        # Sk√≥re z claim graph
        total_claims = claim_graph_stats.get("total_claims", 0)
        supported_claims = claim_graph_stats.get("verification_status_counts", {}).get("supported", 0)

        # V√Ωpoƒçty
        citation_completeness = min(1.0, validation_scores.get("coverage", 0.5))
        claim_support = supported_claims / total_claims if total_claims > 0 else 0.5
        verification_completeness = min(1.0, claim_graph_stats.get("total_evidence", 0) / max(total_claims, 1))

        # V√°≈æen√Ω pr≈Ømƒõr
        confidence = (
            weights["source_quality"] * source_quality +
            weights["citation_completeness"] * citation_completeness +
            weights["claim_support"] * claim_support +
            weights["verification_completeness"] * verification_completeness
        )

        return min(1.0, max(0.0, confidence))

    async def generate_markdown_report(
        self,
        query: str,
        synthesis: str,
        sources: list[dict[str, Any]],
        claim_graph: Any,
        validation_scores: dict[str, float],
        processing_time: float,
        metadata: dict[str, Any] = None
    ) -> str:
        """Generuje kompletn√≠ Markdown report s kompletn√≠ provenienc√≠

        Args:
            query: P≈Øvodn√≠ v√Ωzkumn√Ω dotaz
            synthesis: Fin√°ln√≠ synt√©za
            sources: Seznam pou≈æit√Ωch zdroj≈Ø
            claim_graph: ClaimGraph instance
            validation_scores: Sk√≥re validace
            processing_time: Doba zpracov√°n√≠
            metadata: Dodateƒçn√° metadata

        Returns:
            Markdown form√°tovan√Ω report

        """
        # P≈ô√≠prava z√°kladn√≠ch informac√≠
        report_id = self._generate_report_id(query)
        timestamp = datetime.now(UTC)

        # Statistiky claim graph
        if hasattr(claim_graph, 'get_statistics'):
            claim_stats = claim_graph.get_statistics()
        else:
            claim_stats = {}

        # V√Ωpoƒçet confidence score
        confidence = self._calculate_confidence_score(validation_scores, claim_stats)

        # Vytvo≈ôen√≠ forenzn√≠ho reportu
        forensic_report = ForensicReport(
            report_id=report_id,
            query=query,
            timestamp=timestamp,
            total_sources=len(sources),
            verified_sources=len([s for s in sources if s.get('verified', False)]),
            claim_count=claim_stats.get('total_claims', 0),
            contradiction_count=claim_stats.get('relation_type_counts', {}).get('contradict', 0),
            confidence_score=confidence,
            processing_time=processing_time
        )

        # Generov√°n√≠ Markdown reportu
        report_lines = []

        # Header
        report_lines.extend([
            "# Forenzn√≠ v√Ωzkumn√Ω report",
            "",
            f"**Report ID:** `{report_id}`  ",
            f"**Datum:** {timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}  ",
            f"**Metodologie:** {forensic_report.methodology}  ",
            f"**Confidence Score:** {confidence:.2f}/1.00  ",
            "",
            "---",
            ""
        ])

        # P≈Øvodn√≠ dotaz
        report_lines.extend([
            "## üìã V√Ωzkumn√Ω dotaz",
            "",
            f"> {query}",
            "",
        ])

        # Exekutivn√≠ souhrn
        report_lines.extend([
            "## üìä Exekutivn√≠ souhrn",
            "",
            f"- **Celkem zdroj≈Ø:** {forensic_report.total_sources}",
            f"- **Ovƒõ≈ôen√© zdroje:** {forensic_report.verified_sources}",
            f"- **Extrahovan√° tvrzen√≠:** {forensic_report.claim_count}",
            f"- **Nalezen√© rozpory:** {forensic_report.contradiction_count}",
            f"- **Doba zpracov√°n√≠:** {forensic_report.processing_time:.2f} sekund",
            f"- **Celkov√© sk√≥re d≈Øvƒõryhodnosti:** {confidence:.2f}/1.00",
            "",
        ])

        # Validaƒçn√≠ metriky
        if validation_scores:
            report_lines.extend([
                "## ‚úÖ Validaƒçn√≠ metriky",
                "",
                "| Metrika | Sk√≥re | Status |",
                "|---------|-------|--------|",
            ])

            for metric, score in validation_scores.items():
                status = "‚úÖ Vyhovuj√≠c√≠" if score >= 0.7 else "‚ö†Ô∏è Slab√©" if score >= 0.5 else "‚ùå Nevyhovuj√≠c√≠"
                report_lines.append(f"| {metric.capitalize()} | {score:.2f} | {status} |")

            report_lines.append("")

        # Hlavn√≠ synt√©za
        report_lines.extend([
            "## üìÑ Hlavn√≠ synt√©za",
            "",
            synthesis,
            "",
        ])

        # Anal√Ωza tvrzen√≠ s provenienc√≠
        if hasattr(claim_graph, 'claims') and claim_graph.claims:
            report_lines.extend([
                "## üß© Anal√Ωza tvrzen√≠ s kompletn√≠ provenienc√≠",
                "",
            ])

            for claim_id, claim in claim_graph.claims.items():
                # Z√≠sk√°n√≠ podpory pro tvrzen√≠
                if hasattr(claim_graph, 'get_claim_support'):
                    support_data = claim_graph.get_claim_support(claim_id)
                else:
                    support_data = {}

                verification_status = support_data.get('verification_status', 'neovƒõ≈ôeno')
                support_score = support_data.get('support_score', 0.0)

                # Status emoji
                status_emoji = {
                    'supported': '‚úÖ',
                    'partially_supported': 'üü°',
                    'disputed': '‚ö†Ô∏è',
                    'contradicted': '‚ùå',
                    'neutral': '‚ö™'
                }.get(verification_status, '‚ùì')

                report_lines.extend([
                    f"### {status_emoji} Tvrzen√≠: {claim.text}",
                    "",
                    f"**Status:** {verification_status}  ",
                    f"**Podpora:** {support_score:.2f}/1.00  ",
                    f"**Confidence:** {claim.confidence:.2f}/1.00  ",
                    f"**Extrahov√°no:** {claim.created_at.strftime('%Y-%m-%d %H:%M:%S')}  ",
                    "",
                ])

                # Evidence
                evidence_list = support_data.get('evidence', [])
                if evidence_list:
                    report_lines.extend([
                        "**D≈Økazy:**",
                        "",
                    ])

                    for i, evidence in enumerate(evidence_list, 1):
                        credibility = evidence.credibility_score if hasattr(evidence, 'credibility_score') else 0.7
                        relevance = evidence.relevance_score if hasattr(evidence, 'relevance_score') else 0.7
                        source_url = evidence.source_url if hasattr(evidence, 'source_url') else "N/A"

                        report_lines.extend([
                            f"{i}. **D≈Økaz ID:** `{evidence.id}`",
                            f"   - **Text:** {evidence.text[:200]}{'...' if len(evidence.text) > 200 else ''}",
                            f"   - **Zdroj:** {evidence.source_id}",
                            f"   - **URL:** {source_url}",
                            f"   - **D≈Øvƒõryhodnost:** {credibility:.2f}/1.00",
                            f"   - **Relevance:** {relevance:.2f}/1.00",
                            "",
                        ])

                # Supporting/Contradicting claims
                supporting_claims = support_data.get('supporting_claims', [])
                contradicting_claims = support_data.get('contradicting_claims', [])

                if supporting_claims:
                    report_lines.extend([
                        "**Podporuj√≠c√≠ tvrzen√≠:**",
                        "",
                    ])
                    for supporting in supporting_claims:
                        report_lines.append(f"- {supporting.text}")
                    report_lines.append("")

                if contradicting_claims:
                    report_lines.extend([
                        "**Proti≈ôeƒç√≠c√≠ tvrzen√≠:**",
                        "",
                    ])
                    for contradicting in contradicting_claims:
                        report_lines.append(f"- {contradicting.text}")
                    report_lines.append("")

                report_lines.append("---")
                report_lines.append("")

        # Zdroje a citace
        report_lines.extend([
            "## üìö Kompletn√≠ seznam zdroj≈Ø a citac√≠",
            "",
            "| # | Zdroj | URL | Typ | ƒåasov√Ω otisk | Status |",
            "|---|-------|-----|-----|--------------|--------|",
        ])

        for i, source in enumerate(sources, 1):
            source_type = source.get('metadata', {}).get('source_type', 'nezn√°m√Ω')
            url = source.get('source', 'N/A')
            timestamp_str = source.get('metadata', {}).get('timestamp', 'N/A')
            verified = "‚úÖ Ovƒõ≈ôeno" if source.get('verified', False) else "‚ö™ Neovƒõ≈ôeno"

            # Zkr√°cen√≠ URL pro lep≈°√≠ ƒçitelnost
            display_url = url[:50] + "..." if len(url) > 50 else url

            report_lines.append(f"| {i} | {source_type} | {display_url} | {source_type} | {timestamp_str} | {verified} |")

        report_lines.extend([
            "",
            "## üîç Metodologie a omezen√≠",
            "",
            "### Metodologie",
            "- **Vyhled√°v√°n√≠:** Hybridn√≠ p≈ô√≠stup kombinuj√≠c√≠ s√©mantick√© a lexik√°ln√≠ vyhled√°v√°n√≠",
            "- **Validace:** Automatick√° validace zdroj≈Ø s LLM-based hodnocen√≠m",
            "- **Extrakce tvrzen√≠:** Automatick√° extrakce pomoc√≠ LLM s pattern matching",
            "- **Verification:** Cross-referencing mezi zdroji a detekce rozpor≈Ø",
            "",
            "### Omezen√≠",
            "- Automatick√° extrakce tvrzen√≠ m≈Ø≈æe b√Ωt ne√∫pln√°",
            "- Validace zdroj≈Ø je zalo≈æena na heuristik√°ch",
            "- Nƒõkter√© nuance mohou b√Ωt ztraceny p≈ôi kompresi",
            "- ƒåasov√© raz√≠tko odpov√≠d√° ƒçasu sta≈æen√≠, ne publikov√°n√≠",
            "",
            "### Doporuƒçen√≠ pro dal≈°√≠ v√Ωzkum",
            "- Manu√°ln√≠ verifikace kl√≠ƒçov√Ωch tvrzen√≠",
            "- Konzultace s domain experty",
            "- Roz≈°√≠≈ôen√≠ zdrojov√© b√°ze pro komplexnƒõj≈°√≠ t√©mata",
            "",
            "---",
            "",
            f"*Report generov√°n automaticky syst√©mem Deep Research Tool v {timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}*",
            f"*Report ID: {report_id}*"
        ])

        return "\n".join(report_lines)

    async def export_complete_investigation(
        self,
        query: str,
        results: dict[str, Any],
        output_dir: str = "./forensic_reports"
    ) -> dict[str, str]:
        """Exportuje kompletn√≠ forenzn√≠ investigaci

        Args:
            query: V√Ωzkumn√Ω dotaz
            results: Kompletn√≠ v√Ωsledky z agenta
            output_dir: V√Ωstupn√≠ adres√°≈ô

        Returns:
            Dict s cestami k vytvo≈ôen√Ωm soubor≈Øm

        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        report_id = self._generate_report_id(query)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")

        exported_files = {}

        try:
            # 1. Markdown report
            markdown_report = await self.generate_markdown_report(
                query=query,
                synthesis=results.get("synthesis", ""),
                sources=results.get("retrieved_docs", []),
                claim_graph=results.get("claim_graph"),
                validation_scores=results.get("validation_scores", {}),
                processing_time=results.get("processing_time", 0),
                metadata=results.get("metadata", {})
            )

            markdown_path = output_path / f"{report_id}_report.md"
            with open(markdown_path, 'w', encoding='utf-8') as f:
                f.write(markdown_report)
            exported_files["markdown_report"] = str(markdown_path)

            # 2. JSON dump v≈°ech dat
            json_path = output_path / f"{report_id}_raw_data.json"

            # P≈ô√≠prava dat pro JSON export
            json_data = {
                "report_metadata": {
                    "report_id": report_id,
                    "query": query,
                    "timestamp": datetime.now(UTC).isoformat(),
                    "version": "1.0"
                },
                "results": self._serialize_results_for_json(results)
            }

            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)
            exported_files["raw_data"] = str(json_path)

            # 3. Claim graph export
            if results.get("claim_graph") and hasattr(results["claim_graph"], "export_to_json"):
                claim_graph_path = output_path / f"{report_id}_claim_graph.json"
                results["claim_graph"].export_to_json(str(claim_graph_path))
                exported_files["claim_graph"] = str(claim_graph_path)

            # 4. CSV export zdroj≈Ø
            sources_csv_path = output_path / f"{report_id}_sources.csv"
            await self._export_sources_csv(results.get("retrieved_docs", []), sources_csv_path)
            exported_files["sources_csv"] = str(sources_csv_path)

            # 5. Komprimovan√Ω archiv
            zip_path = output_path / f"{report_id}_complete_investigation.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_type, file_path in exported_files.items():
                    zipf.write(file_path, Path(file_path).name)

            exported_files["archive"] = str(zip_path)

            logger.info(f"‚úÖ Forenzn√≠ investigace exportov√°na: {report_id}")

        except Exception as e:
            logger.error(f"Chyba p≈ôi exportu forenzn√≠ investigace: {e}")
            exported_files["error"] = str(e)

        return exported_files

    def _serialize_results_for_json(self, results: dict[str, Any]) -> dict[str, Any]:
        """Serializuje v√Ωsledky pro JSON export"""
        serialized = {}

        for key, value in results.items():
            if key == "claim_graph":
                # ClaimGraph nen√≠ p≈ô√≠mo serializovateln√Ω
                if hasattr(value, 'get_statistics'):
                    serialized[key] = {
                        "statistics": value.get_statistics(),
                        "type": "ClaimGraph"
                    }
                else:
                    serialized[key] = {"type": "ClaimGraph", "data": "not_serializable"}
            elif isinstance(value, (str, int, float, bool, list, dict)):
                serialized[key] = value
            else:
                serialized[key] = str(value)

        return serialized

    async def _export_sources_csv(self, sources: list[dict[str, Any]], csv_path: Path):
        """Exportuje zdroje do CSV form√°tu"""
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['source_id', 'url', 'source_type', 'timestamp', 'content_length', 'verified', 'metadata']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            writer.writeheader()

            for i, source in enumerate(sources):
                writer.writerow({
                    'source_id': f"source_{i+1}",
                    'url': source.get('source', ''),
                    'source_type': source.get('metadata', {}).get('source_type', 'unknown'),
                    'timestamp': source.get('metadata', {}).get('timestamp', ''),
                    'content_length': len(source.get('content', '')),
                    'verified': source.get('verified', False),
                    'metadata': json.dumps(source.get('metadata', {}))
                })


# Utility funkce pro rychl√© pou≈æit√≠
async def export_investigation_report(
    query: str,
    results: dict[str, Any],
    output_dir: str = "./forensic_reports"
) -> dict[str, str]:
    """Convenience funkce pro rychl√Ω export forenzn√≠ investigace

    Args:
        query: V√Ωzkumn√Ω dotaz
        results: V√Ωsledky z research agenta
        output_dir: V√Ωstupn√≠ adres√°≈ô

    Returns:
        Dict s cestami k vytvo≈ôen√Ωm soubor≈Øm

    """
    exporter = ForensicExporter(output_dir)
    return await exporter.export_complete_investigation(query, results, output_dir)


# P≈ô√≠klad pou≈æit√≠
async def example_forensic_export():
    """P≈ô√≠klad pou≈æit√≠ forenzn√≠ho exportu"""
    # Simulovan√° data
    mock_results = {
        "synthesis": "## Uk√°zkov√° synt√©za\n\nToto je p≈ô√≠klad synt√©zy v√Ωzkumu...",
        "retrieved_docs": [
            {
                "content": "Obsah dokumentu 1...",
                "source": "https://example.com/doc1",
                "metadata": {
                    "source_type": "academic",
                    "timestamp": "2023-12-01T10:00:00Z"
                },
                "verified": True
            }
        ],
        "validation_scores": {
            "relevance": 0.85,
            "quality": 0.78,
            "coverage": 0.92
        },
        "processing_time": 45.2,
        "metadata": {
            "architecture": "langgraph",
            "total_documents": 1
        }
    }

    # Export
    exported = await export_investigation_report(
        "Jak√© jsou trendy v AI?",
        mock_results
    )

    print("üìÅ Exportovan√© soubory:")
    for file_type, path in exported.items():
        print(f"  {file_type}: {path}")


if __name__ == "__main__":
    asyncio.run(example_forensic_export())
