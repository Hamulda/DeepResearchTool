"""
ChromaDB In-Process Client - M1 Optimalized
Bƒõ≈æ√≠ p≈ô√≠mo v Python procesu m√≠sto samostatn√©ho kontejneru
√öspora pamƒõti: ~500MB oproti samostatn√©mu Qdrant kontejneru
"""

import gc
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import json
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading

logger = logging.getLogger(__name__)

try:
    import chromadb
    from chromadb.config import Settings
    from chromadb.utils import embedding_functions
    CHROMADB_AVAILABLE = True
except ImportError:
    logger.warning("ChromaDB not available - falling back to mock implementation")
    CHROMADB_AVAILABLE = False


@dataclass
class M1MemorySettings:
    """M1-specifick√© nastaven√≠ pamƒõti pro ChromaDB"""
    max_batch_size: int = 1000
    embedding_cache_size: int = 10000
    persist_threshold: int = 5000  # Poƒçet dokument≈Ø po kter√Ωch persistovat
    gc_frequency: int = 100  # Garbage collection po N operac√≠ch
    max_memory_usage_mb: int = 1024  # 1GB limit


class M1OptimizedChromaClient:
    """
    ChromaDB klient optimalizovan√Ω pro MacBook Air M1 (8GB RAM)

    Kl√≠ƒçov√© optimalizace:
    - In-process bƒõh (≈æ√°dn√Ω Docker kontejner)
    - Automatick√Ω garbage collection
    - Batch processing s M1-optimalized velikostmi
    - Memory pressure monitoring
    - Intelligent persistence strategie
    """

    def __init__(self,
                 persist_directory: str = "./chroma_db_m1",
                 collection_name: str = "research_docs",
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
                 memory_settings: Optional[M1MemorySettings] = None):

        self.persist_directory = Path(persist_directory)
        self.collection_name = collection_name
        self.memory_settings = memory_settings or M1MemorySettings()

        # Counters pro optimalizace
        self._operation_counter = 0
        self._last_gc_time = time.time()
        self._document_count = 0

        # Thread safety
        self._lock = threading.Lock()

        # ChromaDB komponenty
        self.client = None
        self.collection = None
        self.embedding_function = None

        # Inicializace
        self._initialize_client()
        self._setup_embedding_function(embedding_model)
        self._setup_collection()

        logger.info(f"üöÄ M1-optimalizovan√Ω ChromaDB klient inicializov√°n")
        logger.info(f"üìÅ Persist directory: {self.persist_directory}")
        logger.info(f"üß† Max memory: {self.memory_settings.max_memory_usage_mb}MB")

    def _initialize_client(self):
        """Inicializace ChromaDB klienta s M1 optimalizacemi"""
        if not CHROMADB_AVAILABLE:
            raise ImportError("ChromaDB nen√≠ dostupn√©. Nainstalujte: pip install chromadb")

        # Vytvo≈ôen√≠ persist directory
        self.persist_directory.mkdir(parents=True, exist_ok=True)

        # M1-optimalizovan√© nastaven√≠
        settings = Settings(
            chroma_db_impl="duckdb+parquet",  # Lightweight backend
            persist_directory=str(self.persist_directory),
            anonymized_telemetry=False,  # Vypnout telemetrii
        )

        self.client = chromadb.PersistentClient(settings=settings)
        logger.info("‚úÖ ChromaDB persistent client vytvo≈ôen")

    def _setup_embedding_function(self, model_name: str):
        """Nastaven√≠ embedding funkce s M1 optimalizacemi"""
        try:
            # Pou≈æit√≠ sentence-transformers s M1 optimalizacemi
            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=model_name,
                device="mps" if self._is_mps_available() else "cpu",  # M1 Metal
                normalize_embeddings=True  # Normalizace pro lep≈°√≠ performance
            )
            logger.info(f"‚úÖ Embedding function nastavena: {model_name}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chyba p≈ôi nastaven√≠ embedding funkce: {e}")
            # Fallback na default
            self.embedding_function = embedding_functions.DefaultEmbeddingFunction()

    @staticmethod
    def _is_mps_available() -> bool:
        """Kontrola dostupnosti Metal Performance Shaders na M1"""
        try:
            import torch
            return torch.backends.mps.is_available()
        except ImportError:
            return False

    def _setup_collection(self):
        """Vytvo≈ôen√≠ nebo naƒçten√≠ kolekce"""
        try:
            # Pokus o naƒçten√≠ existuj√≠c√≠ kolekce
            self.collection = self.client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
            self._document_count = self.collection.count()
            logger.info(f"‚úÖ Kolekce naƒçtena: {self.collection_name} ({self._document_count} dokument≈Ø)")

        except Exception:
            # Vytvo≈ôen√≠ nov√© kolekce
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine", "hnsw:M": 16}  # M1 optimalizace
            )
            self._document_count = 0
            logger.info(f"‚úÖ Nov√° kolekce vytvo≈ôena: {self.collection_name}")

    def add_documents(self,
                     documents: List[str],
                     metadatas: List[Dict[str, Any]],
                     ids: List[str],
                     batch_size: Optional[int] = None) -> None:
        """
        P≈ôid√°n√≠ dokument≈Ø s M1-optimalizovan√Ωm batch processingem

        Args:
            documents: Seznam dokument≈Ø
            metadatas: Metadata pro ka≈æd√Ω dokument
            ids: Unik√°tn√≠ ID pro ka≈æd√Ω dokument
            batch_size: Velikost d√°vky (None = auto)
        """
        with self._lock:
            if not batch_size:
                batch_size = self.memory_settings.max_batch_size

            total_docs = len(documents)
            logger.info(f"üìù P≈ôid√°v√°m {total_docs} dokument≈Ø v d√°vk√°ch po {batch_size}")

            # Batch processing
            for i in range(0, total_docs, batch_size):
                batch_end = min(i + batch_size, total_docs)
                batch_docs = documents[i:batch_end]
                batch_meta = metadatas[i:batch_end]
                batch_ids = ids[i:batch_end]

                try:
                    self.collection.add(
                        documents=batch_docs,
                        metadatas=batch_meta,
                        ids=batch_ids
                    )

                    self._document_count += len(batch_docs)
                    self._operation_counter += len(batch_docs)

                    # Progress log
                    if i + batch_size < total_docs:
                        logger.info(f"üìä Zpracov√°no {batch_end}/{total_docs} dokument≈Ø")

                    # M1 Memory management
                    self._maybe_trigger_gc()
                    self._maybe_persist()

                except Exception as e:
                    logger.error(f"‚ùå Chyba p≈ôi p≈ôid√°v√°n√≠ d√°vky {i}-{batch_end}: {e}")
                    raise

            logger.info(f"‚úÖ P≈ôid√°no {total_docs} dokument≈Ø. Celkem: {self._document_count}")

    def query_documents(self,
                       query_text: str,
                       n_results: int = 10,
                       where_filter: Optional[Dict] = None,
                       include_distances: bool = True) -> Dict[str, Any]:
        """
        Dotazov√°n√≠ dokument≈Ø s M1 optimalizacemi

        Args:
            query_text: Text dotazu
            n_results: Poƒçet v√Ωsledk≈Ø
            where_filter: Filtry metadata
            include_distances: Zahrnout vzd√°lenosti

        Returns:
            Slovn√≠k s v√Ωsledky dotazu
        """
        with self._lock:
            try:
                include_list = ['documents', 'metadatas']
                if include_distances:
                    include_list.append('distances')

                results = self.collection.query(
                    query_texts=[query_text],
                    n_results=n_results,
                    where=where_filter,
                    include=include_list
                )

                # M1 Memory cleanup po dotazu
                self._operation_counter += 1
                self._maybe_trigger_gc()

                # Form√°tov√°n√≠ v√Ωsledk≈Ø
                formatted_results = {
                    'documents': results['documents'][0] if results['documents'] else [],
                    'metadatas': results['metadatas'][0] if results['metadatas'] else [],
                    'ids': results['ids'][0] if results['ids'] else []
                }

                if include_distances and 'distances' in results:
                    formatted_results['distances'] = results['distances'][0]

                logger.debug(f"üîç Dotaz vr√°til {len(formatted_results['documents'])} v√Ωsledk≈Ø")
                return formatted_results

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi dotazov√°n√≠: {e}")
                raise

    def delete_documents(self, ids: List[str]) -> None:
        """Smaz√°n√≠ dokument≈Ø podle ID"""
        with self._lock:
            try:
                self.collection.delete(ids=ids)
                self._document_count = max(0, self._document_count - len(ids))
                logger.info(f"üóëÔ∏è Smaz√°no {len(ids)} dokument≈Ø")

                # Cleanup po smaz√°n√≠
                self._maybe_trigger_gc()

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi maz√°n√≠ dokument≈Ø: {e}")
                raise

    def get_collection_stats(self) -> Dict[str, Any]:
        """Statistiky kolekce a vyu≈æit√≠ pamƒõti"""
        try:
            count = self.collection.count()

            # Memory usage estimation
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024

            return {
                'document_count': count,
                'collection_name': self.collection_name,
                'memory_usage_mb': round(memory_mb, 2),
                'memory_limit_mb': self.memory_settings.max_memory_usage_mb,
                'memory_usage_percent': round((memory_mb / self.memory_settings.max_memory_usage_mb) * 100, 1),
                'operations_since_last_gc': self._operation_counter,
                'persist_directory': str(self.persist_directory)
            }

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi z√≠sk√°v√°n√≠ statistik: {e}")
            return {'error': str(e)}

    def _maybe_trigger_gc(self):
        """Spu≈°tƒõn√≠ garbage collection p≈ôi pot≈ôebƒõ (M1 optimalizace)"""
        if self._operation_counter >= self.memory_settings.gc_frequency:
            start_time = time.time()
            collected = gc.collect()
            gc_time = time.time() - start_time

            if collected > 0:
                logger.debug(f"üßπ GC: {collected} objekt≈Ø smaz√°no za {gc_time:.3f}s")

            self._operation_counter = 0
            self._last_gc_time = time.time()

    def _maybe_persist(self):
        """Automatick√° persistence p≈ôi dosa≈æen√≠ prahu"""
        if self._document_count % self.memory_settings.persist_threshold == 0:
            logger.info(f"üíæ Auto-persist: {self._document_count} dokument≈Ø")

    def optimize_for_m1(self):
        """Spu≈°tƒõn√≠ M1-specifick√Ωch optimalizac√≠"""
        logger.info("‚ö° Spou≈°t√≠m M1 optimalizace...")

        # Force garbage collection
        collected = gc.collect()
        logger.info(f"üßπ GC: {collected} objekt≈Ø uvolnƒõno")

        # Memory usage check
        stats = self.get_collection_stats()
        memory_percent = stats.get('memory_usage_percent', 0)

        if memory_percent > 80:
            logger.warning(f"‚ö†Ô∏è Vysok√© vyu≈æit√≠ pamƒõti: {memory_percent}%")

        logger.info(f"‚úÖ M1 optimalizace dokonƒçena. Pamƒõ≈•: {memory_percent}%")
        return stats

    def __enter__(self):
        """Context manager entry"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit s cleanup"""
        try:
            self.optimize_for_m1()
            logger.info("üëã ChromaDB klient uzav≈ôen")
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi uzav√≠r√°n√≠ klienta: {e}")


# Utility funkce pro snadn√© pou≈æit√≠
def create_m1_chroma_client(collection_name: str = "research_docs",
                           max_memory_mb: int = 1024) -> M1OptimizedChromaClient:
    """Factory funkce pro vytvo≈ôen√≠ M1-optimalizovan√©ho klienta"""
    memory_settings = M1MemorySettings(max_memory_usage_mb=max_memory_mb)

    return M1OptimizedChromaClient(
        collection_name=collection_name,
        memory_settings=memory_settings
    )


if __name__ == "__main__":
    # Test M1 optimalizovan√©ho klienta
    print("üß™ Testov√°n√≠ M1 ChromaDB klienta...")

    with create_m1_chroma_client("test_collection") as client:
        # Test p≈ôid√°n√≠ dokument≈Ø
        docs = ["Test document 1", "Test document 2", "Machine learning paper"]
        metas = [{"source": "test"}, {"source": "test"}, {"source": "research"}]
        ids = ["1", "2", "3"]

        client.add_documents(docs, metas, ids)

        # Test dotazu
        results = client.query_documents("machine learning", n_results=2)
        print(f"üìä Nalezeno {len(results['documents'])} dokument≈Ø")

        # Statistiky
        stats = client.get_collection_stats()
        print(f"üìà Statistiky: {stats}")

    print("‚úÖ Test dokonƒçen!")
