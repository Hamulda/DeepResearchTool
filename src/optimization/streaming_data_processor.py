"""
Streamovan√© zpracov√°n√≠ dat pro M1 optimalizaci
Zpracov√°n√≠ velk√Ωch datov√Ωch soubor≈Ø s konstantn√≠ pamƒõ≈•ovou stopou
Pou≈æit√≠ gener√°tor≈Ø a ijson pro streaming JSON parsing
"""

import gc
import logging
import json
from pathlib import Path
from typing import Generator, Dict, Any, Iterator, Optional, List, Union
from dataclasses import dataclass
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

logger = logging.getLogger(__name__)

try:
    import ijson
    IJSON_AVAILABLE = True
except ImportError:
    logger.warning("ijson not available - falling back to standard json")
    IJSON_AVAILABLE = False


@dataclass
class StreamingConfig:
    """Konfigurace pro streamovan√© zpracov√°n√≠"""
    chunk_size: int = 1000  # Velikost chunk≈Ø pro zpracov√°n√≠
    memory_limit_mb: int = 512  # Limit pamƒõti pro jeden worker
    max_workers: int = 2  # Poƒçet paraleln√≠ch worker≈Ø (konzervativn√≠ pro M1)
    gc_frequency: int = 50  # GC po N chunc√≠ch
    progress_frequency: int = 100  # Progress log po N z√°znamech


class M1StreamingDataProcessor:
    """
    M1-optimalizovan√Ω procesor pro streamovan√© zpracov√°n√≠ dat

    Kl√≠ƒçov√© vlastnosti:
    - Konstantn√≠ pamƒõ≈•ov√° stopa nez√°visle na velikosti souboru
    - Gener√°tory pro lazy evaluation
    - Automatick√Ω garbage collection
    - Memory pressure monitoring
    - Batch processing s M1-optimalizovan√Ωmi velikostmi
    """

    def __init__(self, config: Optional[StreamingConfig] = None):
        self.config = config or StreamingConfig()
        self._processed_count = 0
        self._start_time = None
        self._lock = threading.Lock()

        logger.info(f"üöÄ M1 Streaming procesor inicializov√°n")
        logger.info(f"üìä Chunk size: {self.config.chunk_size}")
        logger.info(f"üíæ Memory limit: {self.config.memory_limit_mb}MB")

    def stream_json_file(self, file_path: Union[str, Path]) -> Generator[Dict[str, Any], None, None]:
        """
        Streamov√°n√≠ JSON souboru po jednotliv√Ωch objektech

        Args:
            file_path: Cesta k JSON souboru

        Yields:
            Jednotliv√© JSON objekty
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"Soubor neexistuje: {file_path}")

        logger.info(f"üìÅ Streamov√°n√≠ JSON souboru: {file_path}")
        file_size_mb = file_path.stat().st_size / 1024 / 1024
        logger.info(f"üìè Velikost souboru: {file_size_mb:.2f}MB")

        self._start_time = time.time()

        if IJSON_AVAILABLE:
            yield from self._stream_with_ijson(file_path)
        else:
            yield from self._stream_with_standard_json(file_path)

    def _stream_with_ijson(self, file_path: Path) -> Generator[Dict[str, Any], None, None]:
        """Streamov√°n√≠ pomoc√≠ ijson (efektivnƒõj≈°√≠ pro velk√© soubory)"""
        try:
            with open(file_path, 'rb') as file:
                # P≈ôedpokl√°d√°me array JSON objekt≈Ø nebo jednotliv√© objekty na ≈ô√°dc√≠ch
                parser = ijson.parse(file)
                current_object = {}
                object_depth = 0

                for prefix, event, value in parser:
                    if event == 'start_map':
                        if object_depth == 0:
                            current_object = {}
                        object_depth += 1
                    elif event == 'end_map':
                        object_depth -= 1
                        if object_depth == 0 and current_object:
                            yield current_object
                            self._processed_count += 1
                            self._maybe_log_progress()
                            self._maybe_trigger_gc()
                            current_object = {}
                    elif event in ('string', 'number', 'boolean', 'null') and object_depth == 1:
                        # P≈ôid√°n√≠ hodnoty do objektu
                        key = prefix.split('.')[-1] if '.' in prefix else prefix
                        current_object[key] = value

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi ijson streamov√°n√≠: {e}")
            # Fallback na standard JSON
            yield from self._stream_with_standard_json(file_path)

    def _stream_with_standard_json(self, file_path: Path) -> Generator[Dict[str, Any], None, None]:
        """Fallback streamov√°n√≠ pomoc√≠ standard JSON"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                # Pokus o naƒçten√≠ jako JSON array
                try:
                    data = json.load(file)
                    if isinstance(data, list):
                        for item in data:
                            yield item
                            self._processed_count += 1
                            self._maybe_log_progress()
                            self._maybe_trigger_gc()
                    else:
                        # Jednotliv√Ω objekt
                        yield data
                        self._processed_count += 1

                except json.JSONDecodeError:
                    # Mo≈æn√° JSONL form√°t (jeden JSON objekt na ≈ô√°dek)
                    file.seek(0)
                    for line_num, line in enumerate(file, 1):
                        line = line.strip()
                        if line:
                            try:
                                obj = json.loads(line)
                                yield obj
                                self._processed_count += 1
                                self._maybe_log_progress()
                                self._maybe_trigger_gc()
                            except json.JSONDecodeError as e:
                                logger.warning(f"‚ö†Ô∏è Nevalidn√≠ JSON na ≈ô√°dku {line_num}: {e}")

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi standardn√≠m JSON streamov√°n√≠: {e}")
            raise

    def process_data_chunks(self,
                           data_stream: Generator[Dict[str, Any], None, None],
                           processor_func: callable,
                           chunk_size: Optional[int] = None) -> Generator[List[Any], None, None]:
        """
        Zpracov√°n√≠ dat po chunc√≠ch s danou funkc√≠

        Args:
            data_stream: Generator dat
            processor_func: Funkce pro zpracov√°n√≠ chunk≈Ø
            chunk_size: Velikost chunk≈Ø (None = pou≈æij config)

        Yields:
            Zpracovan√© chunky
        """
        chunk_size = chunk_size or self.config.chunk_size
        chunk = []

        logger.info(f"‚öôÔ∏è Zaƒç√≠n√°m chunk processing (velikost: {chunk_size})")

        try:
            for item in data_stream:
                chunk.append(item)

                if len(chunk) >= chunk_size:
                    # Zpracov√°n√≠ chunky
                    processed_chunk = processor_func(chunk)
                    yield processed_chunk

                    # Memory cleanup
                    chunk.clear()
                    self._maybe_trigger_gc()

            # Zpracov√°n√≠ posledn√≠ho chunky
            if chunk:
                processed_chunk = processor_func(chunk)
                yield processed_chunk

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi chunk processingu: {e}")
            raise
        finally:
            self._log_final_stats()

    def parallel_process_chunks(self,
                               data_stream: Generator[Dict[str, Any], None, None],
                               processor_func: callable,
                               max_workers: Optional[int] = None) -> Generator[Any, None, None]:
        """
        Paraleln√≠ zpracov√°n√≠ chunk≈Ø s ThreadPoolExecutor

        Args:
            data_stream: Generator dat
            processor_func: Funkce pro zpracov√°n√≠
            max_workers: Poƒçet worker≈Ø (None = pou≈æij config)

        Yields:
            Zpracovan√© v√Ωsledky
        """
        max_workers = max_workers or self.config.max_workers
        chunk_size = self.config.chunk_size

        logger.info(f"üîÑ Paraleln√≠ zpracov√°n√≠ s {max_workers} workery")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            chunk = []

            try:
                for item in data_stream:
                    chunk.append(item)

                    if len(chunk) >= chunk_size:
                        # Odesl√°n√≠ chunky na zpracov√°n√≠
                        future = executor.submit(processor_func, chunk.copy())
                        futures.append(future)
                        chunk.clear()

                        # Kontrola dokonƒçen√Ωch √∫kol≈Ø
                        self._check_completed_futures(futures)

                        # Memory pressure check
                        if len(futures) > max_workers * 2:
                            # ƒåek√°n√≠ na dokonƒçen√≠ nƒõkter√Ωch √∫kol≈Ø
                            completed_futures = []
                            for future in as_completed(futures[:max_workers]):
                                try:
                                    result = future.result()
                                    yield result
                                    completed_futures.append(future)
                                except Exception as e:
                                    logger.error(f"‚ùå Chyba p≈ôi zpracov√°n√≠ chunky: {e}")

                            # Odstranƒõn√≠ dokonƒçen√Ωch
                            for f in completed_futures:
                                futures.remove(f)

                # Zpracov√°n√≠ posledn√≠ho chunky
                if chunk:
                    future = executor.submit(processor_func, chunk)
                    futures.append(future)

                # ƒåek√°n√≠ na dokonƒçen√≠ v≈°ech √∫kol≈Ø
                for future in as_completed(futures):
                    try:
                        result = future.result()
                        yield result
                    except Exception as e:
                        logger.error(f"‚ùå Chyba p≈ôi fin√°ln√≠m zpracov√°n√≠: {e}")

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi paraleln√≠m zpracov√°n√≠: {e}")
                raise
            finally:
                self._log_final_stats()

    def _check_completed_futures(self, futures: List):
        """Kontrola a cleanup dokonƒçen√Ωch future objekt≈Ø"""
        completed = [f for f in futures if f.done()]
        for f in completed:
            futures.remove(f)

    def stream_csv_file(self, file_path: Union[str, Path],
                       delimiter: str = ',',
                       encoding: str = 'utf-8') -> Generator[Dict[str, str], None, None]:
        """
        Streamov√°n√≠ CSV souboru po ≈ô√°dc√≠ch

        Args:
            file_path: Cesta k CSV souboru
            delimiter: Oddƒõlovaƒç
            encoding: K√≥dov√°n√≠ souboru

        Yields:
            ≈ò√°dky jako slovn√≠ky
        """
        import csv

        file_path = Path(file_path)
        logger.info(f"üìä Streamov√°n√≠ CSV souboru: {file_path}")

        self._start_time = time.time()

        try:
            with open(file_path, 'r', encoding=encoding) as file:
                reader = csv.DictReader(file, delimiter=delimiter)

                for row in reader:
                    yield row
                    self._processed_count += 1
                    self._maybe_log_progress()
                    self._maybe_trigger_gc()

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi CSV streamov√°n√≠: {e}")
            raise

    def memory_efficient_file_processor(self,
                                      file_paths: List[Union[str, Path]],
                                      processor_func: callable) -> Generator[Any, None, None]:
        """
        Memory-efficient zpracov√°n√≠ v√≠ce soubor≈Ø

        Args:
            file_paths: Seznam cest k soubor≈Øm
            processor_func: Funkce pro zpracov√°n√≠ ka≈æd√©ho souboru

        Yields:
            V√Ωsledky zpracov√°n√≠
        """
        logger.info(f"üìÅ Zpracov√°v√°m {len(file_paths)} soubor≈Ø")

        for i, file_path in enumerate(file_paths, 1):
            logger.info(f"üìÑ Zpracov√°v√°m soubor {i}/{len(file_paths)}: {Path(file_path).name}")

            try:
                # Zpracov√°n√≠ jednoho souboru
                if Path(file_path).suffix.lower() == '.json':
                    data_stream = self.stream_json_file(file_path)
                elif Path(file_path).suffix.lower() == '.csv':
                    data_stream = self.stream_csv_file(file_path)
                else:
                    logger.warning(f"‚ö†Ô∏è Nepodporovan√Ω form√°t souboru: {file_path}")
                    continue

                result = processor_func(data_stream)
                yield result

                # Force GC po ka≈æd√©m souboru
                gc.collect()
                self._log_memory_usage()

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi zpracov√°n√≠ souboru {file_path}: {e}")
                continue

    def _maybe_log_progress(self):
        """Logov√°n√≠ pokroku p≈ôi zpracov√°n√≠"""
        if self._processed_count % self.config.progress_frequency == 0:
            elapsed = time.time() - self._start_time if self._start_time else 0
            rate = self._processed_count / elapsed if elapsed > 0 else 0
            logger.info(f"üìà Zpracov√°no {self._processed_count} z√°znam≈Ø ({rate:.1f}/s)")

    def _maybe_trigger_gc(self):
        """Spu≈°tƒõn√≠ GC p≈ôi pot≈ôebƒõ"""
        if self._processed_count % self.config.gc_frequency == 0:
            collected = gc.collect()
            if collected > 0:
                logger.debug(f"üßπ GC: {collected} objekt≈Ø uvolnƒõno")

    def _log_memory_usage(self):
        """Logov√°n√≠ aktu√°ln√≠ho vyu≈æit√≠ pamƒõti"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            memory_percent = (memory_mb / self.config.memory_limit_mb) * 100

            if memory_percent > 80:
                logger.warning(f"‚ö†Ô∏è Vysok√© vyu≈æit√≠ pamƒõti: {memory_mb:.1f}MB ({memory_percent:.1f}%)")
            else:
                logger.debug(f"üíæ Pamƒõ≈•: {memory_mb:.1f}MB ({memory_percent:.1f}%)")

        except Exception as e:
            logger.debug(f"Chyba p≈ôi kontrole pamƒõti: {e}")

    def _log_final_stats(self):
        """Fin√°ln√≠ statistiky zpracov√°n√≠"""
        if self._start_time:
            elapsed = time.time() - self._start_time
            rate = self._processed_count / elapsed if elapsed > 0 else 0

            logger.info(f"‚úÖ Zpracov√°n√≠ dokonƒçeno:")
            logger.info(f"üìä Celkem z√°znam≈Ø: {self._processed_count}")
            logger.info(f"‚è±Ô∏è ƒåas: {elapsed:.2f}s")
            logger.info(f"üöÄ Rychlost: {rate:.1f} z√°znam≈Ø/s")

            self._log_memory_usage()


# Utility funkce pro jednoduch√© pou≈æit√≠
def stream_large_json(file_path: Union[str, Path],
                     chunk_size: int = 1000) -> Generator[List[Dict[str, Any]], None, None]:
    """
    Jednoduch√° funkce pro streamov√°n√≠ velk√©ho JSON souboru

    Args:
        file_path: Cesta k JSON souboru
        chunk_size: Velikost chunk≈Ø

    Yields:
        Chunky JSON objekt≈Ø
    """
    config = StreamingConfig(chunk_size=chunk_size)
    processor = M1StreamingDataProcessor(config)

    data_stream = processor.stream_json_file(file_path)

    def identity_processor(chunk):
        return chunk

    yield from processor.process_data_chunks(data_stream, identity_processor)


if __name__ == "__main__":
    # Test streamovan√©ho zpracov√°n√≠
    print("üß™ Testov√°n√≠ M1 streamovan√©ho procesoru...")

    # Vytvo≈ôen√≠ test dat
    test_file = Path("test_large_data.json")
    test_data = [{"id": i, "text": f"Document {i}", "value": i * 0.1} for i in range(10000)]

    with open(test_file, 'w') as f:
        json.dump(test_data, f)

    print(f"üìÅ Vytvo≈ôen test soubor: {test_file} ({test_file.stat().st_size / 1024:.1f}KB)")

    # Test streamov√°n√≠
    processor = M1StreamingDataProcessor()

    def test_processor(chunk):
        # Simulace zpracov√°n√≠
        return len(chunk)

    total_processed = 0
    for chunk_result in processor.process_data_chunks(
        processor.stream_json_file(test_file),
        test_processor,
        chunk_size=500
    ):
        total_processed += chunk_result
        print(f"Chunk zpracov√°n: {chunk_result} z√°znam≈Ø")

    print(f"‚úÖ Celkem zpracov√°no: {total_processed} z√°znam≈Ø")

    # Cleanup
    test_file.unlink()
    print("üßπ Test soubor smaz√°n")
