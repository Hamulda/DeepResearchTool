"""
M1 Metal-optimalizovanÃ½ LLM klient
PlnÃ© vyuÅ¾itÃ­ Metal Performance Shaders pro dramatickÃ© zrychlenÃ­ inference
OptimalizovanÃ© pro MacBook Air M1 (8GB RAM) s kvantizovanÃ½mi modely
"""

import gc
import logging
import time
import threading
from typing import Dict, Any, Optional, List, Union, Generator
from dataclasses import dataclass
from pathlib import Path
import psutil
import json

logger = logging.getLogger(__name__)

try:
    import torch
    TORCH_AVAILABLE = torch.backends.mps.is_available()
    logger.info(f"ğŸ”¥ Metal Performance Shaders: {'âœ… DostupnÃ©' if TORCH_AVAILABLE else 'âŒ NedostupnÃ©'}")
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorch nenÃ­ dostupnÃ½")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("Ollama nenÃ­ dostupnÃ½")


@dataclass
class M1ModelConfig:
    """Konfigurace pro M1-optimalizovanÃ© modely"""
    model_name: str = "llama3:8b-instruct-q4_K_M"  # KvantizovanÃ½ model
    max_tokens: int = 2048  # KonzervativnÃ­ pro 8GB RAM
    temperature: float = 0.7
    top_p: float = 0.9
    repeat_penalty: float = 1.1

    # M1-specifickÃ© optimalizace
    use_metal: bool = True
    num_gpu_layers: int = -1  # VÅ¡echny vrstvy na GPU
    num_threads: int = 4  # VyuÅ¾itÃ­ P-cores
    context_window: int = 4096  # RozumnÃ¡ velikost kontextu

    # Memory management
    low_vram: bool = True  # Optimalizace pro unified memory
    batch_size: int = 1  # KonzervativnÃ­ batch size
    offload_kqv: bool = True  # Offload key-query-value na GPU


class M1MetalLLMClient:
    """
    M1 Metal-optimalizovanÃ½ LLM klient

    KlÃ­ÄovÃ© optimalizace:
    - Metal Performance Shaders pro GPU acceleration
    - KvantizovanÃ© modely (Q4_K_M) pro pamÄ›Å¥ovou efektivitu
    - Smart batch processing s M1 memory constraints
    - AutomatickÃ© fallback mechanismy
    - Token/s monitoring a optimalizace
    """

    def __init__(self,
                 config: Optional[M1ModelConfig] = None,
                 model_cache_dir: Optional[str] = None):
        self.config = config or M1ModelConfig()
        self.model_cache_dir = Path(model_cache_dir) if model_cache_dir else Path("./models")

        # Performance tracking
        self._stats = {
            'total_tokens_generated': 0,
            'total_inference_time': 0.0,
            'requests_count': 0,
            'metal_acceleration_used': False,
            'average_tokens_per_second': 0.0
        }

        # Thread safety
        self._lock = threading.Lock()

        # Model state
        self._model_loaded = False
        self._last_memory_check = 0

        # Inicializace
        self._setup_metal_environment()
        self._validate_model_availability()

        logger.info(f"ğŸš€ M1 Metal LLM klient inicializovÃ¡n")
        logger.info(f"ğŸ¤– Model: {self.config.model_name}")
        logger.info(f"ğŸ”¥ Metal: {'âœ…' if self.config.use_metal and TORCH_AVAILABLE else 'âŒ'}")

    def _setup_metal_environment(self):
        """NastavenÃ­ Metal environment pro M1"""
        if not TORCH_AVAILABLE:
            logger.warning("âš ï¸ Metal Performance Shaders nejsou dostupnÃ©")
            self.config.use_metal = False
            return

        try:
            # NastavenÃ­ PyTorch pro MPS
            if torch.backends.mps.is_available():
                torch.backends.mps.empty_cache()
                self._stats['metal_acceleration_used'] = True
                logger.info("âœ… Metal Performance Shaders aktivovÃ¡ny")
            else:
                logger.warning("âš ï¸ MPS nenÃ­ dostupnÃ½, pouÅ¾iji CPU")
                self.config.use_metal = False

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i nastavenÃ­ Metal: {e}")
            self.config.use_metal = False

    def _validate_model_availability(self):
        """Kontrola dostupnosti modelu"""
        if not OLLAMA_AVAILABLE:
            logger.error("âŒ Ollama nenÃ­ dostupnÃ½. Nainstalujte: brew install ollama")
            return

        try:
            # Kontrola bÄ›Å¾Ã­cÃ­ho Ollama serveru
            models = ollama.list()
            available_models = [model['name'] for model in models['models']]

            if self.config.model_name in available_models:
                logger.info(f"âœ… Model {self.config.model_name} je dostupnÃ½")
            else:
                logger.warning(f"âš ï¸ Model {self.config.model_name} nenÃ­ dostupnÃ½")
                logger.info(f"ğŸ“‹ DostupnÃ© modely: {available_models}")

                # Pokus o pull modelu
                self._pull_model_if_needed()

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i kontrole modelu: {e}")

    def _pull_model_if_needed(self):
        """StaÅ¾enÃ­ modelu pokud nenÃ­ dostupnÃ½"""
        try:
            logger.info(f"ğŸ“¥ Stahuji model {self.config.model_name}...")
            ollama.pull(self.config.model_name)
            logger.info(f"âœ… Model {self.config.model_name} ÃºspÄ›Å¡nÄ› staÅ¾en")
        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i stahovÃ¡nÃ­ modelu: {e}")

    def generate(self,
                prompt: str,
                max_tokens: Optional[int] = None,
                temperature: Optional[float] = None,
                stream: bool = False) -> Union[str, Generator[str, None, None]]:
        """
        GenerovÃ¡nÃ­ textu s M1 Metal optimalizacemi

        Args:
            prompt: VstupnÃ­ prompt
            max_tokens: MaximÃ¡lnÃ­ poÄet tokenÅ¯ (None = pouÅ¾ij config)
            temperature: Teplota (None = pouÅ¾ij config)
            stream: StreamovÃ¡nÃ­ vÃ½stupu

        Returns:
            VygenerovanÃ½ text nebo generator pro streaming
        """
        max_tokens = max_tokens or self.config.max_tokens
        temperature = temperature or self.config.temperature

        logger.debug(f"ğŸ§  Generuji odpovÄ›Ä (max_tokens: {max_tokens}, temp: {temperature})")

        # Memory check pÅ™ed inference
        self._check_memory_pressure()

        start_time = time.time()

        try:
            # PÅ™Ã­prava Ollama parametrÅ¯ s M1 optimalizacemi
            options = self._prepare_ollama_options(max_tokens, temperature)

            if stream:
                return self._generate_stream(prompt, options)
            else:
                return self._generate_complete(prompt, options, start_time)

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i generovÃ¡nÃ­: {e}")
            raise

    def _prepare_ollama_options(self, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """PÅ™Ã­prava Ollama options s M1 optimalizacemi"""
        options = {
            'num_predict': max_tokens,
            'temperature': temperature,
            'top_p': self.config.top_p,
            'repeat_penalty': self.config.repeat_penalty,

            # M1-specifickÃ© optimalizace
            'num_gpu': self.config.num_gpu_layers if self.config.use_metal else 0,
            'num_thread': self.config.num_threads,
            'num_ctx': self.config.context_window,

            # Memory optimalizace
            'low_vram': self.config.low_vram,
            'f16_kv': True,  # Half precision pro key-value cache
            'use_mlock': False,  # Nenechat model v pamÄ›ti permanent
        }

        return options

    def _generate_complete(self, prompt: str, options: Dict[str, Any], start_time: float) -> str:
        """KompletnÃ­ generovÃ¡nÃ­ (non-streaming)"""
        try:
            response = ollama.generate(
                model=self.config.model_name,
                prompt=prompt,
                options=options,
                stream=False
            )

            generated_text = response['response']

            # Performance tracking
            inference_time = time.time() - start_time
            tokens_generated = len(generated_text.split())  # Aproximace
            tokens_per_second = tokens_generated / inference_time if inference_time > 0 else 0

            # Update stats
            with self._lock:
                self._stats['total_tokens_generated'] += tokens_generated
                self._stats['total_inference_time'] += inference_time
                self._stats['requests_count'] += 1
                self._stats['average_tokens_per_second'] = (
                    self._stats['total_tokens_generated'] / self._stats['total_inference_time']
                    if self._stats['total_inference_time'] > 0 else 0
                )

            logger.debug(f"âœ… GenerovÃ¡nÃ­ dokonÄeno: {tokens_generated} tokenÅ¯ za {inference_time:.2f}s ({tokens_per_second:.1f} tok/s)")

            # Memory cleanup po inference
            self._cleanup_after_inference()

            return generated_text

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i kompletnÃ­m generovÃ¡nÃ­: {e}")
            raise

    def _generate_stream(self, prompt: str, options: Dict[str, Any]) -> Generator[str, None, None]:
        """StreamovanÃ© generovÃ¡nÃ­"""
        try:
            response_stream = ollama.generate(
                model=self.config.model_name,
                prompt=prompt,
                options=options,
                stream=True
            )

            for chunk in response_stream:
                if 'response' in chunk:
                    yield chunk['response']

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i streamovanÃ©m generovÃ¡nÃ­: {e}")
            raise

    def batch_generate(self,
                      prompts: List[str],
                      max_tokens: Optional[int] = None,
                      temperature: Optional[float] = None) -> List[str]:
        """
        Batch generovÃ¡nÃ­ s M1 memory management

        Args:
            prompts: Seznam promptÅ¯
            max_tokens: MaximÃ¡lnÃ­ poÄet tokenÅ¯
            temperature: Teplota

        Returns:
            Seznam vygenerovanÃ½ch odpovÄ›dÃ­
        """
        logger.info(f"ğŸ”„ Batch generovÃ¡nÃ­ {len(prompts)} promptÅ¯")

        results = []
        batch_start_time = time.time()

        for i, prompt in enumerate(prompts, 1):
            logger.debug(f"ğŸ“ ZpracovÃ¡vÃ¡m prompt {i}/{len(prompts)}")

            try:
                result = self.generate(prompt, max_tokens, temperature)
                results.append(result)

                # M1 Memory management
                if i % 3 == 0:  # GC kaÅ¾dÃ© 3 requesty
                    self._cleanup_after_inference()

                # Progress log
                if i % 5 == 0 or i == len(prompts):
                    elapsed = time.time() - batch_start_time
                    avg_time = elapsed / i
                    eta = avg_time * (len(prompts) - i)
                    logger.info(f"ğŸ“Š Pokrok: {i}/{len(prompts)} ({avg_time:.1f}s/prompt, ETA: {eta:.0f}s)")

            except Exception as e:
                logger.error(f"âŒ Chyba pÅ™i zpracovÃ¡nÃ­ promptu {i}: {e}")
                results.append(f"ERROR: {str(e)}")

        total_time = time.time() - batch_start_time
        logger.info(f"âœ… Batch generovÃ¡nÃ­ dokonÄeno za {total_time:.2f}s")

        return results

    def _check_memory_pressure(self):
        """Kontrola memory pressure pÅ™ed inference"""
        current_time = time.time()

        # Kontrola pouze kaÅ¾dÃ½ch 30 sekund
        if current_time - self._last_memory_check < 30:
            return

        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024

            # VarovÃ¡nÃ­ pÅ™i vysokÃ©m vyuÅ¾itÃ­ pamÄ›ti
            if memory_mb > 6000:  # 6GB+ na 8GB systÃ©mu
                logger.warning(f"âš ï¸ VysokÃ© vyuÅ¾itÃ­ pamÄ›ti: {memory_mb:.0f}MB")
                self._cleanup_after_inference()

            # Metal memory cache cleanup
            if self.config.use_metal and TORCH_AVAILABLE:
                torch.backends.mps.empty_cache()

            self._last_memory_check = current_time

        except Exception as e:
            logger.debug(f"Memory check error: {e}")

    def _cleanup_after_inference(self):
        """Cleanup po inference pro M1 optimalizaci"""
        try:
            # Python garbage collection
            collected = gc.collect()

            # Metal cache cleanup
            if self.config.use_metal and TORCH_AVAILABLE:
                torch.backends.mps.empty_cache()

            if collected > 0:
                logger.debug(f"ğŸ§¹ Cleanup: {collected} objektÅ¯ uvolnÄ›no")

        except Exception as e:
            logger.debug(f"Cleanup error: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """Informace o aktuÃ¡lnÃ­m modelu"""
        try:
            if not OLLAMA_AVAILABLE:
                return {'error': 'Ollama nenÃ­ dostupnÃ½'}

            # Ollama show command pro model info
            model_info = ollama.show(self.config.model_name)

            return {
                'model_name': self.config.model_name,
                'model_info': model_info,
                'metal_enabled': self.config.use_metal and TORCH_AVAILABLE,
                'configuration': {
                    'max_tokens': self.config.max_tokens,
                    'context_window': self.config.context_window,
                    'num_gpu_layers': self.config.num_gpu_layers,
                    'temperature': self.config.temperature
                }
            }

        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i zÃ­skÃ¡vÃ¡nÃ­ model info: {e}")
            return {'error': str(e)}

    def get_performance_stats(self) -> Dict[str, Any]:
        """Statistiky vÃ½konu"""
        with self._lock:
            # Memory usage
            try:
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
            except:
                memory_mb = 0

            return {
                **self._stats,
                'current_memory_mb': round(memory_mb, 1),
                'metal_acceleration': self.config.use_metal and TORCH_AVAILABLE,
                'model_loaded': self._model_loaded,
                'requests_per_minute': (self._stats['requests_count'] * 60 /
                                      max(1, self._stats['total_inference_time']))
            }

    def optimize_for_m1(self):
        """SpuÅ¡tÄ›nÃ­ M1-specifickÃ½ch optimalizacÃ­"""
        logger.info("âš¡ SpouÅ¡tÃ­m M1 LLM optimalizace...")

        # Memory cleanup
        self._cleanup_after_inference()

        # Model cache optimalizace
        if OLLAMA_AVAILABLE:
            try:
                # Preload model do cache
                ollama.generate(
                    model=self.config.model_name,
                    prompt="Test",
                    options={'num_predict': 1}
                )
                self._model_loaded = True
                logger.info("âœ… Model preloaded do cache")
            except Exception as e:
                logger.warning(f"âš ï¸ Model preload selhal: {e}")

        # Performance stats
        stats = self.get_performance_stats()
        logger.info(f"ğŸ“Š LLM Performance:")
        logger.info(f"   Tokens/s: {stats['average_tokens_per_second']:.1f}")
        logger.info(f"   Memory: {stats['current_memory_mb']}MB")
        logger.info(f"   Metal: {'âœ…' if stats['metal_acceleration'] else 'âŒ'}")

        return stats


# Utility funkce pro snadnÃ© pouÅ¾itÃ­
def create_m1_llm_client(model_name: str = "llama3:8b-instruct-q4_K_M",
                        max_tokens: int = 2048,
                        use_metal: bool = True) -> M1MetalLLMClient:
    """Factory funkce pro vytvoÅ™enÃ­ M1-optimalizovanÃ©ho LLM klienta"""
    config = M1ModelConfig(
        model_name=model_name,
        max_tokens=max_tokens,
        use_metal=use_metal
    )

    return M1MetalLLMClient(config=config)


if __name__ == "__main__":
    # Test M1 Metal LLM klienta
    print("ğŸ§ª TestovÃ¡nÃ­ M1 Metal LLM klienta...")

    # VytvoÅ™enÃ­ klienta
    client = create_m1_llm_client()

    # Test single generation
    test_prompt = "VysvÄ›tli struÄnÄ›, co je umÄ›lÃ¡ inteligence."

    print(f"ğŸ¤– Test prompt: {test_prompt}")

    try:
        start_time = time.time()
        response = client.generate(test_prompt, max_tokens=100)
        elapsed = time.time() - start_time

        print(f"âœ… OdpovÄ›Ä ({elapsed:.2f}s):")
        print(response[:200] + "..." if len(response) > 200 else response)

        # Performance stats
        stats = client.get_performance_stats()
        print(f"\nğŸ“Š Performance:")
        print(f"Tokens/s: {stats['average_tokens_per_second']:.1f}")
        print(f"Memory: {stats['current_memory_mb']}MB")
        print(f"Metal: {stats['metal_acceleration']}")

    except Exception as e:
        print(f"âŒ Test selhal: {e}")

    print("âœ… Test dokonÄen!")
