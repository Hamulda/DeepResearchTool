"""Graph-Powered RAG System - F√°ze 2: Anal√Ωza a RAG s podporou grafu
Hybridn√≠ RAG syst√©m kombinuj√≠c√≠ textov√© vyhled√°v√°n√≠ s grafov√Ωmi dotazy
"""

import asyncio
from datetime import datetime
import logging
import sys
from typing import Any

import lancedb
from sentence_transformers import SentenceTransformer

# P≈ôidej src do path
sys.path.append("/app/src")

from knowledge_graph import KnowledgeGraphManager
from text_to_cypher import TextToCypherConverter

# Setup logging
logger = logging.getLogger(__name__)


class GraphPoweredRAG:
    """Hybridn√≠ RAG syst√©m s podporou znalostn√≠ho grafu"""

    def __init__(self, vector_db_path: str = "/app/data/vector_db"):
        self.vector_db_path = vector_db_path

        # Inicializuj komponenty
        self.db = lancedb.connect(str(vector_db_path))
        self.sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

        try:
            self.kg_manager = KnowledgeGraphManager()
            logger.info("‚úÖ Knowledge Graph Manager p≈ôipojen")
        except Exception as e:
            logger.error(f"‚ùå Knowledge Graph nedostupn√Ω: {e}")
            self.kg_manager = None

        try:
            self.cypher_converter = TextToCypherConverter()
            logger.info("‚úÖ Text-to-Cypher converter p≈ôipraven")
        except Exception as e:
            logger.error(f"‚ùå Text-to-Cypher nedostupn√Ω: {e}")
            self.cypher_converter = None

        logger.info("‚úÖ Graph-Powered RAG inicializov√°n")

    async def hybrid_search(
        self,
        query: str,
        limit: int = 10,
        vector_weight: float = 0.6,
        graph_weight: float = 0.4,
        task_id: str = None,
    ) -> dict[str, Any]:
        """Hybridn√≠ vyhled√°v√°n√≠ kombinuj√≠c√≠ vektorov√© a grafov√© v√Ωsledky

        Args:
            query: U≈æivatelsk√Ω dotaz
            limit: Maxim√°ln√≠ poƒçet v√Ωsledk≈Ø
            vector_weight: V√°ha vektorov√Ωch v√Ωsledk≈Ø (0-1)
            graph_weight: V√°ha grafov√Ωch v√Ωsledk≈Ø (0-1)
            task_id: ID √∫lohy pro filtrov√°n√≠

        Returns:
            Kombinovan√© v√Ωsledky s kontextem z obou zdroj≈Ø

        """
        try:
            logger.info(
                f"üîç Hybridn√≠ vyhled√°v√°n√≠: '{query}' (vector: {vector_weight}, graph: {graph_weight})"
            )

            # Paraleln√≠ vyhled√°v√°n√≠
            vector_task = self._vector_search(query, limit, task_id)
            graph_task = self._graph_search(query, limit)

            vector_results, graph_results = await asyncio.gather(
                vector_task, graph_task, return_exceptions=True
            )

            # Zpracuj v√Ωsledky (i v p≈ô√≠padƒõ chyb)
            if isinstance(vector_results, Exception):
                logger.error(f"‚ùå Vektorov√© vyhled√°v√°n√≠ selhalo: {vector_results}")
                vector_results = {"success": False, "results": []}

            if isinstance(graph_results, Exception):
                logger.error(f"‚ùå Grafov√© vyhled√°v√°n√≠ selhalo: {graph_results}")
                graph_results = {"success": False, "results": [], "graph_context": []}

            # Kombinuj v√Ωsledky
            combined_results = self._combine_results(
                vector_results, graph_results, vector_weight, graph_weight, limit
            )

            # Vytvo≈ô fin√°ln√≠ kontext pro LLM
            llm_context = self._create_llm_context(
                query, combined_results, graph_results.get("graph_context", [])
            )

            return {
                "success": True,
                "query": query,
                "results": combined_results,
                "llm_context": llm_context,
                "sources": {
                    "vector_results": len(vector_results.get("results", [])),
                    "graph_results": len(graph_results.get("results", [])),
                    "graph_context_items": len(graph_results.get("graph_context", [])),
                },
                "search_metadata": {
                    "vector_weight": vector_weight,
                    "graph_weight": graph_weight,
                    "timestamp": datetime.now().isoformat(),
                },
            }

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi hybridn√≠m vyhled√°v√°n√≠: {e}")
            return {
                "success": False,
                "query": query,
                "error": str(e),
                "fallback_results": await self._fallback_search(query, limit, task_id),
            }

    async def _vector_search(self, query: str, limit: int, task_id: str = None) -> dict[str, Any]:
        """Vektorov√© vyhled√°v√°n√≠ v LanceDB"""
        try:
            # Generuj embedding pro query
            query_embedding = self.sentence_model.encode([query])[0].tolist()

            # Najdi relevantn√≠ tabulky
            if task_id:
                table_names = [f"rag_documents_{task_id}"]
            else:
                table_names = [
                    name for name in self.db.table_names() if name.startswith("rag_documents_")
                ]

            all_results = []

            for table_name in table_names:
                try:
                    table = self.db.open_table(table_name)
                    results = table.search(query_embedding).limit(limit).to_list()

                    for result in results:
                        result["similarity_score"] = float(result.get("_distance", 0))
                        result["source_type"] = "vector"
                        result["table_source"] = table_name
                        all_results.append(result)

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Chyba v tabulce {table_name}: {e}")
                    continue

            # Se≈ôaƒè podle similarity score
            all_results.sort(key=lambda x: x["similarity_score"], reverse=True)

            return {
                "success": True,
                "results": all_results[:limit],
                "total_found": len(all_results),
            }

        except Exception as e:
            logger.error(f"‚ùå Vektorov√© vyhled√°v√°n√≠ selhalo: {e}")
            return {"success": False, "results": [], "error": str(e)}

    async def _graph_search(self, query: str, limit: int) -> dict[str, Any]:
        """Grafov√© vyhled√°v√°n√≠ pomoc√≠ Cypher dotaz≈Ø"""
        try:
            if not self.kg_manager or not self.cypher_converter:
                return {"success": False, "results": [], "graph_context": []}

            # P≈ôeveƒè dotaz na Cypher
            cypher_result = await self.cypher_converter.convert_to_cypher(query)

            if not cypher_result["success"]:
                logger.warning("‚ö†Ô∏è Text-to-Cypher p≈ôevod selhal")
                return {"success": False, "results": [], "graph_context": []}

            cypher_query = cypher_result["cypher_query"]
            logger.info(f"üìä Spou≈°t√≠m Cypher: {cypher_query[:100]}...")

            # Spus≈• Cypher dotaz
            graph_results = await self._execute_cypher_query(cypher_query)

            # Z√≠skej dodateƒçn√Ω grafov√Ω kontext
            graph_context = await self._get_graph_context(query)

            return {
                "success": True,
                "results": graph_results,
                "graph_context": graph_context,
                "cypher_query": cypher_query,
                "cypher_method": cypher_result["method"],
            }

        except Exception as e:
            logger.error(f"‚ùå Grafov√© vyhled√°v√°n√≠ selhalo: {e}")
            return {"success": False, "results": [], "graph_context": [], "error": str(e)}

    async def _execute_cypher_query(self, cypher_query: str) -> list[dict[str, Any]]:
        """Spus≈• Cypher dotaz a form√°tuj v√Ωsledky"""
        try:
            # Spus≈• dotaz p≈ôes Knowledge Graph Manager
            # Pro obecn√© Cypher dotazy pou≈æijeme Neo4j driver p≈ô√≠mo
            with self.kg_manager.driver.session() as session:
                result = session.run(cypher_query)

                formatted_results = []
                for record in result:
                    # P≈ôeveƒè Neo4j record na dict
                    record_dict = {}
                    for key in record.keys():
                        value = record[key]
                        if hasattr(value, "items"):  # Node object
                            record_dict[key] = dict(value)
                        else:
                            record_dict[key] = value

                    record_dict["source_type"] = "graph"
                    formatted_results.append(record_dict)

                return formatted_results

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi spou≈°tƒõn√≠ Cypher: {e}")
            return []

    async def _get_graph_context(self, query: str) -> list[dict[str, Any]]:
        """Z√≠skej dodateƒçn√Ω kontext ze znalostn√≠ho grafu"""
        try:
            context_items = []

            # Extrahuj kl√≠ƒçov√° slova z dotazu
            keywords = self._extract_keywords(query)

            for keyword in keywords[:3]:  # Max 3 kl√≠ƒçov√° slova
                # Najdi entity obsahuj√≠c√≠ kl√≠ƒçov√© slovo
                entities = await self.kg_manager.query_entities(entity_text=keyword, limit=5)

                for entity in entities:
                    # Z√≠skej sousedy t√©to entity
                    neighbors = await self.kg_manager.get_entity_neighbors(
                        entity["text"], max_depth=1, limit=5
                    )

                    context_items.append(
                        {
                            "type": "entity_network",
                            "center_entity": entity["text"],
                            "entity_type": entity["type"],
                            "neighbors": neighbors["neighbors"],
                            "keyword": keyword,
                        }
                    )

            return context_items[:10]  # Omez kontext

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi z√≠sk√°v√°n√≠ grafov√©ho kontextu: {e}")
            return []

    def _extract_keywords(self, query: str) -> list[str]:
        """Extrahuj kl√≠ƒçov√° slova z dotazu"""
        import re

        # Z√°kladn√≠ ƒçi≈°tƒõn√≠
        query_clean = re.sub(r"[^\w\s]", " ", query.lower())
        words = query_clean.split()

        # Odstra≈à stop words
        stop_words = {
            "a",
            "an",
            "and",
            "are",
            "as",
            "at",
            "be",
            "by",
            "for",
            "from",
            "has",
            "he",
            "in",
            "is",
            "it",
            "its",
            "of",
            "on",
            "that",
            "the",
            "to",
            "was",
            "what",
            "when",
            "where",
            "who",
            "will",
            "with",
            "this",
            "these",
            "they",
            "we",
            "you",
        }

        keywords = [word for word in words if len(word) > 2 and word not in stop_words]

        return keywords[:5]  # Max 5 kl√≠ƒçov√Ωch slov

    def _combine_results(
        self,
        vector_results: dict[str, Any],
        graph_results: dict[str, Any],
        vector_weight: float,
        graph_weight: float,
        limit: int,
    ) -> list[dict[str, Any]]:
        """Kombinuj v√Ωsledky z vektorov√©ho a grafov√©ho vyhled√°v√°n√≠"""
        try:
            combined = []

            # P≈ôidej vektorov√© v√Ωsledky s v√°hou
            for result in vector_results.get("results", []):
                result_copy = result.copy()
                result_copy["final_score"] = result.get("similarity_score", 0) * vector_weight
                result_copy["score_components"] = {
                    "vector_score": result.get("similarity_score", 0),
                    "graph_score": 0,
                    "vector_weight": vector_weight,
                    "graph_weight": 0,
                }
                combined.append(result_copy)

            # P≈ôidej grafov√© v√Ωsledky s v√°hou
            for result in graph_results.get("results", []):
                result_copy = result.copy()
                # Pro grafov√© v√Ωsledky pou≈æij konstantn√≠ sk√≥re
                graph_score = 0.8  # Vysok√© sk√≥re pro p≈ôesn√© grafov√© shody
                result_copy["final_score"] = graph_score * graph_weight
                result_copy["score_components"] = {
                    "vector_score": 0,
                    "graph_score": graph_score,
                    "vector_weight": 0,
                    "graph_weight": graph_weight,
                }
                combined.append(result_copy)

            # Se≈ôaƒè podle fin√°ln√≠ho sk√≥re
            combined.sort(key=lambda x: x.get("final_score", 0), reverse=True)

            return combined[:limit]

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi kombinov√°n√≠ v√Ωsledk≈Ø: {e}")
            return vector_results.get("results", [])[:limit]

    def _create_llm_context(
        self,
        query: str,
        combined_results: list[dict[str, Any]],
        graph_context: list[dict[str, Any]],
    ) -> str:
        """Vytvo≈ô strukturovan√Ω kontext pro LLM"""
        try:
            context_parts = []

            # P≈ôidej u≈æivatelsk√Ω dotaz
            context_parts.append(f"U≈ΩIVATELSK√ù DOTAZ: {query}\n")

            # P≈ôidej textov√© v√Ωsledky
            context_parts.append("RELEVANTN√ç TEXTOV√â DOKUMENTY:")
            text_results = [r for r in combined_results if r.get("source_type") == "vector"]

            for i, result in enumerate(text_results[:5], 1):
                chunk_text = result.get("chunk_text", "")[:300]
                url = result.get("url", "N/A")
                score = result.get("final_score", 0)

                context_parts.append(f"{i}. Zdroj: {url}")
                context_parts.append(f"   Sk√≥re: {score:.3f}")
                context_parts.append(f"   Text: {chunk_text}...\n")

            # P≈ôidej grafov√© v√Ωsledky
            graph_results = [r for r in combined_results if r.get("source_type") == "graph"]
            if graph_results:
                context_parts.append("STRUKTUROVAN√â INFORMACE ZE ZNALOSTN√çHO GRAFU:")

                for i, result in enumerate(graph_results[:5], 1):
                    context_parts.append(f"{i}. {self._format_graph_result(result)}")

            # P≈ôidej s√≠≈•ov√Ω kontext
            if graph_context:
                context_parts.append("S√ç≈§OV√ù KONTEXT (entity a jejich spojen√≠):")

                for i, context_item in enumerate(graph_context[:3], 1):
                    if context_item["type"] == "entity_network":
                        center = context_item["center_entity"]
                        neighbors = context_item["neighbors"]

                        context_parts.append(f"{i}. Entita '{center}' je spojena s:")
                        for neighbor in neighbors[:3]:
                            relations = " ‚Üí ".join(neighbor.get("relations", []))
                            context_parts.append(
                                f"   - {neighbor['entity']} ({neighbor['type']}) via {relations}"
                            )

            return "\n".join(context_parts)

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi vytv√°≈ôen√≠ LLM kontextu: {e}")
            return f"DOTAZ: {query}\n\nKONTEXT: Chyba p≈ôi zpracov√°n√≠ kontextu."

    def _format_graph_result(self, result: dict[str, Any]) -> str:
        """Form√°tuj v√Ωsledek z grafu pro LLM"""
        try:
            formatted_parts = []

            for key, value in result.items():
                if key in ["source_type", "final_score", "score_components"]:
                    continue

                if isinstance(value, dict):
                    # Neo4j node object
                    if "text" in value and "type" in value:
                        formatted_parts.append(f"{key}: {value['text']} ({value['type']})")
                    else:
                        formatted_parts.append(f"{key}: {str(value)[:100]}")
                else:
                    formatted_parts.append(f"{key}: {value}")

            return " | ".join(formatted_parts)

        except Exception:
            return str(result)[:200]

    async def _fallback_search(
        self, query: str, limit: int, task_id: str = None
    ) -> list[dict[str, Any]]:
        """Fallback vyhled√°v√°n√≠ p≈ôi selh√°n√≠ hybridn√≠ho p≈ô√≠stupu"""
        try:
            # Zkus alespo≈à vektorov√© vyhled√°v√°n√≠
            vector_results = await self._vector_search(query, limit, task_id)
            return vector_results.get("results", [])

        except Exception as e:
            logger.error(f"‚ùå I fallback vyhled√°v√°n√≠ selhalo: {e}")
            return []

    async def get_graph_statistics(self) -> dict[str, Any]:
        """Z√≠skej statistiky znalostn√≠ho grafu"""
        try:
            if not self.kg_manager:
                return {"error": "Knowledge Graph nedostupn√Ω"}

            return await self.kg_manager.get_graph_statistics()

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi z√≠sk√°v√°n√≠ statistik: {e}")
            return {"error": str(e)}

    def get_available_tables(self) -> list[str]:
        """Z√≠skej seznam dostupn√Ωch RAG tabulek"""
        try:
            return [name for name in self.db.table_names() if name.startswith("rag_documents_")]
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi z√≠sk√°v√°n√≠ tabulek: {e}")
            return []
