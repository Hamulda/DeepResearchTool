#!/usr/bin/env python3
"""
Smoke Test - Rychl√° validace (<60s, ‚â•1 claim, ‚â•2 citations)
Fail-hard test pro F√ÅZI 1 akceptaƒçn√≠ krit√©ria

Author: Senior Python/MLOps Agent
"""

import asyncio
import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from src.utils.gates import create_gate_manager, GateType
from src.core.enhanced_orchestrator import EnhancedOrchestrator
import config  # Import our config module


async def run_smoke_test() -> bool:
    """
    Spust√≠ smoke test s fail-hard krit√©rii:
    - Doba bƒõhu < 60 sekund
    - ‚â•1 claim vygenerov√°n
    - ‚â•2 citace per claim
    - V≈°echny validation gates pro≈°ly
    """
    print("üî• SMOKE TEST - DeepResearchTool")
    print("=" * 50)
    print("Krit√©ria:")
    print("  ‚è±Ô∏è  Doba bƒõhu: <60 sekund")
    print("  üìã Claims: ‚â•1")
    print("  üìö Citace: ‚â•2 per claim")
    print("  ‚úÖ Validation gates: v≈°echny")
    print()

    start_time = time.time()

    try:
        # Load config
        config_data = config.load_config("config_m1_local.yaml")

        # Initialize gate manager
        gate_manager = create_gate_manager(config_data)

        # Initialize orchestrator with quick profile
        config_data["research_profile"] = "quick"
        config_data["max_documents"] = 20  # Limit for speed
        config_data["synthesis"]["max_claims"] = 3  # Limit claims

        orchestrator = EnhancedOrchestrator(config_data)
        await orchestrator.initialize()

        # Test query
        test_query = "What are the latest developments in large language models?"

        print(f"üîç Testing query: {test_query}")
        print()

        # Run research
        result = await orchestrator.process_query(test_query)

        # Validate timing
        elapsed_time = time.time() - start_time
        print(f"‚è±Ô∏è  Elapsed time: {elapsed_time:.1f}s")

        if elapsed_time >= 60:
            print(f"‚ùå FAIL: Smoke test exceeded 60s limit ({elapsed_time:.1f}s)")
            return False

        # Validate structure
        if "claims" not in result:
            print("‚ùå FAIL: No 'claims' in result")
            return False

        claims = result["claims"]
        if len(claims) < 1:
            print(f"‚ùå FAIL: Expected ‚â•1 claims, got {len(claims)}")
            return False

        print(f"üìã Claims generated: {len(claims)}")

        # Validate citations per claim
        for i, claim in enumerate(claims):
            citations = claim.get("citations", [])
            if len(citations) < 2:
                print(f"‚ùå FAIL: Claim {i+1} has only {len(citations)} citations (required ‚â•2)")
                return False

            print(f"  üìö Claim {i+1}: {len(citations)} citations")

        # Prepare validation data
        validation_data = {
            "timestamp": datetime.now().isoformat(),
            "query": test_query,
            "claims": claims,
            "retrieval_metadata": result.get("retrieval_metadata", {}),
            "metrics": result.get("metrics", {})
        }

        # Run validation gates
        print("\nüö™ Running validation gates...")

        try:
            validation_report = await gate_manager.validate_all(validation_data)
            print("‚úÖ All validation gates passed")

        except Exception as e:
            print(f"‚ùå FAIL: Validation gate failed: {e}")
            return False

        # Save artifacts
        artifacts_dir = Path("artifacts")
        artifacts_dir.mkdir(exist_ok=True)

        # Save smoke test result
        smoke_result = {
            "timestamp": datetime.now().isoformat(),
            "elapsed_time_seconds": elapsed_time,
            "success": True,
            "query": test_query,
            "claims_count": len(claims),
            "total_citations": sum(len(claim.get("citations", [])) for claim in claims),
            "validation_report": validation_report,
            "performance_metrics": {
                "under_60s": elapsed_time < 60,
                "min_claims": len(claims) >= 1,
                "min_citations_per_claim": all(len(claim.get("citations", [])) >= 2 for claim in claims)
            }
        }

        with open(artifacts_dir / "smoke_test_result.json", "w") as f:
            json.dump(smoke_result, f, indent=2)

        print(f"\n‚úÖ SMOKE TEST PASSED")
        print(f"   Time: {elapsed_time:.1f}s / 60s")
        print(f"   Claims: {len(claims)}")
        print(f"   Citations: {sum(len(claim.get('citations', [])) for claim in claims)}")
        print(f"   Artifacts: artifacts/smoke_test_result.json")

        return True

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"‚ùå FAIL: Exception in smoke test: {e}")
        print(f"   Time: {elapsed_time:.1f}s")

        # Save failure report
        artifacts_dir = Path("artifacts")
        artifacts_dir.mkdir(exist_ok=True)

        failure_result = {
            "timestamp": datetime.now().isoformat(),
            "elapsed_time_seconds": elapsed_time,
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__
        }

        with open(artifacts_dir / "smoke_test_failure.json", "w") as f:
            json.dump(failure_result, f, indent=2)

        return False


async def main():
    """Main entry point"""
    success = await run_smoke_test()

    if success:
        print("\nüéâ Smoke test completed successfully!")
        sys.exit(0)
    else:
        print("\nüí• Smoke test failed!")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
