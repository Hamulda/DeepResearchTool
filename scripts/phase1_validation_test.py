#!/usr/bin/env python3
"""
Simplified FÃZE 1 Validation Test
Testuje pouze validation gates bez zÃ¡vislostÃ­ na complex modulech

Author: Senior Python/MLOps Agent
"""

import asyncio
import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime

# Add src to path
current_dir = os.path.dirname(__file__)
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# Direct import of gates without complex dependencies
try:
    from src.utils.gates import (
        GateManager,
        GateConfig,
        GateType,
        EvidenceGateError,
        ComplianceGateError,
        MetricsGateError,
        create_default_gate_config,
    )
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Running standalone validation test...")


async def test_validation_gates_standalone():
    """Standalone test validaÄnÃ­ch bran"""
    print("ğŸšª STANDALONE VALIDATION GATES TEST")
    print("=" * 50)

    # Create gate configuration
    config = create_default_gate_config()
    gate_manager = GateManager(config)

    print("âœ… Gate manager created successfully")

    # Test evidence gate with valid data
    print("\nğŸ“‹ Testing Evidence Gate...")
    valid_data = {
        "timestamp": datetime.now().isoformat(),
        "query": "Test query",
        "claims": [
            {
                "text": "Test claim 1",
                "citations": [
                    {"source_id": "source1", "passage": "Evidence 1"},
                    {"source_id": "source2", "passage": "Evidence 2"},
                ],
            },
            {
                "text": "Test claim 2",
                "citations": [
                    {"source_id": "source3", "passage": "Evidence 3"},
                    {"source_id": "source4", "passage": "Evidence 4"},
                ],
            },
        ],
        "retrieval_metadata": {
            "robots_violations": [],
            "rate_limit_violations": [],
            "accessed_domains": ["example.com"],
        },
        "metrics": {"recall_at_10": 0.75, "ndcg_at_10": 0.65, "citation_precision": 0.85},
    }

    try:
        report = await gate_manager.validate_single(GateType.EVIDENCE, valid_data)
        print(
            f"âœ… Evidence gate passed: {report['total_claims']} claims, {report['total_citations']} citations"
        )
    except Exception as e:
        print(f"âŒ Evidence gate failed: {e}")
        return False

    # Test compliance gate
    print("\nğŸ›¡ï¸ Testing Compliance Gate...")
    try:
        report = await gate_manager.validate_single(GateType.COMPLIANCE, valid_data)
        print("âœ… Compliance gate passed")
    except Exception as e:
        print(f"âŒ Compliance gate failed: {e}")
        return False

    # Test metrics gate
    print("\nğŸ“Š Testing Metrics Gate...")
    try:
        report = await gate_manager.validate_single(GateType.METRICS, valid_data)
        print("âœ… Metrics gate passed")
    except Exception as e:
        print(f"âŒ Metrics gate failed: {e}")
        return False

    # Test all gates together
    print("\nğŸšª Testing All Gates Together...")
    try:
        validation_report = await gate_manager.validate_all(valid_data)
        print("âœ… All validation gates passed")

        # Verify report structure
        assert validation_report["overall_passed"] is True
        assert len(validation_report["failed_gates"]) == 0
        assert "evidence" in validation_report["gates"]
        assert "compliance" in validation_report["gates"]
        assert "metrics" in validation_report["gates"]

        print(f"ğŸ“Š Validation report: {len(validation_report['gates'])} gates validated")

    except Exception as e:
        print(f"âŒ All gates validation failed: {e}")
        return False

    # Test fail-hard behavior
    print("\nğŸ’¥ Testing Fail-Hard Behavior...")
    invalid_data = {
        "claims": [],  # This should fail evidence gate
        "retrieval_metadata": {
            "robots_violations": [],
            "rate_limit_violations": [],
            "accessed_domains": [],
        },
        "metrics": {"recall_at_10": 0.75, "ndcg_at_10": 0.65, "citation_precision": 0.85},
    }

    try:
        await gate_manager.validate_all(invalid_data)
        print("âŒ Fail-hard test failed - should have thrown exception")
        return False
    except EvidenceGateError as e:
        print(f"âœ… Fail-hard behavior correct: {str(e)[:80]}...")
    except Exception as e:
        print(f"âŒ Wrong exception type: {type(e).__name__}")
        return False

    return True


async def test_mock_research_pipeline():
    """Mock test celÃ©ho research pipeline pro FÃZI 1"""
    print("\nğŸ”¬ MOCK RESEARCH PIPELINE TEST")
    print("=" * 50)

    start_time = time.time()

    # Simulate research process
    print("ğŸ” Simulating research query...")
    await asyncio.sleep(2)  # Simulate processing time

    # Generate mock results that satisfy gates
    mock_result = {
        "timestamp": datetime.now().isoformat(),
        "query": "What are the latest developments in large language models?",
        "claims": [
            {
                "text": "Large language models have shown significant improvements in reasoning capabilities",
                "citations": [
                    {
                        "source_id": "arxiv_2023_001",
                        "passage": "Recent studies demonstrate enhanced logical reasoning",
                    },
                    {
                        "source_id": "nature_2023_015",
                        "passage": "Breakthrough in mathematical problem solving",
                    },
                ],
            },
            {
                "text": "Efficiency optimizations have reduced computational costs",
                "citations": [
                    {
                        "source_id": "acl_2023_042",
                        "passage": "Novel attention mechanisms reduce memory usage",
                    },
                    {
                        "source_id": "icml_2023_128",
                        "passage": "Quantization techniques maintain performance",
                    },
                ],
            },
        ],
        "retrieval_metadata": {
            "robots_violations": [],
            "rate_limit_violations": [],
            "accessed_domains": ["arxiv.org", "nature.com", "aclweb.org"],
        },
        "metrics": {"recall_at_10": 0.78, "ndcg_at_10": 0.72, "citation_precision": 0.89},
    }

    elapsed_time = time.time() - start_time
    print(f"â±ï¸ Mock research completed in {elapsed_time:.1f}s")

    # Validate with gates
    config = create_default_gate_config()
    gate_manager = GateManager(config)

    try:
        validation_report = await gate_manager.validate_all(mock_result)
        print("âœ… Mock research passed all validation gates")

        # Extract metrics
        claims = mock_result["claims"]
        total_citations = sum(len(claim["citations"]) for claim in claims)

        print(f"ğŸ“‹ Claims: {len(claims)}")
        print(f"ğŸ“š Citations: {total_citations}")
        print(f"ğŸ“Š Avg citations/claim: {total_citations / len(claims):.1f}")

        return True, mock_result, validation_report

    except Exception as e:
        print(f"âŒ Mock research validation failed: {e}")
        return False, None, None


async def main():
    """Main test runner"""
    print("ğŸ”¥ FÃZE 1 - VALIDATION TEST SUITE")
    print("=" * 60)
    print("Testing validation gates and fail-hard behavior WITHOUT HITL")
    print()

    success = True

    # Test 1: Validation gates
    test1_success = await test_validation_gates_standalone()
    if not test1_success:
        success = False
        print("âŒ Test 1 (Validation Gates) FAILED")
    else:
        print("âœ… Test 1 (Validation Gates) PASSED")

    print()

    # Test 2: Mock pipeline
    test2_success, mock_result, validation_report = await test_mock_research_pipeline()
    if not test2_success:
        success = False
        print("âŒ Test 2 (Mock Pipeline) FAILED")
    else:
        print("âœ… Test 2 (Mock Pipeline) PASSED")

    # Save test results
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True)

    test_result = {
        "timestamp": datetime.now().isoformat(),
        "phase": "FÃZE 1",
        "test_suite": "validation_gates_and_pipeline",
        "success": success,
        "tests": {"validation_gates": test1_success, "mock_pipeline": test2_success},
    }

    if test2_success:
        test_result["mock_research_result"] = mock_result
        test_result["validation_report"] = validation_report

    with open(artifacts_dir / "phase1_test_result.json", "w") as f:
        json.dump(test_result, f, indent=2)

    # Final result
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ FÃZE 1 VALIDATION TEST SUITE - PASSED")
        print("âœ… VÅ¡echny akceptaÄnÃ­ kritÃ©ria splnÄ›na:")
        print("   â€¢ Fail-hard validation gates implementovÃ¡ny")
        print("   â€¢ Evidence binding (â‰¥2 citations/claim)")
        print("   â€¢ Compliance a metrics validation")
        print("   â€¢ Å½ÃDNÃ‰ HITL checkpointy")
        print("   â€¢ Mock pipeline funkÄnÃ­")
        print(f"ğŸ“„ VÃ½sledky: artifacts/phase1_test_result.json")
        sys.exit(0)
    else:
        print("ğŸ’¥ FÃZE 1 VALIDATION TEST SUITE - FAILED")
        print("âŒ NÄ›kterÃ© akceptaÄnÃ­ kritÃ©ria nesplnÄ›na")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
