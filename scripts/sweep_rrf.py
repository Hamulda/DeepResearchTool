#!/usr/bin/env python3
"""
RRF Parameter Sweep Script
Optimalizace RRF k parametru pro r≈Øzn√© sc√©n√°≈ôe

Author: Senior Python/MLOps Agent
"""

import asyncio
import time
import argparse
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Tuple
import statistics
import sys
import itertools

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


class RRFParameterSweep:
    """Parameter sweep pro RRF optimalizaci"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.rrf_config = config.get("retrieval", {}).get("rrf", {})

        # RRF k values to test
        self.k_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

        # Test scenarios with different query types
        self.test_scenarios = [
            {
                "name": "specific_factual",
                "queries": [
                    "What is the melting point of gold?",
                    "When was the first quantum computer built?",
                    "Who invented the transistor?",
                ],
                "expected_behavior": "high_precision",
            },
            {
                "name": "broad_conceptual",
                "queries": [
                    "How does machine learning work?",
                    "What are the effects of climate change?",
                    "Explain quantum computing principles",
                ],
                "expected_behavior": "high_recall",
            },
            {
                "name": "research_synthesis",
                "queries": [
                    "Recent advances in artificial intelligence safety",
                    "Current state of renewable energy technology",
                    "Modern approaches to cancer treatment",
                ],
                "expected_behavior": "balanced",
            },
        ]

        # Simulated retrieval sources (in real implementation, these would be actual retrievers)
        self.retrieval_sources = ["qdrant_semantic", "bm25_sparse", "hybrid_dense"]

    def calculate_rrf_score(self, rankings: List[List[str]], k: int) -> Dict[str, float]:
        """V√Ωpoƒçet RRF sk√≥re pro dan√Ω k parametr"""

        rrf_scores = {}

        # Get all unique documents
        all_docs = set()
        for ranking in rankings:
            all_docs.update(ranking)

        # Calculate RRF score for each document
        for doc in all_docs:
            score = 0
            for ranking in rankings:
                if doc in ranking:
                    rank = ranking.index(doc) + 1  # 1-indexed
                    score += 1 / (k + rank)
            rrf_scores[doc] = score

        return rrf_scores

    async def test_k_value(self, scenario: Dict[str, Any], k: int) -> Dict[str, Any]:
        """Test jedn√© hodnoty k pro dan√Ω sc√©n√°≈ô"""

        start_time = time.time()

        try:
            scenario_results = []

            for query in scenario["queries"]:
                # Simulate different retrieval rankings (mock data)
                # In real implementation, this would call actual retrievers
                mock_rankings = self._generate_mock_rankings(query)

                # Calculate RRF scores
                rrf_scores = self.calculate_rrf_score(mock_rankings, k)

                # Sort by RRF score
                ranked_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

                # Calculate quality metrics (simulated)
                quality_metrics = self._calculate_quality_metrics(
                    ranked_docs, scenario["expected_behavior"], k
                )

                scenario_results.append(
                    {
                        "query": query,
                        "top_10_docs": [doc for doc, score in ranked_docs[:10]],
                        "rrf_scores": dict(ranked_docs[:20]),  # Top 20 for analysis
                        **quality_metrics,
                    }
                )

            # Aggregate scenario results
            avg_precision = statistics.mean([r["precision"] for r in scenario_results])
            avg_recall = statistics.mean([r["recall"] for r in scenario_results])
            avg_ndcg = statistics.mean([r["ndcg"] for r in scenario_results])
            avg_fusion_quality = statistics.mean([r["fusion_quality"] for r in scenario_results])

            processing_time = time.time() - start_time

            return {
                "scenario": scenario["name"],
                "k_value": k,
                "query_results": scenario_results,
                "aggregated_metrics": {
                    "precision": avg_precision,
                    "recall": avg_recall,
                    "ndcg": avg_ndcg,
                    "f1_score": (
                        2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)
                        if (avg_precision + avg_recall) > 0
                        else 0
                    ),
                    "fusion_quality": avg_fusion_quality,
                },
                "processing_time": processing_time,
                "error": None,
            }

        except Exception as e:
            return {
                "scenario": scenario["name"],
                "k_value": k,
                "error": str(e),
                "processing_time": time.time() - start_time,
            }

    def _generate_mock_rankings(self, query: str) -> List[List[str]]:
        """Generov√°n√≠ mock rankings pro testov√°n√≠"""

        # Create mock document IDs
        all_docs = [f"doc_{i:03d}" for i in range(1, 101)]

        # Simulate different retrieval patterns
        rankings = []

        # Semantic ranking (based on query hash for consistency)
        query_hash = abs(hash(query)) % 100
        semantic_ranking = []
        for i in range(50):
            doc_idx = (query_hash + i * 7) % 100
            semantic_ranking.append(all_docs[doc_idx])
        rankings.append(semantic_ranking)

        # BM25 ranking (different pattern)
        bm25_ranking = []
        for i in range(50):
            doc_idx = (query_hash + i * 13) % 100
            bm25_ranking.append(all_docs[doc_idx])
        rankings.append(bm25_ranking)

        # Hybrid ranking (blend of patterns)
        hybrid_ranking = []
        for i in range(50):
            doc_idx = (query_hash + i * 3) % 100
            hybrid_ranking.append(all_docs[doc_idx])
        rankings.append(hybrid_ranking)

        return rankings

    def _calculate_quality_metrics(
        self, ranked_docs: List[Tuple[str, float]], expected_behavior: str, k: int
    ) -> Dict[str, float]:
        """V√Ωpoƒçet metrik kvality pro RRF v√Ωsledky"""

        # Simulate ground truth relevance (mock)
        # In real implementation, this would use actual relevance judgments
        doc_count = len(ranked_docs)

        if expected_behavior == "high_precision":
            # Precision-focused: fewer but more relevant results
            precision = max(0.7, 1.0 - (k - 20) / 100)  # Lower k = higher precision
            recall = min(0.9, 0.6 + (k - 20) / 150)  # Higher k = higher recall
        elif expected_behavior == "high_recall":
            # Recall-focused: more comprehensive results
            precision = min(0.9, 0.5 + (k - 20) / 120)
            recall = max(0.8, 0.9 - (k - 60) / 200)
        else:  # balanced
            # Balanced: optimal around k=40-60
            optimal_k = 50
            k_distance = abs(k - optimal_k)
            precision = max(0.6, 0.85 - k_distance / 100)
            recall = max(0.6, 0.85 - k_distance / 120)

        # Add some noise for realism
        import random

        random.seed(hash(str(ranked_docs[:5])))  # Deterministic but varied
        precision += random.uniform(-0.05, 0.05)
        recall += random.uniform(-0.05, 0.05)

        # Clamp values
        precision = max(0.0, min(1.0, precision))
        recall = max(0.0, min(1.0, recall))

        # nDCG calculation (simplified)
        ndcg = (precision + recall) / 2 + random.uniform(-0.02, 0.02)
        ndcg = max(0.0, min(1.0, ndcg))

        # Fusion quality (how well RRF combines sources)
        fusion_quality = min(1.0, max(0.5, 0.8 - abs(k - 50) / 100))

        return {
            "precision": precision,
            "recall": recall,
            "ndcg": ndcg,
            "fusion_quality": fusion_quality,
        }

    async def run_parameter_sweep(self) -> Dict[str, Any]:
        """Spu≈°tƒõn√≠ kompletn√≠ho parameter sweep"""

        print("üöÄ Starting RRF parameter sweep...")

        sweep_results = {
            "timestamp": time.time(),
            "config": self.rrf_config,
            "k_values_tested": self.k_values,
            "test_scenarios": [s["name"] for s in self.test_scenarios],
            "results": [],
        }

        total_tests = len(self.test_scenarios) * len(self.k_values)
        completed_tests = 0

        # Test each combination
        for scenario in self.test_scenarios:
            print(f"\nüîç Testing scenario: {scenario['name']} ({scenario['expected_behavior']})")

            for k in self.k_values:
                print(f"  k={k}")

                result = await self.test_k_value(scenario, k)
                sweep_results["results"].append(result)

                completed_tests += 1
                if result.get("error"):
                    print(f"    ‚ùå Error: {result['error']}")
                else:
                    metrics = result["aggregated_metrics"]
                    print(
                        f"    ‚úÖ F1: {metrics['f1_score']:.3f}, "
                        f"P: {metrics['precision']:.3f}, "
                        f"R: {metrics['recall']:.3f}"
                    )

                print(f"    Progress: {completed_tests}/{total_tests}")

        # Find optimal k values
        sweep_results["optimal_k_values"] = self._find_optimal_k_values(sweep_results["results"])

        # Generate recommendations
        sweep_results["recommendations"] = self._generate_recommendations(sweep_results["results"])

        return sweep_results

    def _find_optimal_k_values(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Nalezen√≠ optim√°ln√≠ch k hodnot pro r≈Øzn√© sc√©n√°≈ôe"""

        valid_results = [r for r in results if not r.get("error")]

        if not valid_results:
            return {"error": "No valid results found"}

        # Group by scenario
        by_scenario = {}
        for result in valid_results:
            scenario = result["scenario"]
            if scenario not in by_scenario:
                by_scenario[scenario] = []
            by_scenario[scenario].append(result)

        optimal_k = {}

        # Find best k for each scenario
        for scenario, scenario_results in by_scenario.items():
            best_result = max(scenario_results, key=lambda x: x["aggregated_metrics"]["f1_score"])

            optimal_k[scenario] = {
                "k_value": best_result["k_value"],
                "f1_score": best_result["aggregated_metrics"]["f1_score"],
                "precision": best_result["aggregated_metrics"]["precision"],
                "recall": best_result["aggregated_metrics"]["recall"],
                "fusion_quality": best_result["aggregated_metrics"]["fusion_quality"],
            }

        # Find overall best k (averaged across scenarios)
        k_performance = {}
        for result in valid_results:
            k = result["k_value"]
            if k not in k_performance:
                k_performance[k] = []
            k_performance[k].append(result["aggregated_metrics"]["f1_score"])

        overall_best_k = max(k_performance.keys(), key=lambda x: statistics.mean(k_performance[x]))
        overall_best_f1 = statistics.mean(k_performance[overall_best_k])

        return {
            "by_scenario": optimal_k,
            "overall_best": {"k_value": overall_best_k, "avg_f1_score": overall_best_f1},
        }

    def _generate_recommendations(self, results: List[Dict[str, Any]]) -> List[str]:
        """Generov√°n√≠ doporuƒçen√≠ pro RRF optimalizaci"""

        valid_results = [r for r in results if not r.get("error")]
        recommendations = []

        if not valid_results:
            recommendations.append("CRITICAL: All RRF parameter tests failed")
            return recommendations

        # Find performance patterns
        k_performance = {}
        for result in valid_results:
            k = result["k_value"]
            if k not in k_performance:
                k_performance[k] = []
            k_performance[k].append(result["aggregated_metrics"]["f1_score"])

        # Best overall k
        best_k = max(k_performance.keys(), key=lambda x: statistics.mean(k_performance[x]))
        best_f1 = statistics.mean(k_performance[best_k])

        recommendations.append(
            f"OPTIMAL: Use k={best_k} for best overall performance (F1: {best_f1:.3f})"
        )

        # Scenario-specific recommendations
        scenario_performance = {}
        for result in valid_results:
            scenario = result["scenario"]
            if scenario not in scenario_performance:
                scenario_performance[scenario] = {}
            k = result["k_value"]
            scenario_performance[scenario][k] = result["aggregated_metrics"]["f1_score"]

        for scenario, k_scores in scenario_performance.items():
            best_scenario_k = max(k_scores.keys(), key=lambda x: k_scores[x])
            recommendations.append(
                f"{scenario.upper()}: Use k={best_scenario_k} (F1: {k_scores[best_scenario_k]:.3f})"
            )

        # Performance analysis
        low_k_avg = statistics.mean([statistics.mean(k_performance[k]) for k in [10, 20, 30]])
        high_k_avg = statistics.mean([statistics.mean(k_performance[k]) for k in [70, 80, 90, 100]])

        if low_k_avg > high_k_avg:
            recommendations.append(
                "PATTERN: Lower k values generally perform better (precision-focused)"
            )
        else:
            recommendations.append(
                "PATTERN: Higher k values generally perform better (recall-focused)"
            )

        # Configuration recommendations
        k_range = max(k_performance.keys()) - min(k_performance.keys())
        if k_range > 50:
            recommendations.append("CONFIG: Consider adaptive k selection based on query type")

        return recommendations


async def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="RRF Parameter Sweep")
    parser.add_argument("--config", "-c", default="config_m1_local.yaml", help="Configuration file")
    parser.add_argument("--output", "-o", help="Output JSON file")
    parser.add_argument("--k-range", nargs=2, type=int, help="Custom k range (min max)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")

    args = parser.parse_args()

    # Load configuration
    try:
        with open(args.config, "r") as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"‚ùå Configuration file {args.config} not found")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error loading configuration: {e}")
        sys.exit(1)

    # Create sweep instance
    sweep = RRFParameterSweep(config)

    # Override k range if specified
    if args.k_range:
        min_k, max_k = args.k_range
        sweep.k_values = list(range(min_k, max_k + 1, 10))
        print(f"Using custom k range: {sweep.k_values}")

    # Run parameter sweep
    results = await sweep.run_parameter_sweep()

    # Print summary
    optimal = results.get("optimal_k_values", {})
    if "error" in optimal:
        print(f"‚ùå Sweep failed: {optimal['error']}")
        sys.exit(1)

    print(f"\n‚úÖ RRF parameter sweep completed!")

    # Print optimal values
    if "overall_best" in optimal:
        best = optimal["overall_best"]
        print(f"\nüèÜ Overall Best k: {best['k_value']} (F1: {best['avg_f1_score']:.3f})")

    if "by_scenario" in optimal:
        print(f"\nüìä Scenario-specific optimal k values:")
        for scenario, metrics in optimal["by_scenario"].items():
            print(f"  {scenario}: k={metrics['k_value']} (F1: {metrics['f1_score']:.3f})")

    # Print recommendations
    recommendations = results.get("recommendations", [])
    if recommendations:
        print(f"\nüí° Recommendations:")
        for rec in recommendations:
            print(f"  ‚Ä¢ {rec}")

    # Save results
    if args.output:
        with open(args.output, "w") as f:
            json.dump(results, f, indent=2, default=str)
        print(f"üíæ Results saved to {args.output}")


if __name__ == "__main__":
    asyncio.run(main())
