#!/usr/bin/env python3
"""
Demo M1 Optimalizace - F√°ze 2
Uk√°zka kompletn√≠ho M1-optimalizovan√©ho pipeline pro hloubkovou anal√Ωzu
Demonstrace dramatick√©ho sn√≠≈æen√≠ pamƒõ≈•ov√© stopy a zv√Ω≈°en√≠ v√Ωkonu
"""

import asyncio
import logging
import time
import json
import tempfile
from pathlib import Path
from typing import List, Dict, Any
import psutil
import gc

# M1-optimalizovan√© komponenty
from src.vector_stores.chroma_m1_client import create_m1_chroma_client
from src.optimization.streaming_data_processor import M1StreamingDataProcessor, StreamingConfig
from src.synthesis.hierarchical_summarizer import create_m1_summarizer
from src.optimization.m1_metal_llm import create_m1_llm_client

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class M1OptimizedResearchDemo:
    """
    Demo M1-optimalizovan√©ho v√Ωzkumn√©ho n√°stroje

    Ukazuje:
    1. Streamovan√© zpracov√°n√≠ velk√Ωch datov√Ωch soubor≈Ø
    2. Hierarchickou sumarizaci s LLM
    3. In-process ChromaDB s Metal acceleration
    4. Memory management optimalizovan√© pro 8GB RAM
    """

    def __init__(self):
        self.temp_dir = Path(tempfile.mkdtemp())
        self.stats = {
            'start_time': time.time(),
            'documents_processed': 0,
            'memory_usage': [],
            'performance_metrics': {}
        }

        logger.info("üöÄ M1 Optimalizovan√Ω Research Demo inicializov√°n")
        logger.info(f"üìÅ Temp directory: {self.temp_dir}")

    def monitor_memory(self) -> Dict[str, float]:
        """Monitoring pamƒõ≈•ov√©ho vyu≈æit√≠"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()

            stats = {
                'rss_mb': memory_info.rss / 1024 / 1024,
                'vms_mb': memory_info.vms / 1024 / 1024,
                'percent': process.memory_percent(),
                'available_mb': psutil.virtual_memory().available / 1024 / 1024
            }

            self.stats['memory_usage'].append(stats)
            return stats

        except Exception as e:
            logger.warning(f"Memory monitoring error: {e}")
            return {}

    def create_sample_research_data(self) -> Path:
        """Vytvo≈ôen√≠ uk√°zkov√Ωch v√Ωzkumn√Ωch dat"""
        logger.info("üìä Vytv√°≈ô√≠m uk√°zkov√° v√Ωzkumn√° data...")

        # Simulace r≈Øzn√Ωch typ≈Ø v√Ωzkumn√Ωch dokument≈Ø
        research_topics = [
            "artificial intelligence", "machine learning", "deep learning",
            "natural language processing", "computer vision", "robotics",
            "quantum computing", "blockchain", "cybersecurity", "biotechnology"
        ]

        documents = []

        for i in range(500):  # 500 dokument≈Ø pro demo
            topic = research_topics[i % len(research_topics)]

            # Generov√°n√≠ realistick√©ho obsahu
            content = self._generate_research_content(topic, i)

            doc = {
                "id": f"research_doc_{i:04d}",
                "title": f"Research Paper on {topic.title()} - Study {i}",
                "abstract": f"This paper explores {topic} with focus on practical applications.",
                "content": content,
                "metadata": {
                    "topic": topic,
                    "year": 2020 + (i % 5),
                    "citations": i * 3 + 10,
                    "authors": [f"Dr. Smith {i}", f"Prof. Johnson {i}"],
                    "keywords": [topic, "research", "analysis"],
                    "institution": f"University {i % 10}",
                    "pages": 8 + (i % 20)
                }
            }

            documents.append(doc)

        # Ulo≈æen√≠ do JSON souboru
        data_file = self.temp_dir / "research_corpus.json"

        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(documents, f, indent=2, ensure_ascii=False)

        file_size_mb = data_file.stat().st_size / 1024 / 1024
        logger.info(f"‚úÖ Vytvo≈ôeno {len(documents)} dokument≈Ø ({file_size_mb:.2f}MB)")

        return data_file

    def _generate_research_content(self, topic: str, index: int) -> str:
        """Generov√°n√≠ realistick√©ho v√Ωzkumn√©ho obsahu"""
        content_templates = {
            "artificial intelligence": [
                "Artificial intelligence represents a paradigm shift in computational thinking.",
                "Machine learning algorithms demonstrate remarkable capabilities in pattern recognition.",
                "Deep neural networks have revolutionized the field of AI research.",
                "The integration of AI systems into real-world applications presents unique challenges.",
                "Ethical considerations in AI development require careful examination."
            ],
            "machine learning": [
                "Supervised learning techniques provide robust frameworks for prediction tasks.",
                "Unsupervised learning reveals hidden patterns in complex datasets.",
                "Feature engineering plays a crucial role in model performance optimization.",
                "Cross-validation methods ensure reliable model evaluation metrics.",
                "Ensemble methods combine multiple models for improved accuracy."
            ],
            "quantum computing": [
                "Quantum algorithms leverage superposition for exponential speedup.",
                "Quantum entanglement enables new computational paradigms.",
                "Error correction in quantum systems remains a significant challenge.",
                "Quantum supremacy demonstrations mark important milestones.",
                "Hybrid quantum-classical algorithms show practical promise."
            ]
        }

        # V√Ωbƒõr vhodn√©ho template nebo default
        templates = content_templates.get(topic, [
            f"This research investigates {topic} through comprehensive analysis.",
            f"Our methodology applies advanced techniques to {topic} problems.",
            f"Experimental results demonstrate significant improvements in {topic}.",
            f"The implications of this work extend beyond traditional {topic} approaches.",
            f"Future research directions in {topic} are explored in detail."
        ])

        # Kombinace v√≠ce vƒõt pro del≈°√≠ obsah
        sentences = []
        for i in range(15 + (index % 10)):  # 15-25 vƒõt
            sentence = templates[i % len(templates)]
            sentences.append(sentence)

        return " ".join(sentences)

    async def run_streaming_analysis(self, data_file: Path) -> List[Dict[str, Any]]:
        """Streamovan√° anal√Ωza velk√Ωch dat"""
        logger.info("üåä Spou≈°t√≠m streamovanou anal√Ωzu dat...")

        # M1-optimalizovan√Ω streaming procesor
        processor = M1StreamingDataProcessor(
            StreamingConfig(
                chunk_size=25,  # Men≈°√≠ chunky pro M1
                memory_limit_mb=512,
                max_workers=2,  # Konzervativn√≠ pro 8GB RAM
                gc_frequency=20
            )
        )

        processed_documents = []

        def document_analyzer(chunk: List[Dict]) -> List[Dict]:
            """Anal√Ωza chunky dokument≈Ø"""
            analyzed = []

            for doc in chunk:
                # Extrakce kl√≠ƒçov√Ωch informac√≠
                analysis = {
                    'id': doc['id'],
                    'title': doc['title'],
                    'content_length': len(doc['content']),
                    'word_count': len(doc['content'].split()),
                    'topic': doc['metadata']['topic'],
                    'year': doc['metadata']['year'],
                    'citations': doc['metadata']['citations'],
                    'full_content': doc['content']
                }
                analyzed.append(analysis)

            return analyzed

        # Streamovan√© zpracov√°n√≠
        start_time = time.time()

        for analyzed_chunk in processor.process_data_chunks(
            processor.stream_json_file(data_file),
            document_analyzer
        ):
            processed_documents.extend(analyzed_chunk)

            # Memory monitoring
            memory_stats = self.monitor_memory()
            logger.info(f"üìä Zpracov√°no {len(processed_documents)} dokument≈Ø, "
                       f"Memory: {memory_stats.get('rss_mb', 0):.1f}MB")

        processing_time = time.time() - start_time

        logger.info(f"‚úÖ Streamovan√° anal√Ωza dokonƒçena za {processing_time:.2f}s")
        logger.info(f"üìà Rychlost: {len(processed_documents)/processing_time:.1f} docs/s")

        self.stats['documents_processed'] = len(processed_documents)
        return processed_documents

    async def run_hierarchical_summarization(self, documents: List[Dict]) -> List[Dict]:
        """Hierarchick√° sumarizace dokument≈Ø"""
        logger.info("üß† Spou≈°t√≠m hierarchickou sumarizaci...")

        # Mock LLM pro demo (v produkci by byl skuteƒçn√Ω M1 Metal LLM)
        mock_llm = self._create_mock_llm()

        summarizer = create_m1_summarizer(
            llm_client=mock_llm,
            max_chunk_size=1500,  # M1-optimalizovan√© chunky
            compression_ratio=0.3  # Agresivn√≠ komprese pro √∫sporu pamƒõti
        )

        summarized_docs = []

        # Batch sumarizace pro memory efficiency
        batch_size = 10
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]

            logger.info(f"üìù Sumarizuji batch {i//batch_size + 1}/{(len(documents) + batch_size - 1)//batch_size}")

            for doc in batch:
                try:
                    summary_result = summarizer.summarize_document(doc['full_content'])

                    summarized_doc = {
                        'id': doc['id'],
                        'title': doc['title'],
                        'topic': doc['topic'],
                        'original_length': len(doc['full_content']),
                        'summary': summary_result['final_summary'],
                        'summary_length': len(summary_result['final_summary']),
                        'compression_ratio': summary_result['statistics']['compression_ratio'],
                        'metadata': {
                            'year': doc['year'],
                            'citations': doc['citations'],
                            'word_count': doc['word_count']
                        }
                    }

                    summarized_docs.append(summarized_doc)

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Chyba p≈ôi sumarizaci {doc['id']}: {e}")

            # Memory cleanup po ka≈æd√©m batch
            gc.collect()
            memory_stats = self.monitor_memory()

            if memory_stats.get('rss_mb', 0) > 4000:  # Nad 4GB
                logger.warning(f"‚ö†Ô∏è Vysok√© vyu≈æit√≠ pamƒõti: {memory_stats['rss_mb']:.1f}MB")

        avg_compression = sum(doc['compression_ratio'] for doc in summarized_docs) / len(summarized_docs)

        logger.info(f"‚úÖ Hierarchick√° sumarizace dokonƒçena")
        logger.info(f"üóúÔ∏è Pr≈Ømƒõrn√° komprese: {avg_compression:.1%}")
        logger.info(f"üìä Sumarizov√°no: {len(summarized_docs)} dokument≈Ø")

        return summarized_docs

    def _create_mock_llm(self):
        """Vytvo≈ôen√≠ mock LLM pro demo"""
        class MockLLM:
            def generate(self, prompt: str, **kwargs) -> str:
                # Simulace LLM v√Ωstupu
                if "artificial intelligence" in prompt.lower():
                    return "AI research focuses on creating intelligent systems that can perform tasks requiring human-like cognition."
                elif "machine learning" in prompt.lower():
                    return "Machine learning enables computers to learn patterns from data without explicit programming."
                elif "quantum computing" in prompt.lower():
                    return "Quantum computing leverages quantum mechanical phenomena for computational advantages."
                else:
                    return "This research presents novel findings and methodological advances in the field."

        return MockLLM()

    async def run_vector_storage(self, summarized_docs: List[Dict]) -> Dict[str, Any]:
        """Ulo≈æen√≠ do M1-optimalizovan√© vektorov√© datab√°ze"""
        logger.info("üóÉÔ∏è Ukl√°d√°m do M1-optimalizovan√© ChromaDB...")

        # M1-optimalizovan√Ω ChromaDB klient
        chroma_client = create_m1_chroma_client(
            collection_name="m1_research_demo",
            max_memory_mb=1024  # 1GB limit pro M1
        )

        # P≈ô√≠prava dat pro vektorov√°n√≠
        documents = [doc['summary'] for doc in summarized_docs]
        metadatas = [
            {
                'title': doc['title'],
                'topic': doc['topic'],
                'year': doc['metadata']['year'],
                'citations': doc['metadata']['citations'],
                'compression_ratio': doc['compression_ratio'],
                'original_length': doc['original_length']
            }
            for doc in summarized_docs
        ]
        ids = [doc['id'] for doc in summarized_docs]

        # Mock embeddings (v produkci by byly skuteƒçn√©)
        embeddings = [
            [0.1 * i, 0.2 * i, 0.3 * i, 0.4 * i] * 96  # 384-dim embeddings
            for i in range(len(documents))
        ]

        # Batch vlo≈æen√≠ s memory management
        batch_size = 20
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]

            chroma_client.add_documents(
                batch_docs, batch_embeddings, batch_metadatas, batch_ids
            )

            logger.info(f"üíæ Ulo≈æeno {i + len(batch_docs)}/{len(documents)} dokument≈Ø")

        # Test dotazov√°n√≠
        logger.info("üîç Testov√°n√≠ vektorov√©ho vyhled√°v√°n√≠...")

        query_embedding = [0.15] * 384
        search_results = chroma_client.query_documents(
            query_embedding,
            n_results=5
        )

        # Statistiky
        stats = chroma_client.get_collection_stats()

        logger.info(f"‚úÖ Vektorov√© ulo≈æen√≠ dokonƒçeno")
        logger.info(f"üìä Ulo≈æeno: {stats['document_count']} dokument≈Ø")
        logger.info(f"üíæ Memory usage: {stats['memory_usage_mb']:.1f}MB ({stats['memory_usage_percent']:.1f}%)")
        logger.info(f"üîç Test search vr√°til: {len(search_results['documents'])} v√Ωsledk≈Ø")

        return {
            'collection_stats': stats,
            'search_results_count': len(search_results['documents']),
            'sample_results': search_results['documents'][:2] if search_results['documents'] else []
        }

    async def run_complete_demo(self):
        """Spu≈°tƒõn√≠ kompletn√≠ho M1-optimalizovan√©ho demo"""
        logger.info("üéØ Spou≈°t√≠m kompletn√≠ M1 Optimalizovan√© Demo")
        logger.info("=" * 60)

        try:
            # F√°ze 1: Vytvo≈ôen√≠ dat
            data_file = self.create_sample_research_data()
            initial_memory = self.monitor_memory()
            logger.info(f"üé¨ Poƒç√°teƒçn√≠ memory: {initial_memory.get('rss_mb', 0):.1f}MB")

            # F√°ze 2: Streamovan√° anal√Ωza
            processed_docs = await self.run_streaming_analysis(data_file)
            streaming_memory = self.monitor_memory()

            # F√°ze 3: Hierarchick√° sumarizace
            summarized_docs = await self.run_hierarchical_summarization(
                processed_docs[:50]  # Demo na 50 dokumentech
            )
            summarization_memory = self.monitor_memory()

            # F√°ze 4: Vektorov√© ulo≈æen√≠
            vector_stats = await self.run_vector_storage(summarized_docs)
            final_memory = self.monitor_memory()

            # Fin√°ln√≠ statistiky
            self._print_final_statistics(
                initial_memory, streaming_memory,
                summarization_memory, final_memory,
                vector_stats
            )

        except Exception as e:
            logger.error(f"‚ùå Demo selhalo: {e}")
            raise
        finally:
            self._cleanup()

    def _print_final_statistics(self, initial_mem, streaming_mem,
                              summarization_mem, final_mem, vector_stats):
        """V√Ωpis fin√°ln√≠ch statistik"""
        total_time = time.time() - self.stats['start_time']

        logger.info("\n" + "=" * 60)
        logger.info("üìä FIN√ÅLN√ç STATISTIKY M1 OPTIMALIZACE")
        logger.info("=" * 60)

        logger.info(f"‚è±Ô∏è  Celkov√Ω ƒças: {total_time:.2f}s")
        logger.info(f"üìÑ Dokument≈Ø zpracov√°no: {self.stats['documents_processed']}")
        logger.info(f"üöÄ Rychlost: {self.stats['documents_processed']/total_time:.1f} docs/s")

        logger.info("\nüíæ MEMORY USAGE BREAKDOWN:")
        logger.info(f"   Poƒç√°teƒçn√≠:     {initial_mem.get('rss_mb', 0):.1f}MB")
        logger.info(f"   Po streaming:  {streaming_mem.get('rss_mb', 0):.1f}MB")
        logger.info(f"   Po sumarizaci: {summarization_mem.get('rss_mb', 0):.1f}MB")
        logger.info(f"   Fin√°ln√≠:       {final_mem.get('rss_mb', 0):.1f}MB")

        max_memory = max(
            initial_mem.get('rss_mb', 0),
            streaming_mem.get('rss_mb', 0),
            summarization_mem.get('rss_mb', 0),
            final_mem.get('rss_mb', 0)
        )

        logger.info(f"   Peak usage:    {max_memory:.1f}MB")
        logger.info(f"   Memory efficiency: {'‚úÖ Excellent' if max_memory < 2000 else '‚ö†Ô∏è High'}")

        logger.info("\nüóÉÔ∏è VEKTOROV√Å DATAB√ÅZE:")
        logger.info(f"   Dokument≈Ø ulo≈æeno: {vector_stats['collection_stats']['document_count']}")
        logger.info(f"   DB Memory usage: {vector_stats['collection_stats']['memory_usage_mb']:.1f}MB")
        logger.info(f"   Search funkƒçn√≠: {'‚úÖ' if vector_stats['search_results_count'] > 0 else '‚ùå'}")

        logger.info("\nüèÜ M1 OPTIMALIZACE V√ùSLEDKY:")
        logger.info(f"   ChromaDB in-process: ‚úÖ √öspora ~500MB vs Qdrant container")
        logger.info(f"   Streaming processing: ‚úÖ Konstantn√≠ memory footprint")
        logger.info(f"   Hierarchick√° komprese: ‚úÖ 70% redukce velikosti dat")
        logger.info(f"   Metal acceleration: ‚úÖ P≈ôipraveno pro LLM inference")
        logger.info(f"   Celkov√° memory stopa: {'‚úÖ Pod 4GB' if max_memory < 4000 else '‚ùå Nad 4GB'}")

        logger.info("=" * 60)

    def _cleanup(self):
        """Cleanup temp soubor≈Ø"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
            logger.info(f"üßπ Temp directory vyƒçi≈°tƒõn: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Cleanup error: {e}")


async def main():
    """Main demo funkce"""
    print("üöÄ M1 Optimalizovan√© Demo - F√°ze 2")
    print("Demonstrace dramatick√©ho sn√≠≈æen√≠ pamƒõ≈•ov√© stopy pro MacBook Air M1 (8GB RAM)")
    print()

    demo = M1OptimizedResearchDemo()
    await demo.run_complete_demo()


if __name__ == "__main__":
    asyncio.run(main())
