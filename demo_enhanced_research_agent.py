"""
Demo pro pokroÄilÃ½ Research Agent s human-in-the-loop funkcionalitou
UkÃ¡zka vÅ¡ech novÃ½ch funkcÃ­: validace zdrojÅ¯, rozÅ¡Ã­Å™enÃ© nÃ¡stroje, Streamlit UI

Author: Senior Python/MLOps Agent
"""

import asyncio
import json
import time
from typing import Dict, Any
import logging
import os
from pathlib import Path

from src.core.langgraph_agent import ResearchAgentGraph, ResearchAgentState
from src.core.config_langgraph import load_config
from src.core.enhanced_tools import (
    semantic_scholar_search,
    data_gov_search,
    wayback_machine_search,
    cross_reference_sources
)
from src.observability.langfuse_integration import get_observability_manager
from src.evaluation.evaluation_pipeline import EvaluationPipeline, RAGEvaluator
from src.graph.expert_committee import ExpertCommitteeGraph, ExpertType

# Konfigurace loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedResearchDemo:
    """Demo tÅ™Ã­da pro ukÃ¡zku pokroÄilÃ½ch funkcÃ­ Research Agenta"""

    def __init__(self):
        """Inicializace demo"""
        self.config = self._load_demo_config()
        self.agent = None
        self.expert_committee = None
        self.evaluation_pipeline = None
        self.observability = get_observability_manager()

    def _load_demo_config(self) -> Dict[str, Any]:
        """NaÄte konfiguraci pro demo"""
        return {
            "llm": {
                "model": "gpt-4o-mini",
                "temperature": 0.1,
                "synthesis_model": "gpt-4o",
                "synthesis_temperature": 0.2
            },
            "memory_store": {
                "type": "chroma",
                "collection_name": "enhanced_demo_collection",
                "persist_directory": "./demo_chroma_db"
            },
            "tools": {
                "enabled": ["semantic_scholar", "web_scraper", "wayback_machine"],
                "scraper_config": {
                    "max_pages": 5,
                    "timeout": 30
                }
            },
            "evaluation": {
                "golden_dataset_path": "evaluation/golden_dataset.json",
                "metrics_enabled": True
            },
            "observability": {
                "langfuse_enabled": True,
                "trace_level": "detailed"
            }
        }

    async def initialize(self):
        """Inicializace vÅ¡ech komponent"""
        logger.info("ğŸš€ Inicializace Enhanced Research Agent Demo...")

        # Inicializace hlavnÃ­ho agenta
        self.agent = ResearchAgentGraph(self.config)
        await self.agent.initialize()

        # Inicializace expert committee
        tools_registry = {
            "academic": ["semantic_scholar_search", "arxiv_search"],
            "web": ["firecrawl_scraper", "web_search"],
            "technical": ["github_search", "documentation_search"]
        }

        # Mock LLM client pro demo
        class DemoLLMClient:
            async def generate(self, prompt: str, temperature: float = 0.1, max_tokens: int = 1000):
                # Simulace LLM odpovÄ›di
                if "academic" in prompt.lower():
                    return "AkademickÃ© vÃ½sledky ukazujÃ­ vÃ½znamnÃ© pokroky v danÃ© oblasti..."
                elif "technical" in prompt.lower():
                    return "Z technickÃ©ho hlediska jsou klÃ­ÄovÃ© nÃ¡sledujÃ­cÃ­ aspekty..."
                elif "synthesis" in prompt.lower():
                    return "Kombinace expertnÃ­ch poznatkÅ¯ naznaÄuje..."
                else:
                    return "RelevantnÃ­ informace z dostupnÃ½ch zdrojÅ¯..."

        llm_client = DemoLLMClient()
        self.expert_committee = ExpertCommitteeGraph(llm_client, tools_registry)

        # Inicializace evaluaÄnÃ­ pipeline
        self.evaluation_pipeline = EvaluationPipeline(self.agent, llm_client)

        logger.info("âœ… Inicializace dokonÄena")

    async def demo_basic_research(self):
        """Demo zÃ¡kladnÃ­ho vÃ½zkumu"""
        print("\n" + "="*60)
        print("ğŸ” DEMO: ZÃ¡kladnÃ­ vÃ½zkum s observability")
        print("="*60)

        query = "JakÃ© jsou nejnovÄ›jÅ¡Ã­ trendy v AI safety research?"

        if self.observability.is_enabled():
            tracer = self.observability.get_tracer()
            with tracer.trace_research_session(query, {"demo": "basic_research"}) as trace:
                result = await self.agent.run(query)

                # ZaznamenÃ¡nÃ­ metrik
                tracer.log_metrics({
                    "query_complexity": 0.7,
                    "sources_found": len(result.get("sources", [])),
                    "processing_time": 2.5
                }, trace_id=trace.id if trace else None)
        else:
            result = await self.agent.run(query)

        print(f"Dotaz: {query}")
        print(f"OdpovÄ›Ä: {result.get('answer', 'N/A')[:200]}...")
        print(f"PoÄet zdrojÅ¯: {len(result.get('sources', []))}")

        return result

    async def demo_expert_committee(self):
        """Demo multi-agentnÃ­ expert committee"""
        print("\n" + "="*60)
        print("ğŸ¤ DEMO: Multi-Agent Expert Committee")
        print("="*60)

        complex_query = "Analyzuj dopad kvantovÃ©ho poÄÃ­tÃ¡nÃ­ na kryptografii a bezpeÄnost"

        print(f"KomplexnÃ­ dotaz: {complex_query}")
        print("ZapojenÃ­ expertÅ¯...")

        result = await self.expert_committee.run(complex_query)

        print(f"\nVÃ½sledek od vÃ½boru expertÅ¯:")
        print(f"OdpovÄ›Ä: {result.get('answer', 'N/A')[:300]}...")
        print(f"PoÄet expertnÃ­ch odpovÄ›dÃ­: {len(result.get('expert_responses', []))}")
        print(f"CelkovÃ¡ confidence: {result.get('confidence_scores', {}).get('overall', 0):.2f}")
        print(f"Iterace: {result.get('iterations', 0)}")

        return result

    async def demo_evaluation_pipeline(self):
        """Demo evaluaÄnÃ­ pipeline"""
        print("\n" + "="*60)
        print("ğŸ“Š DEMO: AutomatizovanÃ¡ evaluace")
        print("="*60)

        # Test na prvnÃ­ch 3 otÃ¡zkÃ¡ch z Golden Dataset
        if not self.evaluation_pipeline.golden_dataset:
            print("âŒ Golden Dataset nenÃ­ k dispozici")
            return None

        print(f"Golden Dataset obsahuje {len(self.evaluation_pipeline.golden_dataset)} otÃ¡zek")
        print("SpouÅ¡tÃ­m evaluaci na vzorku...")

        # TestovÃ¡nÃ­ na prvnÃ­ch 3 otÃ¡zkÃ¡ch pro demo
        sample_dataset = self.evaluation_pipeline.golden_dataset[:3]
        self.evaluation_pipeline.golden_dataset = sample_dataset

        results = await self.evaluation_pipeline.run_full_evaluation()
        metrics = results["metrics"]

        print("\nğŸ“ˆ VÃ½sledky evaluace:")
        print(f"Overall Score: {metrics.get('overall_score', 0):.3f}")
        print(f"Context Precision: {metrics.get('avg_context_precision', 0):.3f}")
        print(f"Context Recall: {metrics.get('avg_context_recall', 0):.3f}")
        print(f"Faithfulness: {metrics.get('avg_faithfulness', 0):.3f}")
        print(f"Answer Relevance: {metrics.get('avg_answer_relevance', 0):.3f}")
        print(f"Answer Correctness: {metrics.get('avg_answer_correctness', 0):.3f}")
        print(f"Success Rate: {metrics.get('success_rate', 0):.3f}")

        # Kontrola regresi
        regression_check = self.evaluation_pipeline.check_regression_thresholds(metrics)
        print(f"\nğŸ” Regression Check: {'âœ… PASSED' if regression_check else 'âŒ FAILED'}")

        return results

    async def demo_observability_dashboard(self):
        """Demo observability funkcÃ­"""
        print("\n" + "="*60)
        print("ğŸ“Š DEMO: Observability & Monitoring")
        print("="*60)

        if not self.observability.is_enabled():
            print("âŒ Langfuse observability nenÃ­ zapnutÃ¡")
            print("Pro aktivaci nastavte LANGFUSE_SECRET_KEY a LANGFUSE_PUBLIC_KEY")
            return

        print("âœ… Langfuse observability je aktivnÃ­")
        print("ğŸ”— Dashboard: http://localhost:3000")

        # Demo traced operace
        queries = [
            "Co je GPT-4?",
            "VysvÄ›tli kvantovou supremacii",
            "JakÃ© jsou trendy v renewable energy?"
        ]

        for i, query in enumerate(queries):
            print(f"\nğŸ”„ Trace {i+1}/3: {query}")

            tracer = self.observability.get_tracer()
            with tracer.trace_research_session(
                query,
                {"demo_batch": True, "query_index": i}
            ) as trace:
                start_time = time.time()
                result = await self.agent.run(query)
                duration = time.time() - start_time

                # DetailnÃ­ metriky
                tracer.log_metrics({
                    "duration_seconds": duration,
                    "query_length": len(query),
                    "answer_length": len(result.get("answer", "")),
                    "sources_count": len(result.get("sources", [])),
                    "demo_run": True
                }, trace_id=trace.id if trace else None)

                print(f"  âœ… Completed in {duration:.2f}s")

        print("\nğŸ“Š VÅ¡echny traces jsou dostupnÃ© v Langfuse dashboard")

    async def demo_production_readiness(self):
        """Demo production readiness checklist"""
        print("\n" + "="*60)
        print("ğŸš€ DEMO: Production Readiness Checklist")
        print("="*60)

        checklist = {
            "ğŸ” Observability": self.observability.is_enabled(),
            "ğŸ“Š Evaluation Pipeline": len(self.evaluation_pipeline.golden_dataset) >= 15,
            "ğŸ¤ Multi-Agent Support": self.expert_committee is not None,
            "ğŸ”§ Configuration Management": self.config is not None,
            "ğŸ“ Golden Dataset": Path("evaluation/golden_dataset.json").exists(),
            "ğŸ³ Docker Support": Path("docker-compose.observability.yml").exists(),
            "ğŸ”„ CI/CD Pipeline": Path(".github/workflows/ci-cd-pipeline.yml").exists(),
            "ğŸ“‹ Scaling Plan": Path("docs/production_scaling_plan.md").exists()
        }

        total_checks = len(checklist)
        passed_checks = sum(checklist.values())

        print("Production Readiness Status:")
        for item, status in checklist.items():
            print(f"  {item}: {'âœ…' if status else 'âŒ'}")

        print(f"\nğŸ“Š Overall Score: {passed_checks}/{total_checks} ({passed_checks/total_checks*100:.1f}%)")

        if passed_checks >= total_checks * 0.8:
            print("ğŸ‰ READY FOR PRODUCTION!")
        else:
            print("âš ï¸  Needs more work before production deployment")

    async def run_complete_demo(self):
        """SpuÅ¡tÄ›nÃ­ kompletnÃ­ho demo"""
        print("ğŸ¯ ENHANCED RESEARCH AGENT - COMPLETE DEMO")
        print("="*80)

        await self.initialize()

        # PostupnÃ© demo vÅ¡ech funkcÃ­
        await self.demo_basic_research()
        await self.demo_expert_committee()
        await self.demo_evaluation_pipeline()
        await self.demo_observability_dashboard()
        await self.demo_production_readiness()

        print("\n" + "="*80)
        print("ğŸŠ DEMO DOKONÄŒENO!")
        print("="*80)
        print("\nDalÅ¡Ã­ kroky:")
        print("1. ğŸ³ SpusÅ¥te Langfuse: docker-compose -f docker-compose.observability.yml up")
        print("2. ğŸ§ª SpusÅ¥te testy: pytest tests/test_evaluation_pipeline.py -v")
        print("3. ğŸš€ NasaÄte do produkce dle docs/production_scaling_plan.md")


async def main():
    """HlavnÃ­ funkce"""
    demo = EnhancedResearchDemo()
    await demo.run_complete_demo()


if __name__ == "__main__":
    asyncio.run(main())
